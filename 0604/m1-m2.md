알겠습니다. 요청하신 "Python 기반 기초 통계 → A/B Test 실전 학습자료"를 제공해주신 `Python_Core_Concepts_Practice1.ipynb` 파일의 스타일과 문체를 적용하여 풍부한 분량의 마크다운 튜토리얼로 재구성해 드리겠습니다.

---

## 🐍 Python 기반 기초 통계 → A/B Test 실전 워크샵

이 워크샵은 참가자들이 기초 통계 지식을 Python을 사용하여 실제 데이터에 적용하고, A/B 테스트를 설계, 실행, 해석하여 데이터 기반 의사결정을 내릴 수 있는 역량을 강화하는 것을 목표로 합니다.

**사전 준비 사항:**

* Python 기본 문법에 대한 이해
* Pandas DataFrame 사용 경험
  * (필요시 `Python_Core_Concepts_Practice1.ipynb` 및 `Pandas_Basic_Practice.ipynb`를 사전 학습자료로 권장합니다.)

---

## ☀️ DAY 1: Python을 활용한 핵심 통계 개념 정복

**총 시간:** 8시간 (강의: 약 3.2시간, 실습: 약 4.8시간)

---

### 🕤 09:30-10:00: 오리엔테이션 (30분)

#### 💡 개념 (Concept)

**워크샵 목표 및 로드맵 소개:**

* **2일간의 학습 여정:** 기초 통계부터 A/B 테스트 실전까지 체계적으로 학습합니다.
* **학습 목표:**
  * 핵심 통계 개념의 명확한 이해.
  * Python 코드를 활용한 통계 분석 기법 적용 능력 함양.
  * A/B 테스트의 설계, 실행, 분석 및 결과 해석 능력 배양.
  * 데이터 기반의 합리적인 의사결정 능력 향상.
* **학습 방법:**
  * 주요 이론 및 핵심 개념 강의.
  * 강의 내용을 바로 적용해보는 집중적인 실습 세션.
  * 실제와 유사한 데이터셋을 사용한 문제 해결.

**Colab 템플릿 배포 및 환경 체크:**

* **필수 라이브러리 소개:**
  * `pandas`: 데이터 조작 및 분석을 위한 핵심 라이브러리.
  * `numpy`: 수치 계산, 특히 배열 및 행렬 연산에 필수적인 라이브러리.
  * `scipy.stats`: 다양한 통계 함수 및 확률 분포 기능을 제공하는 라이브러리.
  * `statsmodels`: 통계 모델링, 통계 테스트, 시계열 분석 등을 위한 라이브러리.
  * `matplotlib` / `seaborn`: 데이터 시각화를 위한 라이브러리.
* **Colab 노트북 템플릿 배포:** 위 라이브러리들이 포함된 실습용 Colab 노트북 템플릿을 제공하고, 각자의 환경에서 정상적으로 실행되는지 확인합니다.
* **워크샵 진행 안내:**
  * Colab 노트북 사용법 및 주요 기능 간략 소개.
  * 워크샵 자료의 구조 및 활용 방안 안내.
  * 질의응답 및 원활한 소통 방법 안내.

---

### 📊 M1. 기술 통계 Review (10:00-11:20 | 80분: 강의 30분, 실습 50분)

🎯 **학습 목표:**

* 데이터의 중심 경향을 나타내는 주요 측정값(평균, 중앙값, 최빈값)을 이해하고 계산할 수 있습니다.
* 데이터의 흩어진 정도를 나타내는 산포도 측정값(분산, 표준편차, 범위, 사분위수 범위)을 이해하고 계산할 수 있습니다.
* 데이터 분포의 모양을 설명하는 왜도와 첨도의 개념을 이해하고 해석할 수 있습니다.
* Python의 주요 라이브러리(Pandas, Scipy)를 활용하여 기술 통계량을 효율적으로 분석하고 시각화할 수 있습니다.

#### 💡 개념 (Concept)

기술 통계(Descriptive Statistics)는 수집된 데이터를 요약하고 설명하여 데이터의 주요 특징을 파악하는 데 도움을 줍니다.

**1. 중심 경향치 (Central Tendency):** 데이터의 중심을 나타내는 값들입니다.

* **평균 (Mean):** 모든 데이터 값을 더한 후 데이터의 총개수로 나눈 값입니다. 이상치(outlier)에 민감하게 반응합니다.
    $\mu = \frac{\sum_{i=1}^{N} x_i}{N}$
    또는 표본 평균:
    $\bar{x} = \frac{\sum_{i=1}^{n} x_i}{n}$
* **중앙값 (Median):** 데이터를 크기 순으로 정렬했을 때 정확히 가운데에 위치하는 값입니다. 데이터의 개수가 짝수일 경우, 가운데 두 값의 평균을 사용합니다. 이상치에 덜 민감합니다(robust).
* **최빈값 (Mode):** 데이터에서 가장 빈번하게 나타나는 값입니다. 범주형 데이터나 이산형 데이터에 주로 사용되며, 여러 개 존재할 수도 있고 존재하지 않을 수도 있습니다.

**2. 산포도 (Dispersion/Variability):** 데이터가 얼마나 흩어져 있는지를 나타내는 값들입니다.

* **분산 (Variance):** 각 데이터 값이 평균으로부터 얼마나 떨어져 있는지를 제곱하여 평균 낸 값입니다. 데이터의 흩어진 정도를 나타내지만, 단위가 원래 데이터의 단위의 제곱이 됩니다.
    모분산: $\sigma^2 = \frac{\sum_{i=1}^{N} (x_i - \mu)^2}{N}$
    표본분산: $s^2 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}$ (n-1로 나누는 것은 불편추정량을 얻기 위함)
* **표준편차 (Standard Deviation):** 분산의 제곱근으로, 데이터의 변동성을 원래 데이터와 같은 단위로 직관적으로 보여줍니다.
    모표준편차: $\sigma = \sqrt{\sigma^2}$
    표본표준편차: $s = \sqrt{s^2}$
* **범위 (Range):** 데이터의 최댓값과 최솟값의 차이입니다. 계산이 간단하지만, 데이터 양 끝의 이상치에 크게 영향을 받습니다.
    $\text{Range} = \text{Max}(x) - \text{Min}(x)$
* **사분위수 범위 (Interquartile Range, IQR):** 데이터를 크기 순으로 정렬했을 때, 제3사분위수(Q3, 상위 25%)와 제1사분위수(Q1, 하위 25%)의 차이입니다. 데이터의 중간 50%가 포함되는 범위로, 이상치에 덜 민감합니다.
    $\text{IQR} = Q3 - Q1$

**3. 분포의 형태 (Shape of Distribution):**

* **왜도 (Skewness):** 데이터 분포의 비대칭성을 나타내는 지표입니다.
  * **정규분포의 왜도 = 0:** 좌우 대칭입니다.
  * **양의 왜도 (Positive Skew, 오른쪽 꼬리 분포):** 분포의 오른쪽 꼬리가 왼쪽보다 깁니다. 평균 > 중앙값 > 최빈값 순서일 가능성이 높습니다.
  * **음의 왜도 (Negative Skew, 왼쪽 꼬리 분포):** 분포의 왼쪽 꼬리가 오른쪽보다 깁니다. 최빈값 > 중앙값 > 평균 순서일 가능성이 높습니다.
* **첨도 (Kurtosis):** 데이터 분포의 뾰족한 정도와 꼬리의 두께를 나타내는 지표입니다.
  * **정규분포의 첨도 = 3 (또는 초과 첨도(Excess Kurtosis) = 0):** 표준적인 뾰족함을 가집니다. `scipy.stats.kurtosis()`는 초과 첨도를 기준으로 계산하며, 정규분포일 경우 0이 나옵니다.
  * **고첨도 (Leptokurtic, 초과 첨도 > 0):** 분포의 중심이 더 뾰족하고 꼬리가 두껍습니다 (이상치가 많을 수 있음).
  * **저첨도 (Platykurtic, 초과 첨도 < 0):** 분포의 중심이 더 평평하고 꼬리가 얇습니다.

**4. 데이터 시그니처 해석:**

기술 통계량과 시각화 도구(히스토그램, 박스플롯 등)를 함께 사용하여 데이터에 숨겨진 패턴, 경향성, 이상치 등을 종합적으로 파악합니다. 예를 들어, 평균과 중앙값이 크게 차이 나면 데이터가 한쪽으로 치우쳐 있을 가능성을 시사합니다.

#### 💻 예시 코드 (Example Code)

가상의 데이터셋 `단다컴퍼니_매출.csv` (날짜, 상품ID, 카테고리, 매출액, 판매량 컬럼 포함 가정)를 사용합니다.

```python
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

# 가상 데이터 생성 (실제로는 pd.read_csv() 사용)
data = {
    'date': pd.to_datetime(['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03', '2023-01-03', '2023-01-04', '2023-01-04', '2023-01-05', '2023-01-05'] * 10),
    'product_id': [f'P{100+i}' for i in range(10)] * 10,
    'category': ['Electronics', 'Books', 'Clothing', 'Electronics', 'Books', 'Clothing', 'Home Goods', 'Beauty', 'Sports', 'Electronics'] * 10,
    'revenue': np.random.lognormal(mean=np.log(100), sigma=0.8, size=100).round(2) + np.random.randint(10, 500, size=100), # 매출액 (양의 왜도를 가지도록)
    'quantity': np.random.randint(1, 15, size=100)
}
df = pd.DataFrame(data)

# 데이터 불러오기 (실제 경우)
# df = pd.read_csv('단다컴퍼니_매출.csv')

# 기본 정보 확인
print("--- 데이터 정보 ---")
df.info()
print("\n--- 데이터 처음 5행 ---")
print(df.head())
print("\n--- 데이터 형태 (행, 열) ---")
print(df.shape)

# 주요 기술 통계량 계산
print("\n--- 매출액(revenue) 기술 통계량 (Pandas) ---")
print(df['revenue'].describe())

print("\n--- 판매량(quantity) 기술 통계량 (Pandas) ---")
print(df['quantity'].describe())

# 개별 통계량 계산
print(f"\n매출액 평균: {df['revenue'].mean():.2f}")
print(f"매출액 중앙값: {df['revenue'].median():.2f}")
print(f"매출액 최빈값: {df['revenue'].mode().iloc[0] if not df['revenue'].mode().empty else 'N/A'}") # 최빈값이 여러 개일 수 있음
print(f"매출액 분산: {df['revenue'].var():.2f}")
print(f"매출액 표준편차: {df['revenue'].std():.2f}")
print(f"매출액 범위: {df['revenue'].max() - df['revenue'].min():.2f}")
print(f"매출액 IQR: {df['revenue'].quantile(0.75) - df['revenue'].quantile(0.25):.2f}")

# Scipy.stats 활용
print(f"\n매출액 왜도 (Scipy): {stats.skew(df['revenue']):.2f}")
print(f"매출액 첨도 (Scipy, 초과첨도): {stats.kurtosis(df['revenue']):.2f}") # Fisher's definition (정규분포 = 0)

# 결측치 간략 확인 (여기서는 없지만, 일반적인 확인 방법)
print("\n--- 결측치 수 ---")
print(df.isnull().sum())

# 이상치 간략 확인 (IQR rule 예시)
Q1 = df['revenue'].quantile(0.25)
Q3 = df['revenue'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers = df[(df['revenue'] < lower_bound) | (df['revenue'] > upper_bound)]
print(f"\n--- 매출액 이상치 (IQR rule) 개수: {len(outliers)} ---")
# print(outliers) # 실제 이상치 데이터 확인

# 맞춤 지표 함수 작성 예시
def coefficient_of_variation(data_series):
    """변동 계수 (CV)를 계산합니다."""
    mean_val = data_series.mean()
    std_val = data_series.std()
    if mean_val == 0:
        return np.nan # 평균이 0이면 정의되지 않음
    return (std_val / mean_val) * 100 # 백분율로 표시

print(f"\n매출액 변동 계수 (CV): {coefficient_of_variation(df['revenue']):.2f}%")

# 시각화 (Matplotlib/Seaborn)
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.histplot(df['revenue'], kde=True)
plt.title('매출액 분포 (Histogram)')
plt.xlabel('매출액')
plt.ylabel('빈도')

plt.subplot(1, 2, 2)
sns.boxplot(y=df['revenue'])
plt.title('매출액 분포 (Boxplot)')
plt.ylabel('매출액')

plt.tight_layout()
plt.show()
```

#### ✏️ 연습 문제 (Practice Problems)

1. `단다컴퍼니_매출.csv` 데이터 (또는 위 예시 코드로 생성된 `df`)에서 `매출액(revenue)`과 `판매량(quantity)` 컬럼에 대해 각각 평균, 중앙값, 표준편차, 왜도, 첨도를 계산하고, 그 의미를 주석으로 간략히 설명하세요.
2. 상품 `카테고리(category)`별 평균 `매출액(revenue)`을 계산하고, 가장 높은 평균 매출액을 기록한 카테고리를 찾으세요. (힌트: `groupby()` 메소드 사용)
3. `판매량(quantity)`에 대한 히스토그램과 박스플롯을 그리고, 분포의 특징(대칭성, 이상치 유무 등)을 주석으로 해석해보세요.

```python
# 연습 문제 1번 풀이 공간
# 매출액(revenue) 기술 통계량
revenue_mean = df['revenue'].mean()
revenue_median = df['revenue'].median()
revenue_std = df['revenue'].std()
revenue_skewness = stats.skew(df['revenue'])
revenue_kurtosis = stats.kurtosis(df['revenue'])

print(f"매출액 평균: {revenue_mean:.2f}") # 데이터의 산술 평균값, 대표값 중 하나
print(f"매출액 중앙값: {revenue_median:.2f}") # 데이터를 순서대로 나열했을 때 중앙에 위치하는 값, 이상치에 덜 민감
print(f"매출액 표준편차: {revenue_std:.2f}") # 데이터가 평균으로부터 얼마나 흩어져 있는지를 나타내는 지표
print(f"매출액 왜도: {revenue_skewness:.2f}") # 분포의 비대칭 정도. 양수면 오른쪽 꼬리가 길고, 음수면 왼쪽 꼬리가 김. 0에 가까울수록 대칭.
print(f"매출액 첨도: {revenue_kurtosis:.2f}") # 분포의 뾰족한 정도. 양수면 정규분포보다 뾰족, 음수면 더 평평. 0에 가까울수록 정규분포의 뾰족함.

# 판매량(quantity) 기술 통계량
quantity_mean = df['quantity'].mean()
quantity_median = df['quantity'].median()
quantity_std = df['quantity'].std()
quantity_skewness = stats.skew(df['quantity'])
quantity_kurtosis = stats.kurtosis(df['quantity'])

print(f"\n판매량 평균: {quantity_mean:.2f}")
print(f"판매량 중앙값: {quantity_median:.2f}")
print(f"판매량 표준편차: {quantity_std:.2f}")
print(f"판매량 왜도: {quantity_skewness:.2f}")
print(f"판매량 첨도: {quantity_kurtosis:.2f}")

# 연습 문제 2번 풀이 공간
category_mean_revenue = df.groupby('category')['revenue'].mean().sort_values(ascending=False)
print("\n카테고리별 평균 매출액:")
print(category_mean_revenue)
print(f"\n가장 높은 평균 매출액을 기록한 카테고리: {category_mean_revenue.index[0]} (평균 매출액: {category_mean_revenue.iloc[0]:.2f})")

# 연습 문제 3번 풀이 공간
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.histplot(df['quantity'], kde=True, bins=max(1, df['quantity'].nunique())) # nunique로 고유값 개수만큼 bin 설정 시도
plt.title('판매량 분포 (Histogram)')
plt.xlabel('판매량')
plt.ylabel('빈도')

plt.subplot(1, 2, 2)
sns.boxplot(y=df['quantity'])
plt.title('판매량 분포 (Boxplot)')
plt.ylabel('판매량')

plt.tight_layout()
plt.show()

# 판매량 분포 해석:
# 히스토그램을 보면 판매량 데이터는 대체로 낮은 값에 몰려있고 오른쪽으로 꼬리가 약간 있는 형태를 보일 수 있습니다 (데이터 생성 방식에 따라 다름).
# 이는 양의 왜도를 가질 가능성을 시사합니다.
# 박스플롯을 통해 중앙값, 사분위수 범위를 확인할 수 있으며, 데이터 포인트가 상단/하단 수염(whisker)을 벗어나면 이상치로 간주될 수 있습니다.
# 예시 데이터의 경우, 판매량은 1에서 14 사이의 정수이므로, 극단적인 이상치는 적을 수 있지만 분포는 균등하지 않을 수 있습니다.
```

---

### 🎲 M2. 확률·분포 초압축 (11:30-12:30 | 60분: 강의 25분, 실습 35분)

🎯 **학습 목표:**

* 주요 확률분포(표준정규분포, t-분포, 카이제곱분포)의 특징과 실제 활용 사례를 이해합니다.
* 확률밀도함수(PDF), 누적분포함수(CDF), 생존함수(SF)의 개념과 차이점을 명확히 파악합니다.
* Python을 활용하여 데이터의 정규성을 검정하는 방법(시각적 검토 및 통계적 검정)을 익힙니다.

#### 💡 개념 (Concept)

**1. 확률 변수 (Random Variable) 와 확률 분포 (Probability Distribution):**

* **확률 변수:** 무작위 실험의 결과에 의해 값이 결정되는 변수입니다.
  * **이산형 확률 변수 (Discrete Random Variable):** 셀 수 있는 값(주로 정수)을 가집니다. (예: 주사위 눈, 하루 동안의 고객 방문 수)
  * **연속형 확률 변수 (Continuous Random Variable):** 특정 범위 내의 모든 실수 값을 가질 수 있습니다. (예: 키, 몸무게, 온도, 시간)
* **확률 분포:** 확률 변수가 특정 값 또는 값의 범위에 속할 확률을 나타내는 함수 또는 규칙입니다.

**2. 주요 연속 확률 분포:**

* **정규 분포 (Normal Distribution):** $N(\mu, \sigma^2)$
  * 자연 현상 및 사회 현상에서 가장 흔하게 발견되는 종 모양(bell-shaped)의 대칭적인 분포입니다.
  * 평균($\mu$)과 표준편차($\sigma$) 두 모수에 의해 모양이 결정됩니다.
  * 평균 = 중앙값 = 최빈값입니다.
* **표준 정규 분포 (Standard Normal Distribution):** $Z \sim N(0, 1)$
  * 평균이 0이고 표준편차가 1인 특별한 정규 분포입니다.
  * 일반 정규 분포를 따르는 확률 변수 X는 다음과 같은 Z-점수(Z-score) 변환을 통해 표준 정규 분포로 표준화할 수 있습니다:
        $Z = \frac{X - \mu}{\sigma}$
* **t-분포 (Student's t-Distribution):**
  * 정규 분포와 유사한 종 모양이지만, 꼬리가 더 두껍습니다(fat tails).
  * 모집단의 분산($\sigma^2$)을 알지 못하고 표본 크기(n)가 작을 때, 표본 평균($\bar{x}$)을 이용하여 모평균($\mu$)을 추정하거나 검정할 때 사용됩니다.
  * **자유도 (degrees of freedom, df)**라는 모수에 따라 모양이 변하며, 자유도가 커질수록 표준 정규 분포에 근사합니다 (보통 df = n-1).
* **카이제곱 분포 (Chi-squared Distribution, $\chi^2$-Distribution):**
  * k개의 서로 독립적인 표준 정규 확률 변수들의 제곱의 합이 따르는 분포입니다. $\chi^2(k)$ 로 표기하며, k는 자유도입니다.
  * 오른쪽으로 긴 꼬리를 가지는 비대칭 분포입니다. 자유도가 커질수록 대칭성에 가까워집니다.
  * 주로 분산 추정, 적합도 검정(goodness-of-fit test), 독립성 검정(test of independence) 등에 사용됩니다.

**3. 확률 분포 함수:**

* **PDF (Probability Density Function; 확률 밀도 함수):**
  * 연속 확률 변수 X에 대해, 변수가 특정 구간 [a, b]에 속할 확률 $P(a \le X \le b)$는 PDF $f(x)$를 해당 구간에서 적분한 값($\int_a^b f(x)dx$)으로 계산됩니다.
  * PDF 자체는 특정 지점에서의 확률이 아니라, 해당 지점에서의 확률의 밀도(상대적 가능성)를 나타냅니다. 연속 확률 변수에서 특정 한 지점에서의 확률은 0입니다 ($P(X=x)=0$).
  * PDF 곡선 아래 전체 면적은 1입니다.
* **CDF (Cumulative Distribution Function; 누적 분포 함수):**
  * 확률 변수 X가 특정 값 x보다 작거나 같을 확률을 나타냅니다: $F(x) = P(X \le x)$.
  * PDF를 $-\infty$부터 x까지 적분한 값입니다.
  * 0에서 시작하여 1로 끝나는 비감소 함수입니다.
* **SF (Survival Function; 생존 함수) 또는 CCFD (Complementary Cumulative Distribution Function):**
  * 확률 변수 X가 특정 값 x보다 클 확률을 나타냅니다: $S(x) = P(X > x)$.
  * $S(x) = 1 - CDF(x)$.
  * 생존 분석 등에서 주로 사용됩니다.

**4. 정규성 (Normality) 검정:**

* 데이터가 정규 분포를 따르는지 확인하는 과정입니다. 많은 통계 분석 기법(예: t-검정, ANOVA)들이 데이터의 정규성 가정을 필요로 합니다.
* **시각적 방법:**
  * 히스토그램: 분포의 모양이 종 모양인지 확인.
  * Q-Q Plot (Quantile-Quantile Plot): 데이터의 분위수와 이론적 정규분포의 분위수를 산점도로 그려 비교. 점들이 직선에 가까울수록 정규성을 만족합니다.
* **통계적 검정 방법 (가설검정 기반):**
  * 귀무가설 ($H_0$): 데이터는 정규 분포를 따른다.
  * 대립가설 ($H_1$): 데이터는 정규 분포를 따르지 않는다.
  * **Shapiro-Wilk Test:** 표본 크기가 비교적 작을 때 (보통 n < 50 또는 n < 2000 등 문헌마다 다름) 유용한 정규성 검정 방법입니다. 검정력이 좋다고 알려져 있습니다.
  * **Kolmogorov-Smirnov Test (KS-test):** 데이터가 특정 분포(예: 정규분포, 지수분포 등)를 따르는지 검정합니다. 정규분포의 경우, 평균과 표준편차를 데이터로부터 추정하여 사용하면 검정력이 낮아질 수 있어 Lilliefors test (KS test의 수정 버전)가 사용되기도 합니다.
  * **D'Agostino and Pearson's Test (normaltest):** 데이터의 왜도와 첨도를 함께 고려하여 정규성을 검정합니다.

#### 💻 예시 코드 (Example Code)

```python
import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

# M1에서 사용한 df의 'revenue' 또는 임의 생성 데이터 사용
# 여기서는 'revenue' 사용
data_for_normality_check = df['revenue'].dropna() # 결측치 제거

# 1. 분포 시각화
plt.figure(figsize=(18, 5))

# 정규 분포 예시
mu, sigma = 0, 1 # 평균, 표준편차
s_norm = np.random.normal(mu, sigma, 1000)
plt.subplot(1, 3, 1)
sns.histplot(s_norm, kde=True, stat="density")
x_norm = np.linspace(stats.norm.ppf(0.001, mu, sigma), stats.norm.ppf(0.999, mu, sigma), 100)
plt.plot(x_norm, stats.norm.pdf(x_norm, mu, sigma), 'r-', lw=2, label='Normal PDF')
plt.title(f'Normal Distribution N({mu},{sigma}^2)')
plt.legend()

# t-분포 예시
df_t = 5 # 자유도
s_t = np.random.standard_t(df_t, 1000)
plt.subplot(1, 3, 2)
sns.histplot(s_t, kde=True, stat="density")
x_t = np.linspace(stats.t.ppf(0.001, df_t), stats.t.ppf(0.999, df_t), 100)
plt.plot(x_t, stats.t.pdf(x_t, df_t), 'g-', lw=2, label=f't-dist PDF (df={df_t})')
plt.title(f't-Distribution (df={df_t})')
plt.legend()

# 카이제곱 분포 예시
df_chi2 = 3 # 자유도
s_chi2 = np.random.chisquare(df_chi2, 1000)
plt.subplot(1, 3, 3)
sns.histplot(s_chi2, kde=True, stat="density")
x_chi2 = np.linspace(stats.chi2.ppf(0.001, df_chi2), stats.chi2.ppf(0.999, df_chi2), 100)
plt.plot(x_chi2, stats.chi2.pdf(x_chi2, df_chi2), 'b-', lw=2, label=f'Chi2 PDF (df={df_chi2})')
plt.title(f'Chi-squared Distribution (df={df_chi2})')
plt.legend()

plt.tight_layout()
plt.show()

# 2. Q-Q Plot (매출액 데이터 사용)
plt.figure(figsize=(6, 5))
stats.probplot(data_for_normality_check, dist="norm", plot=plt)
plt.title('Q-Q Plot for Revenue (vs Normal Distribution)')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Sample Quantiles')
plt.grid(True)
plt.show()
# 해석: 점들이 빨간색 직선에 가깝게 분포할수록 데이터는 정규분포를 따른다고 볼 수 있습니다.
# 예시 매출액 데이터는 로그정규분포와 유사하게 생성했으므로 직선에서 벗어날 가능성이 높습니다.

# 3. 정규성 검정 통계량 (매출액 데이터 사용)
print("\n--- 정규성 검정 (Revenue) ---")

# Shapiro-Wilk Test
shapiro_stat, shapiro_p_value = stats.shapiro(data_for_normality_check)
print(f"Shapiro-Wilk Test: Statistic={shapiro_stat:.4f}, p-value={shapiro_p_value:.4f}")
alpha = 0.05
if shapiro_p_value > alpha:
    print("  Shapiro-Wilk: 데이터는 정규분포를 따르는 것으로 보임 (귀무가설 채택)")
else:
    print("  Shapiro-Wilk: 데이터는 정규분포를 따르지 않는 것으로 보임 (귀무가설 기각)")

# Kolmogorov-Smirnov Test (vs 'norm')
# KS 검정은 평균과 표준편차를 인자로 전달해야 더 정확하지만, scipy는 이를 자동으로 추정하지 않음
# 따라서 더 정확한 검정을 위해서는 데이터의 평균과 표준편차를 계산하여 args로 전달해야 함
data_mean = np.mean(data_for_normality_check)
data_std = np.std(data_for_normality_check, ddof=1) # 표본표준편차
ks_stat, ks_p_value = stats.kstest(data_for_normality_check, 'norm', args=(data_mean, data_std))
print(f"\nKolmogorov-Smirnov Test: Statistic={ks_stat:.4f}, p-value={ks_p_value:.4f}")
if ks_p_value > alpha:
    print("  Kolmogorov-Smirnov: 데이터는 정규분포를 따르는 것으로 보임 (귀무가설 채택)")
else:
    print("  Kolmogorov-Smirnov: 데이터는 정규분포를 따르지 않는 것으로 보임 (귀무가설 기각)")

# D'Agostino and Pearson's Test (normaltest)
normaltest_stat, normaltest_p_value = stats.normaltest(data_for_normality_check)
print(f"\nD'Agostino and Pearson's Test: Statistic={normaltest_stat:.4f}, p-value={normaltest_p_value:.4f}")
if normaltest_p_value > alpha:
    print("  D'Agostino and Pearson: 데이터는 정규분포를 따르는 것으로 보임 (귀무가설 채택)")
else:
    print("  D'Agostino and Pearson: 데이터는 정규분포를 따르지 않는 것으로 보임 (귀무가설 기각)")


# PDF, CDF, SF 예시 (표준 정규 분포)
x_val = 1.0
mu_norm, std_norm = 0, 1 # 표준 정규 분포

pdf_val = stats.norm.pdf(x_val, loc=mu_norm, scale=std_norm)
cdf_val = stats.norm.cdf(x_val, loc=mu_norm, scale=std_norm)
sf_val = stats.norm.sf(x_val, loc=mu_norm, scale=std_norm)

print(f"\n--- PDF, CDF, SF 예시 (표준정규분포, x={x_val}) ---")
print(f"PDF at x={x_val}: {pdf_val:.4f}")
print(f"CDF at x={x_val} (P(X <= {x_val})): {cdf_val:.4f}")
print(f"SF at x={x_val} (P(X > {x_val})): {sf_val:.4f}")
print(f"CDF + SF = {cdf_val + sf_val:.4f} (should be close to 1)")
```

#### ✏️ 연습 문제 (Practice Problems)

1. M1에서 사용한 `df`의 `매출액(revenue)` 데이터에 대해 Q-Q plot을 그려보고, 시각적으로 정규성을 평가해보세요. 어떤 결론을 내릴 수 있나요?
2. `scipy.stats.shapiro()`와 `scipy.stats.normaltest()`를 사용하여 `매출액(revenue)` 데이터의 정규성을 검정하고, p-value를 기준으로 결과를 해석하세요. (유의수준 $\alpha=0.05$ 로 가정)
3. 평균이 50, 표준편차가 10인 정규분포에서 100개의 샘플을 생성하세요 (`np.random.normal()`). 이 샘플 데이터에 대해, $x=60$일 때의 CDF 값과 SF 값을 `scipy.stats.norm.cdf()`와 `scipy.stats.norm.sf()`를 사용하여 각각 구하고, 그 의미를 설명하세요.

```python
# 연습 문제 1번 풀이 공간
plt.figure(figsize=(6, 5))
stats.probplot(df['revenue'].dropna(), dist="norm", plot=plt)
plt.title('Q-Q Plot for Revenue (vs Normal Distribution)')
plt.xlabel('Theoretical Quantiles (Normal)')
plt.ylabel('Sample Quantiles (Revenue)')
plt.grid(True)
plt.show()
# 해석: (위 예시코드에서 이미 실행)
# 생성된 'revenue' 데이터는 로그정규분포와 유사하거나 임의의 양의 왜도를 가지도록 만들어졌으므로,
# Q-Q plot에서 점들이 직선에서 체계적으로 벗어나는 패턴을 보일 것입니다.
# 특히, 낮은 값과 높은 값에서 직선보다 위나 아래로 벗어나는 모습을 보일 수 있으며, 이는 정규분포와 거리가 있음을 시사합니다.

# 연습 문제 2번 풀이 공간
revenue_data = df['revenue'].dropna()
alpha = 0.05

# Shapiro-Wilk Test
shapiro_stat, shapiro_p_value = stats.shapiro(revenue_data)
print(f"Shapiro-Wilk Test for Revenue: Statistic={shapiro_stat:.4f}, p-value={shapiro_p_value:.4f}")
if shapiro_p_value > alpha:
    print("  매출액 데이터는 정규분포를 따르는 것으로 보입니다 (귀무가설 채택).")
else:
    print("  매출액 데이터는 정규분포를 따르지 않는 것으로 보입니다 (귀무가설 기각).")

# D'Agostino and Pearson's Test (normaltest)
normaltest_stat, normaltest_p_value = stats.normaltest(revenue_data)
print(f"\nD'Agostino and Pearson's Test for Revenue: Statistic={normaltest_stat:.4f}, p-value={normaltest_p_value:.4f}")
if normaltest_p_value > alpha:
    print("  매출액 데이터는 정규분포를 따르는 것으로 보입니다 (귀무가설 채택).")
else:
    print("  매출액 데이터는 정규분포를 따르지 않는 것으로 보입니다 (귀무가설 기각).")
# 해석: 생성된 'revenue' 데이터의 특성상 p-value는 alpha보다 작게 나와 귀무가설을 기각할 가능성이 높습니다.
# 즉, 데이터가 정규분포를 따르지 않는다는 결론을 내릴 수 있습니다.

# 연습 문제 3번 풀이 공간
mean_practice = 50
std_practice = 10
sample_size_practice = 100
x_value_practice = 60

# 정규분포에서 샘플 생성
sampled_data_practice = np.random.normal(loc=mean_practice, scale=std_practice, size=sample_size_practice)

# CDF 계산
cdf_at_60 = stats.norm.cdf(x_value_practice, loc=mean_practice, scale=std_practice)
print(f"\n평균 {mean_practice}, 표준편차 {std_practice}인 정규분포에서 x={x_value_practice}일 때의 CDF 값: {cdf_at_60:.4f}")
print(f"  의미: 데이터 값이 {x_value_practice}보다 작거나 같을 확률은 약 {cdf_at_60*100:.2f}% 입니다.")

# SF 계산
sf_at_60 = stats.norm.sf(x_value_practice, loc=mean_practice, scale=std_practice)
print(f"평균 {mean_practice}, 표준편차 {std_practice}인 정규분포에서 x={x_value_practice}일 때의 SF 값: {sf_at_60:.4f}")
print(f"  의미: 데이터 값이 {x_value_practice}보다 클 확률은 약 {sf_at_60*100:.2f}% 입니다.")

# 참고: 생성된 샘플 데이터(sampled_data_practice)는 이론적인 CDF, SF 값을 계산하는 데 직접 사용되진 않습니다.
# 이 문제의 의도는 scipy.stats.norm의 기능을 이해하는 것입니다.
# 만약 샘플 데이터 자체에서 x=60 이하인 비율을 구하려면 다음과 같이 합니다:
# empirical_cdf_at_60 = np.sum(sampled_data_practice <= x_value_practice) / sample_size_practice
# print(f"샘플 데이터에서 x={x_value_practice} 이하인 비율 (Empirical CDF): {empirical_cdf_at_60:.4f}")
```

---

**(Day 1의 나머지 모듈과 Day 2 전체 내용은 분량이 매우 방대하여, 여기서는 Day 1의 M1, M2 모듈까지만 상세히 작성했습니다. 전체 튜토리얼을 원하시면 이어서 작성해 드릴 수 있습니다. Jupyter Notebook 과제 생성은 이 마크다운 튜토리얼이 완성된 후, 각 모듈의 연습문제를 기반으로 별도의 노트북 파일로 구성하는 방식으로 진행될 것입니다.)**
