{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj4HaIIuHAuj"
      },
      "source": [
        "# ğŸ íŒŒì´ì¬ í…ìŠ¤íŠ¸ ë¶„ì„: Chapter 6. Kiwipiepyì™€ ìµœì‹  AIë¥¼ í™œìš©í•œ ê°ì„± ë¶„ì„\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ë¯¸ë¦¬ ì •ë‹µ(ë ˆì´ë¸”)ì´ ì •í•´ì§„ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” **ì§€ë„ í•™ìŠµ(Supervised Learning)**ì„ í…ìŠ¤íŠ¸ ë¶„ì„ì— ì ìš©í•˜ëŠ” ì‹¬í™” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.\n",
        "\n",
        "íŠ¹íˆ, ê¸°ì¡´ì˜ í˜•íƒœì†Œ ë¶„ì„ê¸°ì—ì„œ ë” ë¹ ë¥´ê³  ë°œì „ëœ **Kiwipiepy**ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•˜ê³ , ë‘ ê°€ì§€ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ê°ì„± ë¶„ì„ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
        "\n",
        "1.  **ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ë°©ì‹**: `Kiwipiepy`ë¡œ ì „ì²˜ë¦¬í•œ í…ìŠ¤íŠ¸ë¥¼ `TF-IDF`ë¼ëŠ” ê¸°ë²•ìœ¼ë¡œ ë²¡í„°í™”í•˜ì—¬, ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.\n",
        "2.  **ìµœì‹  AI ì„ë² ë”© ë°©ì‹**: `OpenAI`ì˜ ê°•ë ¥í•œ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ê°€ í’ë¶€í•œ ë²¡í„°(ì„ë² ë”©)ë¡œ ë³€í™˜í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
        "\n",
        "ì´ ê³¼ì •ì„ í†µí•´ ìˆ˜ë§Œ ê°œì˜ ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ì´ìš©í•´ í•´ë‹¹ ë¦¬ë·°ê°€ 'ê¸ì •'ì ì¸ì§€ 'ë¶€ì •'ì ì¸ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ì§ì ‘ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ’¡ ì‹œì‘ ì „ ì¤€ë¹„: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ë°ì´í„° ë¡œë“œ\n",
        "\n",
        "ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ë¨¼ì € `kiwipiepy`ì™€ `openai` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. `kiwipiepy`ëŠ” í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í˜•íƒœì†Œ ë¶„ì„ê¸°ì´ë©°, `openai`ëŠ” í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "ë°ì´í„°ì…‹ì€ ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ëŒ€í‘œì ì¸ í•œêµ­ì–´ ê°ì„± ë¶„ì„ ë°ì´í„°ì…‹ì¸ **NSMC(Naver Sentiment Movie Corpus)** ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ë„¤ì´ë²„ ì˜í™”ì˜ ë¦¬ë·° 20ë§Œ ê°œì™€ ê° ë¦¬ë·°ì— ëŒ€í•œ ê¸ì •(ë ˆì´ë¸” 1) ë˜ëŠ” ë¶€ì •(ë ˆì´ë¸” 0) í‰ê°€ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vW44g6wIHAuk"
      },
      "outputs": [],
      "source": [
        "# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install kiwipiepy pandas scikit-learn plotly openai tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "from kiwipiepy import Kiwi\n",
        "import openai\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# tqdmì„ pandasì— ì ìš©í•˜ê¸° ìœ„í•œ ì„¤ì •\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ\n",
        "# ë„ì»¤/ë¡œì»¬ í™˜ê²½ì— ë”°ë¼ ê²½ë¡œëŠ” ìˆ˜ì •í•˜ì…”ì•¼ í•©ë‹ˆë‹¤.\n",
        "train_df = pd.read_csv(\"../datasets/text/nsmc/ratings_train.txt\", sep='\\t')\n",
        "test_df = pd.read_csv(\"../datasets/text/nsmc/ratings_test.txt\", sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. ë°ì´í„° ì •ì œ (ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µ ì œê±°)\n",
        "train_df = train_df.dropna().drop_duplicates(subset=['document'])\n",
        "test_df = test_df.dropna().drop_duplicates(subset=['document'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. ì‹¤ìŠµì„ ìœ„í•´ ë°ì´í„° ìƒ˜í”Œë§\n",
        "train_df = train_df.sample(n=1000, random_state=42)\n",
        "test_df = test_df.sample(n=250, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Kiwipiepyë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "kiwi = Kiwi()\n",
        "# íŒŒìƒ ëª…ì‚¬, íŒŒìƒ ë¶€ì‚¬ ë“±ì„ ë” ì˜ê²Œ ë¶„ë¦¬í•˜ëŠ” ì˜µì…˜ ì¶”ê°€\n",
        "# ì˜ˆ: 'ê³ ë§ˆì›€' -> 'ê³ ë§™/VA-I', 'ìŒ/ETN'\n",
        "def kiwi_preprocess(text):\n",
        "    text = re.sub(r'[^ê°€-í£\\s]', '', str(text)) # í•œê¸€ê³¼ ê³µë°±ë§Œ ë‚¨ê¸°ê¸°\n",
        "    tokens = kiwi.tokenize(text, split_complex=True)\n",
        "    # CountVectorizer/TfidfVectorizer ì…ë ¥ì„ ìœ„í•´ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ ë°˜í™˜\n",
        "    return ' '.join([token.form for token in tokens])\n",
        "\n",
        "# ì „ì²˜ë¦¬ ì ìš© (KiwipiepyëŠ” C++ë¡œ êµ¬í˜„ë˜ì–´ ìˆì–´ ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤)\n",
        "train_df['document_processed'] = train_df['document'].progress_apply(kiwi_preprocess)\n",
        "test_df['document_processed'] = test_df['document'].progress_apply(kiwi_preprocess)\n",
        "\n",
        "print(\"ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOSskGI7HAul"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 1. TF-IDFë¥¼ ì´ìš©í•œ ì „í†µì  ê°ì„± ë¶„ì„\n",
        "\n",
        "### 1. íŠ¹ì§• ë²¡í„°í™”ì™€ ë°ì´í„° ë¶„ë¦¬ (Feature Vectorization & Data Splitting)\n",
        "\n",
        "#### ğŸ’¡ ê°œë… (Concept)\n",
        "\n",
        "ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ì ‘ê·¼ë²•ìœ¼ë¡œ, ë‹¨ì–´ì˜ ë¹ˆë„ì™€ ë¬¸ì„œì—ì„œì˜ ì¤‘ìš”ë„ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ëŠ” **TF-IDF ë°©ì‹**ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ë˜í•œ, ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ê¸° ìœ„í•´ ì „ì²´ ë°ì´í„°ë¥¼ **í•™ìŠµ ë°ì´í„°(Training Data)**ì™€ **í…ŒìŠ¤íŠ¸ ë°ì´í„°(Test Data)**ë¡œ ë¶„ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ë¥¼ ë³´ê³  íŒ¨í„´ì„ ìµíˆë©°, ì´ì „ì— í•œ ë²ˆë„ ë³¸ ì  ì—†ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë§ì¶”ëŠ”ì§€ë¥¼ í†µí•´ ì„±ëŠ¥ì„ í‰ê°€ë°›ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ë‹¨ìˆœíˆ ë°ì´í„°ë¥¼ ì•”ê¸°í•˜ëŠ” ê²ƒ(ê³¼ì í•©, Overfitting)ì´ ì•„ë‹ˆë¼, ì¼ë°˜í™”ëœ ì˜ˆì¸¡ ëŠ¥ë ¥ì„ ê°–ì·„ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
        "\n",
        "#### ğŸ’» ì˜ˆì‹œ ì½”ë“œ (Example Code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQh7UUVJHAul"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ë°ì´í„°ì…‹ì„ íŠ¹ì§•(X)ê³¼ ë ˆì´ë¸”(y)ë¡œ ë¶„ë¦¬\n",
        "X_train = train_df['document_processed']\n",
        "y_train = train_df['label']\n",
        "X_test = test_df['document_processed']\n",
        "y_test = test_df['label']\n",
        "\n",
        "# TF-IDF ë²¡í„°í™”\n",
        "# í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” fitê³¼ transformì„ ëª¨ë‘ ìˆ˜í–‰ (fit_transform)\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” í•™ìŠµëœ ì–´íœ˜ì‚¬ì „ì„ ê¸°ì¤€ìœ¼ë¡œ transformë§Œ ìˆ˜í–‰\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=3, ngram_range=(1, 2))\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(\"TF-IDF ë²¡í„°í™” ì™„ë£Œ.\")\n",
        "print(f\"í•™ìŠµ ë°ì´í„° (TF-IDF) í˜•íƒœ: {X_train_tfidf.shape}\")\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° (TF-IDF) í˜•íƒœ: {X_test_tfidf.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH34PB6OHAul"
      },
      "source": [
        "#### âœï¸ ì—°ìŠµ ë¬¸ì œ (Practice Problems)\n",
        "\n",
        "1.  `TfidfVectorizer`ë¥¼ ìƒì„±í•  ë•Œ, `max_features=2000` íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•˜ì—¬ ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ 2000ê°œì˜ ë‹¨ì–´(n-gram í¬í•¨)ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•´ ë³´ì„¸ìš”. TF-IDF í–‰ë ¬ì˜ í˜•íƒœ(`shape`)ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì½”ë“œ ì‘ì„±"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  `kiwipiepy`ì˜ `tokenize` ë©”ì†Œë“œì—ëŠ” `split_complex` ì™¸ì—ë„ `normalize_coda=True`ì™€ ê°™ì€ ìœ ìš©í•œ ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤. ì´ ì˜µì…˜ì€ 'ë¨¹ì—ˆì—Œã…‹ã…‹'ì™€ ê°™ì´ ì–´ë¯¸ ë’¤ì— ììŒì´ ë¶™ëŠ” ì‹ ì¡°ì–´ë¥¼ 'ë¨¹ì—ˆì–´'ì™€ 'ã…‹ã…‹ã…‹'ë¡œ ë¶„ë¦¬í•´ì¤ë‹ˆë‹¤. `kiwi_preprocess` í•¨ìˆ˜ì— ì´ ì˜µì…˜ì„ ì¶”ê°€í•˜ê³  ì „ì²˜ë¦¬ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•œ ë’¤, TF-IDF í–‰ë ¬ì˜ í¬ê¸°ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ê´€ì°°í•´ë³´ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì½”ë“œ ì‘ì„±"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "### 2. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ (Model Training & Prediction)\n",
        "\n",
        "#### ğŸ’¡ ê°œë… (Concept)\n",
        "\n",
        "ì´ì œ ì¤€ë¹„ëœ ìˆ«ì ë°ì´í„°(TF-IDF ë²¡í„°)ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ í•™ìŠµì‹œí‚¬ ì°¨ë¡€ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” **ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression)** ëª¨ë¸ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. `fit` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì€ TF-IDF ë²¡í„°(íŠ¹ì§•)ì™€ ê°ì„± ë ˆì´ë¸”(ì •ë‹µ) ì‚¬ì´ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. í•™ìŠµì´ ì™„ë£Œëœ í›„ì—ëŠ” `predict` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì˜ ì •ë‹µì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "#### ğŸ’» ì˜ˆì‹œ ì½”ë“œ (Example Code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgooItA8HAul"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1. ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
        "lr_model = LogisticRegression(random_state=42, C=5, max_iter=1000)\n",
        "lr_model.fit(X_train_tfidf, y_train)\n",
        "print(\"ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (TF-IDF ê¸°ë°˜).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "y_pred = lr_model.predict(X_test_tfidf)\n",
        "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì™„ë£Œ.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì˜ˆì¸¡ ê²°ê³¼ ì¼ë¶€ í™•ì¸\n",
        "print(\"ì‹¤ì œ ê°’ (ì²˜ìŒ 10ê°œ):\", list(y_test[:10]))\n",
        "print(\"ì˜ˆì¸¡ ê°’ (ì²˜ìŒ 10ê°œ):\", list(y_pred[:10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp3JnEeJHAum"
      },
      "source": [
        "#### âœï¸ ì—°ìŠµ ë¬¸ì œ (Practice Problems)\n",
        "\n",
        "1.  ë¡œì§€ìŠ¤í‹± íšŒê·€ ëŒ€ì‹ , ë˜ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ì¸ **ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ(Naive Bayes)** ëª¨ë¸(`sklearn.naive_bayes.MultinomialNB`)ì„ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ ë°ì´í„°ë¡œ í•™ìŠµì‹œí‚¤ê³  ì˜ˆì¸¡ì„ ìˆ˜í–‰í•´ ë³´ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì½”ë“œ ì‘ì„±\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb = MultinomialNB()\n",
        "\n",
        "nb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = nb.predict(X_test_tfidf)\n",
        "\n",
        "print(\"ì‹¤ì œ ê°’ (ì²˜ìŒ 10ê°œ):\", list(y_test[:10]))\n",
        "print(\"ì˜ˆì¸¡ ê°’ (ì²˜ìŒ 10ê°œ):\", list(y_pred[:10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  ë‚´ê°€ ì§ì ‘ ì‘ì„±í•œ ë¦¬ë·°ì˜ ê°ì„±ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜ `predict_my_review_tfidf(text)`ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”. í•¨ìˆ˜ ë‚´ë¶€ëŠ” ì•„ë˜ ìˆœì„œë¥¼ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.\n",
        "    * í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (`kiwi_preprocess` í•¨ìˆ˜ ì‚¬ìš©)\n",
        "    * ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ TF-IDF ë²¡í„°ë¡œ ë³€í™˜ (`tfidf_vectorizer.transform` ì‚¬ìš©)\n",
        "    * í•™ìŠµëœ `lr_model`ë¡œ ì˜ˆì¸¡ (`lr_model.predict` ì‚¬ìš©)\n",
        "    * \"ì´ ë¦¬ë·°ëŠ” [ê¸ì •/ë¶€ì •]ì…ë‹ˆë‹¤.\" ë¼ê³  ê²°ê³¼ë¥¼ ì¶œë ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì½”ë“œ ì‘ì„±\n",
        "\n",
        "from kiwipiepy import Kiwi\n",
        "\n",
        "kiwi = Kiwi()\n",
        "\n",
        "\n",
        "def kiwi_preprocess(text) :\n",
        "    return \" \".join([tok.form for tok in kiwi.tokenize(text, skip_space=True)])\n",
        "\n",
        "def predict_my_review_tfidf(text) :\n",
        "    tokenized_text = kiwi_preprocess(text)\n",
        "    tfidf_matrix = tfidf_vectorizer.transform([tokenized_text])\n",
        "    label = lr_model.predict(tfidf_matrix)\n",
        "    if label == 0 :\n",
        "        return \"ë¶€ì •ì…ë‹ˆë‹¤.\"\n",
        "    else :\n",
        "        return \"ê¸ì •ì…ë‹ˆë‹¤.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "### 3. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€í•˜ê¸° (Model Performance Evaluation)\n",
        "\n",
        "#### ğŸ’¡ ê°œë… (Concept)\n",
        "\n",
        "ëª¨ë¸ì´ ì˜ˆì¸¡ì„ ì–¼ë§ˆë‚˜ ì˜ ìˆ˜í–‰í–ˆëŠ”ì§€ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë‹¤ì–‘í•œ **í‰ê°€ ì§€í‘œ**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "* **ì •í™•ë„ (Accuracy)**: ì „ì²´ ì˜ˆì¸¡ ì¤‘ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨. ê°€ì¥ ì§ê´€ì ì´ì§€ë§Œ, ë°ì´í„°ì˜ í´ë˜ìŠ¤(ê¸ì •/ë¶€ì •)ê°€ ë¶ˆê· í˜•í•  ê²½ìš° ì„±ëŠ¥ì„ ì™œê³¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "* **ì •ë°€ë„ (Precision)**: ëª¨ë¸ì´ 'ê¸ì •'ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒë“¤ ì¤‘, ì‹¤ì œ 'ê¸ì •'ì¸ ë¦¬ë·°ì˜ ë¹„ìœ¨.\n",
        "* **ì¬í˜„ìœ¨ (Recall)**: ì‹¤ì œ 'ê¸ì •'ì¸ ë¦¬ë·°ë“¤ ì¤‘, ëª¨ë¸ì´ 'ê¸ì •'ìœ¼ë¡œ ì˜ˆì¸¡í•´ë‚¸ ë¹„ìœ¨.\n",
        "* **F1 ì ìˆ˜ (F1-Score)**: ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™” í‰ê· . ë‘ ì§€í‘œê°€ ëª¨ë‘ ì¤‘ìš”í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ê· í˜• ì¡íŒ ì§€í‘œì…ë‹ˆë‹¤.\n",
        "* **í˜¼ë™ í–‰ë ¬ (Confusion Matrix)**: ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‘œ í˜•íƒœë¡œ ì •ë¦¬í•œ ê²ƒ. True Positive, False Positive, True Negative, False Negative ê°’ì„ í†µí•´ ëª¨ë¸ì´ ì–´ë–¤ ë¶€ë¶„ì—ì„œ ì‹¤ìˆ˜ë¥¼ í•˜ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "#### ğŸ’» ì˜ˆì‹œ ì½”ë“œ (Example Code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "emo2y04PHAum"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.express as px\n",
        "\n",
        "# ê° í‰ê°€ ì§€í‘œ ê³„ì‚°\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì €ì¥ (ë‚˜ì¤‘ì— ë¹„êµ ì‹œê°í™”ë¥¼ ìœ„í•¨)\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
        "    'TF-IDF': [accuracy, precision, recall, f1]\n",
        "})\n",
        "\n",
        "print(\"--- TF-IDF ê¸°ë°˜ ëª¨ë¸ ì„±ëŠ¥ ---\")\n",
        "print(f\"ì •í™•ë„ (Accuracy): {accuracy:.4f}\")\n",
        "print(f\"ì •ë°€ë„ (Precision): {precision:.4f}\")\n",
        "print(f\"ì¬í˜„ìœ¨ (Recall): {recall:.4f}\")\n",
        "print(f\"F1 ì ìˆ˜ (F1 Score): {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "fig = ff.create_annotated_heatmap(\n",
        "    z=conf_matrix,\n",
        "    x=['ë¶€ì •(ì˜ˆì¸¡)', 'ê¸ì •(ì˜ˆì¸¡)'],\n",
        "    y=['ë¶€ì •(ì‹¤ì œ)', 'ê¸ì •(ì‹¤ì œ)'],\n",
        "    colorscale='Blues'\n",
        ")\n",
        "fig.update_layout(title_text='<b>í˜¼ë™ í–‰ë ¬ (Confusion Matrix) - TF-IDF</b>')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvR6eAcMHAum"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2. OpenAI ì„ë² ë”©ì„ ì´ìš©í•œ ìµœì‹  AI ê°ì„± ë¶„ì„\n",
        "\n",
        "### 4. OpenAI ì„ë² ë”©ìœ¼ë¡œ íŠ¹ì§• ë²¡í„°í™”í•˜ê¸°\n",
        "\n",
        "#### ğŸ’¡ ê°œë… (Concept)\n",
        "\n",
        "TF-IDFëŠ” ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì´ì§€ë§Œ, ë‹¨ì–´ì˜ 'ì˜ë¯¸' ìì²´ë¥¼ ì´í•´í•˜ì§€ëŠ” ëª»í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 'ì¬ë¯¸ìˆë‹¤'ì™€ 'í¥ë¯¸ë¡­ë‹¤'ëŠ” ì˜ë¯¸ê°€ ë¹„ìŠ·í•˜ì§€ë§Œ TF-IDFëŠ” ì´ë“¤ì„ ì™„ì „íˆ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì·¨ê¸‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "**í…ìŠ¤íŠ¸ ì„ë² ë”©(Text Embedding)**ì€ ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ê¸°ìˆ ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ì  ê´€ê³„ê°€ ë³´ì¡´ëœ ê³ ì°¨ì›ì˜ **ë°€ì§‘ ë²¡í„°(Dense Vector)**ë¡œ í‘œí˜„í•©ë‹ˆë‹¤. ì´ ë²¡í„° ê³µê°„ì—ì„œëŠ” ì˜ë¯¸ê°€ ë¹„ìŠ·í•œ ë‹¨ì–´ë‚˜ ë¬¸ì¥ë“¤ì´ ì„œë¡œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ì¡´ì¬í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ì—¬ê¸°ì„œëŠ” `OpenAI`ê°€ ì œê³µí•˜ëŠ” ì‚¬ì „ í•™ìŠµëœ(Pre-trained) ì„ë² ë”© ëª¨ë¸(`text-embedding-3-small`)ì„ ì‚¬ìš©í•˜ì—¬, ìš°ë¦¬ ë°ì´í„°ì˜ ê° ë¦¬ë·°ë¥¼ ê³ ìœ í•œ ì˜ë¯¸ ë²¡í„°ë¡œ ë³€í™˜í•˜ê² ìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ì–»ì€ ë²¡í„°ëŠ” ê·¸ ìì²´ë¡œ ë§¤ìš° ê°•ë ¥í•œ íŠ¹ì§•(Feature)ì´ ë˜ì–´ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "#### ğŸ’» ì˜ˆì‹œ ì½”ë“œ (Example Code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UPSEkXyHAum"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv()\n",
        "\n",
        "# ğŸš¨ ì¤‘ìš”: OpenAI API í‚¤ ì„¤ì •\n",
        "# ë³„ë„ì˜ ì„¤ì • íŒŒì¼(.env)ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.\n",
        "# https://platform.openai.com/api-keys ì—ì„œ í‚¤ë¥¼ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "# ë³¸ì¸ì˜ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n",
        "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "# client = openai.OpenAI() # í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYì— í‚¤ê°€ ë“±ë¡ë˜ì–´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì…ë ¥ì´ ë©ë‹ˆë‹¤.\n",
        "client.models.list() # API í‚¤ ìœ íš¨ì„± ê²€ì‚¬\n",
        "print(\"âœ… OpenAI API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
        "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
        "    # OpenAI ì •ì±…ì— ë”°ë¼ \\n ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜\n",
        "    text = str(text).replace(\"\\n\", \" \")\n",
        "    if not text.strip(): # ë¹„ì–´ìˆê±°ë‚˜ ê³µë°±ë§Œ ìˆëŠ” í…ìŠ¤íŠ¸ ì²˜ë¦¬\n",
        "        return None\n",
        "    try:\n",
        "        return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
        "    except Exception as e:\n",
        "        print(f\"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {text[:30]}... - {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì„ë² ë”© ì ìš©\n",
        "# # ì´ ê³¼ì •ì€ OpenAI APIë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ì¸í„°ë„· ì—°ê²°ì´ í•„ìš”í•˜ë©°, ë°ì´í„° ì–‘ì— ë”°ë¼ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "# print(\"í•™ìŠµ ë°ì´í„° ì„ë² ë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "# train_df['embedding'] = train_df['document_processed'].progress_apply(get_embedding)\n",
        "# print(\"\\ní…ŒìŠ¤íŠ¸ ë°ì´í„° ì„ë² ë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "# test_df['embedding'] = test_df['document_processed'].progress_apply(get_embedding)\n",
        "\n",
        "# # ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í•œ ê²½ìš° (ì˜ˆ: API ì˜¤ë¥˜, ë¹ˆ í…ìŠ¤íŠ¸) í•´ë‹¹ í–‰ ì œê±°\n",
        "# train_df_emb = train_df.dropna(subset=['embedding'])\n",
        "# test_df_emb = test_df.dropna(subset=['embedding'])\n",
        "\n",
        "# print(f\"ì„ë² ë”© ìƒì„± ì™„ë£Œ. {len(train_df) - len(train_df_emb)}ê°œì˜ í•™ìŠµ ë°ì´í„°, {len(test_df) - len(test_df_emb)}ê°œì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì €ì¥ëœ ì„ë² ë”© ë°ì´í„° ë¡œë“œ\n",
        "import pickle\n",
        "\n",
        "with open('../datasets/text/nsmc/train_df.pkl', 'rb') as f:\n",
        "    train_df = pickle.load(f)\n",
        "\n",
        "with open('../datasets/text/nsmc/test_df.pkl', 'rb') as f:\n",
        "    test_df = pickle.load(f)\n",
        "\n",
        "# ì„ë² ë”©ì´ ì—†ëŠ” í–‰ ì œê±°\n",
        "train_df_emb = train_df.dropna(subset=['embedding'])\n",
        "test_df_emb = test_df.dropna(subset=['embedding'])\n",
        "\n",
        "print(\"ì €ì¥ëœ ì„ë² ë”© ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
        "print(f\"ë¡œë“œëœ í•™ìŠµ ë°ì´í„° í¬ê¸°: {len(train_df_emb)}ê°œ\")\n",
        "print(f\"ë¡œë“œëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {len(test_df_emb)}ê°œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNDd-5qsHAum"
      },
      "source": [
        "---\n",
        "\n",
        "### 5. ì„ë² ë”© ë²¡í„°ë¡œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€í•˜ê¸°\n",
        "\n",
        "#### ğŸ’¡ ê°œë… (Concept)\n",
        "\n",
        "ì´ì œ ê° ë¦¬ë·°ëŠ” 1536ì°¨ì›(`text-embedding-3-small` ê¸°ì¤€)ì˜ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì„ë² ë”© ë²¡í„°ë¥¼ íŠ¹ì§•(X)ìœ¼ë¡œ, ê¸°ì¡´ì˜ ê¸ì •/ë¶€ì • ë ˆì´ë¸”(y)ì„ ì •ë‹µìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ê³¼ì—° ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•˜ëŠ” ì„ë² ë”© ë²¡í„°ê°€ TF-IDFë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤„ê¹Œìš”? ë™ì¼í•œ í‰ê°€ ì§€í‘œë¥¼ í†µí•´ ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì§ì ‘ ë¹„êµí•´ë´…ì‹œë‹¤.\n",
        "\n",
        "#### ğŸ’» ì˜ˆì‹œ ì½”ë“œ (Example Code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlBNvTFwHAum"
      },
      "outputs": [],
      "source": [
        "# 1. ì„ë² ë”© ë²¡í„°ì™€ ë ˆì´ë¸” ë¶„ë¦¬\n",
        "X_train_emb = np.array(train_df_emb['embedding'].tolist())\n",
        "y_train_emb = train_df_emb['label']\n",
        "X_test_emb = np.array(test_df_emb['embedding'].tolist())\n",
        "y_test_emb = test_df_emb['label']\n",
        "\n",
        "print(f\"í•™ìŠµ ë°ì´í„° (Embedding) í˜•íƒœ: {X_train_emb.shape}\")\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° (Embedding) í˜•íƒœ: {X_test_emb.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. ìƒˆë¡œìš´ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
        "lr_model_emb = LogisticRegression(random_state=42, C=5, max_iter=1000)\n",
        "lr_model_emb.fit(X_train_emb, y_train_emb)\n",
        "print(\"ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (OpenAI Embedding ê¸°ë°˜).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€\n",
        "y_pred_emb = lr_model_emb.predict(X_test_emb)\n",
        "\n",
        "accuracy_emb = accuracy_score(y_test_emb, y_pred_emb)\n",
        "precision_emb = precision_score(y_test_emb, y_pred_emb)\n",
        "recall_emb = recall_score(y_test_emb, y_pred_emb)\n",
        "f1_emb = f1_score(y_test_emb, y_pred_emb)\n",
        "\n",
        "# í‰ê°€ ê²°ê³¼ë¥¼ ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€\n",
        "metrics_df['OpenAI_Embedding'] = [accuracy_emb, precision_emb, recall_emb, f1_emb]\n",
        "\n",
        "print(\"--- OpenAI Embedding ê¸°ë°˜ ëª¨ë¸ ì„±ëŠ¥ ---\")\n",
        "print(metrics_df.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "conf_matrix_emb = confusion_matrix(y_test_emb, y_pred_emb)\n",
        "fig_emb = ff.create_annotated_heatmap(\n",
        "    z=conf_matrix_emb,\n",
        "    x=['ë¶€ì •(ì˜ˆì¸¡)', 'ê¸ì •(ì˜ˆì¸¡)'],\n",
        "    y=['ë¶€ì •(ì‹¤ì œ)', 'ê¸ì •(ì‹¤ì œ)'],\n",
        "    colorscale='Greens'\n",
        ")\n",
        "fig_emb.update_layout(title_text='<b>í˜¼ë™ í–‰ë ¬ (Confusion Matrix) - OpenAI Embedding</b>')\n",
        "fig_emb.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. ë‘ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”\n",
        "fig_compare = px.bar(\n",
        "    metrics_df,\n",
        "    x='Metric',\n",
        "    y=['TF-IDF', 'OpenAI_Embedding'],\n",
        "    barmode='group',\n",
        "    title='<b>TF-IDF vs OpenAI Embedding ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ</b>',\n",
        "    labels={'value': 'Score', 'variable': 'Model'},\n",
        "    text_auto='.4f'\n",
        ")\n",
        "fig_compare.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl2ykujcHAun"
      },
      "source": [
        "#### âœï¸ ì—°ìŠµ ë¬¸ì œ (Practice Problems)\n",
        "\n",
        "1.  OpenAIì—ëŠ” `text-embedding-3-small` ì™¸ì— `text-embedding-3-large`ì™€ ê°™ì€ ë” í° ëª¨ë¸ì´ ìˆìŠµë‹ˆë‹¤. `get_embedding` í•¨ìˆ˜ì—ì„œ ëª¨ë¸ ì´ë¦„ì„ ë³€ê²½í•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•˜ê³ , ì„±ëŠ¥ì´ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ ë¹„êµí•´ë³´ì„¸ìš”. (ì£¼ì˜: ë” í° ëª¨ë¸ì€ ë¹„ìš©ê³¼ ì‹œê°„ì´ ë” ë§ì´ ì†Œìš”ë©ë‹ˆë‹¤.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 200ê°œì˜ ë°ì´í„°ì…‹ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. (í•™ìŠµëª©ì ì— ë§ê²Œ ì‹œê°„ê³¼ ë¹„ìš©ì„ ì¤„ì´ê¸° ìœ„í•¨)\n",
        "train_200_df = train_df.sample(200)\n",
        "test_200_df = test_df.sample(200)\n",
        "train_200_df['large_embedding'] = train_200_df['document_processed'].progress_apply(lambda text: get_embedding(text, model=\"text-embedding-3-large\"))\n",
        "test_200_df['large_embedding'] = test_200_df['document_processed'].progress_apply(lambda text: get_embedding(text, model=\"text-embedding-3-large\"))\n",
        "\n",
        "train_200_df_emb = train_200_df.dropna(subset=['large_embedding'])\n",
        "test_200_df_emb = test_200_df.dropna(subset=['large_embedding'])\n",
        "\n",
        "print(f\"ë¡œë“œëœ í•™ìŠµ ë°ì´í„° í¬ê¸°: {len(train_200_df_emb)}ê°œ\")\n",
        "print(f\"ë¡œë“œëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {len(test_200_df_emb)}ê°œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "large_emb_lr_model = LogisticRegression(random_state=42, C=5, max_iter=1000)\n",
        "\n",
        "large_emb_lr_model.fit(train_200_df_emb['large_embedding'].tolist(), train_200_df_emb['label'])\n",
        "\n",
        "y_pred_large_emb = large_emb_lr_model.predict(test_200_df_emb['large_embedding'].tolist())\n",
        "\n",
        "print(f\"Large Embedding ëª¨ë¸ ì •í™•ë„: {accuracy_score(test_200_df_emb['label'], y_pred_large_emb)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì˜ `C` íŒŒë¼ë¯¸í„°ëŠ” ê·œì œ(Regularization)ì˜ ê°•ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤. `C` ê°’ì„ 1, 5, 10ìœ¼ë¡œ ë³€ê²½í•´ê°€ë©° ì„ë² ë”© ê¸°ë°˜ ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•˜ê³ , ê° ê²½ìš°ì˜ ì •í™•ë„ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”. `C` ê°’ì´ í´ìˆ˜ë¡ ê·œì œê°€ ì•½í•´ì§‘ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì½”ë“œ ì‘ì„±\n",
        "custom_large_emb_lr_models = {\n",
        "    f\"C{c}\": LogisticRegression(random_state=42, C=c, max_iter=1000)\n",
        "    for c in [1,5,10]\n",
        "}\n",
        "\n",
        "for c, model in custom_large_emb_lr_models.items():\n",
        "    model.fit(train_200_df_emb['large_embedding'].tolist(), train_200_df_emb['label'])\n",
        "    y_pred_large_emb = model.predict(test_200_df_emb['large_embedding'].tolist())\n",
        "    print(f\"Large Embedding ëª¨ë¸({c}) ì •í™•ë„: {accuracy_score(test_200_df_emb['label'], y_pred_large_emb)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3. ì œë¡œìƒ· ë¶„ë¥˜: ë ˆì´ë¸” ì„ë² ë”© ìœ ì‚¬ë„ í™œìš©\n",
        "\n",
        "### 6. ë ˆì´ë¸” ì˜ë¯¸ ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œ ì œë¡œìƒ·(Zero-Shot) ê°ì„± ë¶„ì„\n",
        "\n",
        "#### ğŸ’¡ ê°œë… (Concept)\n",
        "\n",
        "ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” (í…ìŠ¤íŠ¸ ë²¡í„°, ë ˆì´ë¸”) ìŒìœ¼ë¡œ êµ¬ì„±ëœ í•™ìŠµ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜ ëª¨ë¸(`LogisticRegression`)ì„ 'í•™ìŠµ'ì‹œì¼°ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ OpenAIì™€ ê°™ì€ ê°•ë ¥í•œ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´, ëª¨ë¸ì„ ì „í˜€ í•™ìŠµì‹œí‚¤ì§€ ì•Šê³ ë„ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” **ì œë¡œìƒ·(Zero-Shot) ë¶„ë¥˜**ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì•„ì´ë””ì–´ëŠ” ë§¤ìš° ì§ê´€ì ì…ë‹ˆë‹¤.\n",
        "1.  ë¶„ì„í•  ë¬¸ì¥ (ì˜ˆ: \"ë°°ìš°ë“¤ ì—°ê¸°ê°€ ì •ë§ í™˜ìƒì ì´ì—ˆì–´ìš”.\")ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ êµ¬í•©ë‹ˆë‹¤.\n",
        "2.  ìš°ë¦¬ê°€ ë¶„ë¥˜í•˜ê³  ì‹¶ì€ ë ˆì´ë¸”, ì¦‰ **\"ê¸ì •\"ì´ë¼ëŠ” ë‹¨ì–´ì™€ \"ë¶€ì •\"ì´ë¼ëŠ” ë‹¨ì–´ ìì²´ì˜ ì„ë² ë”© ë²¡í„°**ë¥¼ êµ¬í•©ë‹ˆë‹¤.\n",
        "3.  ë¶„ì„í•  ë¬¸ì¥ì˜ ë²¡í„°ê°€ 'ê¸ì •' ë²¡í„°ì™€ 'ë¶€ì •' ë²¡í„° ì¤‘ ì–´ëŠ ìª½ê³¼ ë” ê°€ê¹Œìš´ì§€(ìœ ì‚¬í•œì§€)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "4.  ë” ê°€ê¹Œìš´ ìª½ ë ˆì´ë¸”ë¡œ ë¬¸ì¥ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì´ ë°©ì‹ì€ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸”ì˜ 'ì˜ë¯¸'ë¥¼ ë²¡í„° ê³µê°„ì—ì„œ ì´í•´í•˜ê³  ìˆë‹¤ëŠ” ì „ì œ í•˜ì— ì‘ë™í•©ë‹ˆë‹¤. ë³„ë„ì˜ í•™ìŠµ ë°ì´í„°ê°€ í•„ìš” ì—†ê¸° ë•Œë¬¸ì— ìƒˆë¡œìš´ ë ˆì´ë¸”(ì˜ˆ: 'ì¤‘ë¦½')ì„ ì¶”ê°€í•˜ëŠ” ë° ë§¤ìš° ìœ ì—°í•˜ë‹¤ëŠ” ì—„ì²­ë‚œ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ë²¡í„° ê°„ì˜ ìœ ì‚¬ë„ëŠ” ì£¼ë¡œ **ì½”ì‚¬ì¸ ìœ ì‚¬ë„(Cosine Similarity)**ë¡œ ì¸¡ì •í•©ë‹ˆë‹¤. ë‘ ë²¡í„°ê°€ ì´ë£¨ëŠ” ê°ë„ì˜ ì½”ì‚¬ì¸ ê°’ìœ¼ë¡œ, ë°©í–¥ì´ ì™„ì „íˆ ê°™ìœ¼ë©´ 1, 90Â°ë¡œ ì§êµí•˜ë©´ 0, ì™„ì „íˆ ë°˜ëŒ€ ë°©í–¥ì´ë©´ -1ì˜ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.\n",
        "\n",
        "#### ğŸ’» ì˜ˆì‹œ ì½”ë“œ (Example Code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy.linalg import norm\n",
        "from numpy import dot\n",
        "\n",
        "# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜\n",
        "def cosine_similarity(A, B):\n",
        "    if A is None or B is None:\n",
        "        return -1 # ì˜¤ë¥˜ ë˜ëŠ” ìœ íš¨í•˜ì§€ ì•Šì€ ì„ë² ë”©\n",
        "    return dot(A, B) / (norm(A) * norm(B))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. ë¶„ë¥˜í•  ë ˆì´ë¸” ì •ì˜ ë° ì„ë² ë”©\n",
        "# \"ê¸ì •\", \"ë¶€ì •\" ë³´ë‹¤ ì¢€ ë” í’ë¶€í•œ í‘œí˜„ì´ ì„±ëŠ¥ì— ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "positive_label_text = \"ë§¤ìš° ê¸ì •ì ì´ê³  ì¢‹ì€ í‰ê°€\"\n",
        "negative_label_text = \"ë§¤ìš° ë¶€ì •ì ì´ê³  ë‚˜ìœ í‰ê°€\"\n",
        "\n",
        "positive_embedding = get_embedding(positive_label_text)\n",
        "negative_embedding = get_embedding(negative_label_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. ì œë¡œìƒ· ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "# í…ŒìŠ¤íŠ¸ì…‹ì˜ ê° ë¦¬ë·° ì„ë² ë”©ì— ëŒ€í•´ ë ˆì´ë¸” ì„ë² ë”©ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "y_pred_zero_shot = []\n",
        "for review_embedding in tqdm(X_test_emb, desc=\"Zero-Shot ì˜ˆì¸¡ ì¤‘\"):\n",
        "    sim_positive = cosine_similarity(review_embedding, positive_embedding)\n",
        "    sim_negative = cosine_similarity(review_embedding, negative_embedding)\n",
        "\n",
        "    if sim_positive > sim_negative:\n",
        "        y_pred_zero_shot.append(1) # ê¸ì •\n",
        "    else:\n",
        "        y_pred_zero_shot.append(0) # ë¶€ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. ì œë¡œìƒ· ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€\n",
        "accuracy_zs = accuracy_score(y_test_emb, y_pred_zero_shot)\n",
        "precision_zs = precision_score(y_test_emb, y_pred_zero_shot)\n",
        "recall_zs = recall_score(y_test_emb, y_pred_zero_shot)\n",
        "f1_zs = f1_score(y_test_emb, y_pred_zero_shot)\n",
        "\n",
        "# í‰ê°€ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€\n",
        "metrics_df['OpenAI_Zero_Shot'] = [accuracy_zs, precision_zs, recall_zs, f1_zs]\n",
        "\n",
        "print(\"--- OpenAI Zero-Shot ê¸°ë°˜ ëª¨ë¸ ì„±ëŠ¥ ---\")\n",
        "print(metrics_df.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "positive_large_embedding = get_embedding(positive_label_text, model='text-embedding-3-large')\n",
        "negative_large_embedding = get_embedding(negative_label_text, model='text-embedding-3-large')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_large_model_zero_shot = []\n",
        "for review_embedding in tqdm(test_200_df_emb['large_embedding'].tolist(), desc=\"Zero-Shot ì˜ˆì¸¡ ì¤‘\"):\n",
        "    sim_positive = cosine_similarity(review_embedding, positive_large_embedding)\n",
        "    sim_negative = cosine_similarity(review_embedding, negative_large_embedding)\n",
        "\n",
        "    if sim_positive > sim_negative:\n",
        "        y_pred_large_model_zero_shot.append(1) # ê¸ì •\n",
        "    else:\n",
        "        y_pred_large_model_zero_shot.append(0) # ë¶€ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. ì œë¡œìƒ· ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€\n",
        "accuracy_zs = accuracy_score(test_200_df_emb['label'].tolist(), y_pred_large_model_zero_shot)\n",
        "precision_zs = precision_score(test_200_df_emb['label'].tolist(), y_pred_large_model_zero_shot)\n",
        "recall_zs = recall_score(test_200_df_emb['label'].tolist(), y_pred_large_model_zero_shot)\n",
        "f1_zs = f1_score(test_200_df_emb['label'].tolist(), y_pred_large_model_zero_shot)\n",
        "\n",
        "# í‰ê°€ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€\n",
        "metrics_df['OpenAI_Large_Model_Zero_Shot'] = [accuracy_zs, precision_zs, recall_zs, f1_zs]\n",
        "\n",
        "print(\"--- OpenAI Zero-Shot ê¸°ë°˜ ëª¨ë¸ ì„±ëŠ¥ ---\")\n",
        "print(metrics_df.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "conf_matrix_zs = confusion_matrix(y_test_emb, y_pred_zero_shot)\n",
        "fig_zs = ff.create_annotated_heatmap(\n",
        "    z=conf_matrix_zs,\n",
        "    x=['ë¶€ì •(ì˜ˆì¸¡)', 'ê¸ì •(ì˜ˆì¸¡)'],\n",
        "    y=['ë¶€ì •(ì‹¤ì œ)', 'ê¸ì •(ì‹¤ì œ)'],\n",
        "    colorscale='Oranges'\n",
        ")\n",
        "fig_zs.update_layout(title_text='<b>í˜¼ë™ í–‰ë ¬ (Confusion Matrix) - OpenAI Zero-Shot</b>')\n",
        "fig_zs.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. ì„¸ ê°€ì§€ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”\n",
        "fig_compare_all = px.bar(\n",
        "    metrics_df,\n",
        "    x='Metric',\n",
        "    y=['TF-IDF', 'OpenAI_Embedding', 'OpenAI_Zero_Shot'],\n",
        "    barmode='group',\n",
        "    title='<b>ì„¸ ê°€ì§€ ë°©ì‹ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ</b>',\n",
        "    labels={'value': 'Score', 'variable': 'Model'},\n",
        "    text_auto='.4f'\n",
        ")\n",
        "fig_compare_all.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### âœï¸ ì—°ìŠµ ë¬¸ì œ (Practice Problems)\n",
        "\n",
        "1.  ë ˆì´ë¸”ì˜ í‘œí˜„ ë°©ì‹ì€ ì œë¡œìƒ· ë¶„ë¥˜ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `positive_label_text`ë¥¼ 'ì•„ì£¼ ì¬ë¯¸ìˆê³  í›Œë¥­í•œ ì˜í™”'ì™€ ê°™ì´ ë” êµ¬ì²´ì ì´ê³  ê°ì •ì´ í’ë¶€í•œ í‘œí˜„ìœ¼ë¡œ ë°”ê¿”ë³´ì„¸ìš”. ë¶€ì • ë ˆì´ë¸”ë„ 'ì§€ë£¨í•˜ê³  ì‹¤ë§ìŠ¤ëŸ¬ìš´ ì˜í™”' ë“±ìœ¼ë¡œ ë°”ê¾¸ì–´ ì„±ëŠ¥ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìƒˆë¡œìš´ ë ˆì´ë¸” ì •ì˜\n",
        "positive_label_enhanced = \"ì•„ì£¼ ì¬ë¯¸ìˆê³  í›Œë¥­í•œ ì˜í™”\"\n",
        "negative_label_enhanced = \"ì§€ë£¨í•˜ê³  ì‹¤ë§ìŠ¤ëŸ¬ìš´ ì˜í™”\"\n",
        "\n",
        "# í–¥ìƒëœ ë ˆì´ë¸”ì˜ ì„ë² ë”© ê³„ì‚°\n",
        "positive_embedding_enhanced = get_embedding(positive_label_enhanced)\n",
        "negative_embedding_enhanced = get_embedding(negative_label_enhanced)\n",
        "\n",
        "# í–¥ìƒëœ ë ˆì´ë¸”ë¡œ ì œë¡œìƒ· ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "y_pred_enhanced_zero_shot = []\n",
        "\n",
        "print(\"í–¥ìƒëœ ë ˆì´ë¸”ë¡œ Zero-Shot ì˜ˆì¸¡ ì¤‘...\")\n",
        "for embedding in tqdm(test_200_df_emb['embedding'].tolist(), desc=\"í–¥ìƒëœ Zero-Shot ì˜ˆì¸¡ ì¤‘\"):\n",
        "    # ê° ë ˆì´ë¸”ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "    pos_similarity = cosine_similarity(embedding, positive_embedding_enhanced)\n",
        "    neg_similarity = cosine_similarity(embedding, negative_embedding_enhanced)\n",
        "    \n",
        "    # ë” ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ ë ˆì´ë¸”ë¡œ ì˜ˆì¸¡\n",
        "    if pos_similarity > neg_similarity:\n",
        "        y_pred_enhanced_zero_shot.append(1)  # ê¸ì •\n",
        "    else:\n",
        "        y_pred_enhanced_zero_shot.append(0)  # ë¶€ì •\n",
        "\n",
        "# í–¥ìƒëœ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
        "accuracy_enhanced = accuracy_score(test_200_df_emb['label'].tolist(), y_pred_enhanced_zero_shot)\n",
        "precision_enhanced = precision_score(test_200_df_emb['label'].tolist(), y_pred_enhanced_zero_shot)\n",
        "recall_enhanced = recall_score(test_200_df_emb['label'].tolist(), y_pred_enhanced_zero_shot)\n",
        "f1_enhanced = f1_score(test_200_df_emb['label'].tolist(), y_pred_enhanced_zero_shot)\n",
        "\n",
        "# ê¸°ì¡´ ê²°ê³¼ì™€ ë¹„êµ\n",
        "print(\"--- ë ˆì´ë¸” ê°œì„  ì „í›„ ì„±ëŠ¥ ë¹„êµ ---\")\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Original_Labels': [accuracy_zs, precision_zs, recall_zs, f1_zs],\n",
        "    'Enhanced_Labels': [accuracy_enhanced, precision_enhanced, recall_enhanced, f1_enhanced]\n",
        "})\n",
        "print(comparison_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ì„±ëŠ¥ ê°œì„  ì‹œê°í™”\n",
        "fig_enhancement = px.bar(\n",
        "    comparison_df,\n",
        "    x='Metric',\n",
        "    y=['Original_Labels', 'Enhanced_Labels'],\n",
        "    barmode='group',\n",
        "    title='<b>ë ˆì´ë¸” ê°œì„  ì „í›„ Zero-Shot ì„±ëŠ¥ ë¹„êµ</b>',\n",
        "    labels={'value': 'Score', 'variable': 'Label Type'},\n",
        "    text_auto='.4f'\n",
        ")\n",
        "fig_enhancement.show()\n",
        "\n",
        "# í˜¼ë™ í–‰ë ¬ ë¹„êµ\n",
        "conf_matrix_enhanced = confusion_matrix(test_200_df_emb['label'].tolist(), y_pred_enhanced_zero_shot)\n",
        "fig_enhanced_cm = ff.create_annotated_heatmap(\n",
        "    z=conf_matrix_enhanced,\n",
        "    x=['ë¶€ì •(ì˜ˆì¸¡)', 'ê¸ì •(ì˜ˆì¸¡)'],\n",
        "    y=['ë¶€ì •(ì‹¤ì œ)', 'ê¸ì •(ì‹¤ì œ)'],\n",
        "    colorscale='Greens'\n",
        ")\n",
        "fig_enhanced_cm.update_layout(title_text='<b>í˜¼ë™ í–‰ë ¬ - í–¥ìƒëœ ë ˆì´ë¸” Zero-Shot</b>')\n",
        "fig_enhanced_cm.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  í˜„ì¬ëŠ” 'ê¸ì •'/'ë¶€ì •'ì˜ ì´ì§„ ë¶„ë¥˜ë§Œ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ì´ ì œë¡œìƒ· ê¸°ë²•ì„ 'ì½”ë¯¸ë””', 'ì•¡ì…˜', 'ë¡œë§¨ìŠ¤', 'ê³µí¬'ì™€ ê°™ì€ ë‹¤ì¤‘ ì¥ë¥´ ë¶„ë¥˜ ë¬¸ì œì— ì ìš©í•˜ëŠ” í•¨ìˆ˜ `predict_genre_zero_shot(text)`ë¥¼ ë§Œë“¤ì–´ ë³´ì„¸ìš”. ì´ í•¨ìˆ˜ëŠ” ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„, 4ê°œì˜ ì¥ë¥´ ë ˆì´ë¸”ê³¼ì˜ ì„ë² ë”© ìœ ì‚¬ë„ë¥¼ ê°ê° ê³„ì‚°í•˜ê³  ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ì¥ë¥´ë¥¼ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì—°ìŠµ ë¬¸ì œ 2: ë‹¤ì¤‘ ì¥ë¥´ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì œë¡œìƒ· í•¨ìˆ˜ êµ¬í˜„\n",
        "\n",
        "def predict_genre_zero_shot(text):\n",
        "    \"\"\"\n",
        "    ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ì¥ë¥´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì œë¡œìƒ· ë¶„ë¥˜ í•¨ìˆ˜\n",
        "    \n",
        "    Args:\n",
        "        text (str): ë¶„ë¥˜í•  ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸\n",
        "        \n",
        "    Returns:\n",
        "        str: ì˜ˆì¸¡ëœ ì¥ë¥´ ('ì½”ë¯¸ë””', 'ì•¡ì…˜', 'ë¡œë§¨ìŠ¤', 'ê³µí¬')\n",
        "    \"\"\"\n",
        "    if not client:\n",
        "        return \"ì˜¤ë¥˜: OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\"\n",
        "    \n",
        "    # ì¥ë¥´ ë ˆì´ë¸” ì •ì˜\n",
        "    genre_labels = {\n",
        "        'ì½”ë¯¸ë””': 'ì¬ë¯¸ìˆê³  ìœ ë¨¸ëŸ¬ìŠ¤í•œ ì½”ë¯¸ë”” ì˜í™”',\n",
        "        'ì•¡ì…˜': 'ìŠ¤ë¦´ ë„˜ì¹˜ê³  ë°•ì§„ê° ìˆëŠ” ì•¡ì…˜ ì˜í™”', \n",
        "        'ë¡œë§¨ìŠ¤': 'ê°ë™ì ì´ê³  ë¡œë§¨í‹±í•œ ì‚¬ë‘ ì˜í™”',\n",
        "        'ê³µí¬': 'ë¬´ì„­ê³  ê¸´ì¥ê° ë„˜ì¹˜ëŠ” ê³µí¬ ì˜í™”'\n",
        "    }\n",
        "    \n",
        "    # ì…ë ¥ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "    processed_text = kiwi_preprocess(text)\n",
        "    \n",
        "    # ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ê³„ì‚°\n",
        "    text_embedding = get_embedding(processed_text)\n",
        "    if text_embedding is None:\n",
        "        return \"ì˜¤ë¥˜: í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨\"\n",
        "    \n",
        "    # ê° ì¥ë¥´ ë ˆì´ë¸”ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "    similarities = {}\n",
        "    for genre, label_text in genre_labels.items():\n",
        "        label_embedding = get_embedding(label_text)\n",
        "        if label_embedding is not None:\n",
        "            similarity = cosine_similarity(text_embedding, label_embedding)\n",
        "            similarities[genre] = similarity\n",
        "    \n",
        "    # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ ì¥ë¥´ ë°˜í™˜\n",
        "    if similarities:\n",
        "        predicted_genre = max(similarities, key=similarities.get)\n",
        "        return predicted_genre\n",
        "    else:\n",
        "        return \"ì˜¤ë¥˜: ì¥ë¥´ ì˜ˆì¸¡ ì‹¤íŒ¨\"\n",
        "\n",
        "# í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\n",
        "test_reviews = [\n",
        "    \"ì •ë§ ì›ƒê²¨ì„œ ë°°ê¼½ ë¹ ì§ˆ ë»”í–ˆì–´ìš”. ê°œê·¸ë§¨ë“¤ì˜ ì—°ê¸°ê°€ ë„ˆë¬´ ì¬ë°Œì—ˆìŠµë‹ˆë‹¤.\",\n",
        "    \"ì•¡ì…˜ ì¥ë©´ì´ ì •ë§ ë°•ì§„ê° ë„˜ì¹˜ê³  ìŠ¤í„´íŠ¸ê°€ ëŒ€ë‹¨í–ˆì–´ìš”. ì†ì— ë•€ì„ ì¥ê²Œ í•˜ëŠ” ì˜í™”ì…ë‹ˆë‹¤.\",\n",
        "    \"ë‘ ì£¼ì¸ê³µì˜ ì‚¬ë‘ ì´ì•¼ê¸°ê°€ ë„ˆë¬´ ê°ë™ì ì´ì—ˆì–´ìš”. ëˆˆë¬¼ì´ ë‚  ì •ë„ë¡œ ì•„ë¦„ë‹¤ìš´ ë¡œë§¨ìŠ¤ì˜€ìŠµë‹ˆë‹¤.\",\n",
        "    \"ì •ë§ ë¬´ì„œì›Œì„œ ì ì„ ëª» ì˜ ê²ƒ ê°™ì•„ìš”. ê·€ì‹ ì´ ë‚˜ì˜¤ëŠ” ì¥ë©´ì—ì„œ ì†Œë¦¬ë¥¼ ì§ˆë €ìŠµë‹ˆë‹¤.\"\n",
        "]\n",
        "\n",
        "print(\"ë‹¤ì¤‘ ì¥ë¥´ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\")\n",
        "print(\"-\" * 50)\n",
        "for i, review in enumerate(test_reviews, 1):\n",
        "    predicted_genre = predict_genre_zero_shot(review)\n",
        "    print(f\"{i}. ë¦¬ë·°: {review}\")\n",
        "    print(f\"   ì˜ˆì¸¡ ì¥ë¥´: {predicted_genre}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "### 7. ğŸš€ ìµœì¢… ì‹¤ìŠµ ê³¼ì œ: ë‚˜ë§Œì˜ ê°ì„± ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì™„ì„±í•˜ê¸°\n",
        "\n",
        "ì§€ê¸ˆê¹Œì§€ ë°°ìš´ `Kiwipiepy` ì „ì²˜ë¦¬, TF-IDF, ì§€ë„ í•™ìŠµ ê¸°ë°˜ ì„ë² ë”© ëª¨ë¸, ê·¸ë¦¬ê³  ì œë¡œìƒ· ë¶„ë¥˜ê¹Œì§€ ëª¨ë‘ ì¢…í•©í•˜ì—¬, ìƒˆë¡œìš´ ì˜í™” ë¦¬ë·°ì— ëŒ€í•œ ê°ì„±ì„ ì˜ˆì¸¡í•˜ëŠ” ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•´ë´…ì‹œë‹¤.\n",
        "\n",
        "ì•„ë˜ `predict_sentiment_pipeline` í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”. ì´ í•¨ìˆ˜ëŠ” `method` ì¸ìì— ë”°ë¼ 'tfidf', 'openai_logistic', 'openai_zero_shot' ì„¸ ê°€ì§€ ë°©ì‹ì„ ì„ íƒí•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì´ ì…€ì„ ì‹¤í–‰í•˜ê¸° ì „ì—, ìœ„ì—ì„œ ëª¨ë“  ëª¨ë¸ê³¼ ë²¡í„°ë¼ì´ì €ê°€ ì¤€ë¹„ë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
        "\n",
        "# ì œë¡œìƒ· ì˜ˆì¸¡ì„ ìœ„í•´ ë ˆì´ë¸” ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°í•´ ë‘¡ë‹ˆë‹¤.\n",
        "positive_label_text = \"ë§¤ìš° ê¸ì •ì ì´ê³  ì¢‹ì€ í‰ê°€\"\n",
        "negative_label_text = \"ë§¤ìš° ë¶€ì •ì ì´ê³  ë‚˜ìœ í‰ê°€\"\n",
        "positive_embedding = get_embedding(positive_label_text)\n",
        "negative_embedding = get_embedding(negative_label_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr_model = LogisticRegression(random_state=42, C=5, max_iter=1000)\n",
        "\n",
        "lr_model.fit(train_200_df_emb['large_embedding'].tolist(), train_200_df_emb['label'].tolist())\n",
        "\n",
        "y_pred_large_model = lr_model.predict(test_200_df_emb['large_embedding'].tolist())\n",
        "\n",
        "print(f\"Large Embedding ëª¨ë¸ ì •í™•ë„: {accuracy_score(test_200_df_emb['label'].tolist(), y_pred_large_model)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_sentiment_pipeline(review_text, method='openai_zero_shot'):\n",
        "    \"\"\"\n",
        "    ìƒˆë¡œìš´ ì˜í™” ë¦¬ë·°ì˜ ê°ì„±ì„ ì˜ˆì¸¡í•˜ëŠ” íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜\n",
        "\n",
        "    Args:\n",
        "        review_text (str): ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•  ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸.\n",
        "        method (str): 'tfidf', 'openai_logistic', 'openai_zero_shot' ì¤‘ ì‚¬ìš©í•  ì˜ˆì¸¡ ë°©ì‹ì„ ì„ íƒ.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (ì˜ˆì¸¡ ë ˆì´ë¸”(0 or 1), ì˜ˆì¸¡ ê²°ê³¼('ê¸ì •' or 'ë¶€ì •'))\n",
        "    \"\"\"\n",
        "    # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "    preprocessed_text = kiwi_preprocess(review_text)\n",
        "\n",
        "    if method == 'tfidf':\n",
        "        if 'lr_model' not in globals(): return None, \"ì˜¤ë¥˜: TF-IDF ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\"\n",
        "        vectorized_text = tfidf_vectorizer.transform([preprocessed_text])\n",
        "        prediction = lr_model.predict(vectorized_text)\n",
        "\n",
        "    elif method == 'openai_logistic':\n",
        "        if 'lr_model_emb' not in globals(): return None, \"ì˜¤ë¥˜: ì§€ë„í•™ìŠµ ì„ë² ë”© ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\"\n",
        "        embedding_vector = get_embedding(preprocessed_text)\n",
        "        if embedding_vector is None: return None, \"ì˜¤ë¥˜: ì„ë² ë”© ìƒì„± ì‹¤íŒ¨\"\n",
        "        vectorized_text = np.array([embedding_vector])\n",
        "        prediction = lr_model_emb.predict(vectorized_text)\n",
        "\n",
        "    elif method == 'openai_zero_shot':\n",
        "        if not all([positive_embedding, negative_embedding]): return None, \"ì˜¤ë¥˜: ë ˆì´ë¸” ì„ë² ë”©ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\"\n",
        "        review_embedding = get_embedding(preprocessed_text)\n",
        "        if review_embedding is None: return None, \"ì˜¤ë¥˜: ë¦¬ë·° ì„ë² ë”© ìƒì„± ì‹¤íŒ¨\"\n",
        "\n",
        "        sim_pos = cosine_similarity(review_embedding, positive_embedding)\n",
        "        sim_neg = cosine_similarity(review_embedding, negative_embedding)\n",
        "\n",
        "        prediction = np.array([1 if sim_pos > sim_neg else 0])\n",
        "\n",
        "    else:\n",
        "        return None, \"ì˜¤ë¥˜: 'tfidf', 'openai_logistic', 'openai_zero_shot' ë°©ì‹ì„ ì„ íƒí•´ì£¼ì„¸ìš”.\"\n",
        "\n",
        "    result_text = 'ê¸ì •' if prediction[0] == 1 else 'ë¶€ì •'\n",
        "    return prediction[0], result_text\n",
        "\n",
        "# ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œëœ í›„ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”.\n",
        "if 'lr_model' in globals() and 'lr_model_emb' in globals():\n",
        "    my_review1 = \"ì´ ì˜í™”ëŠ” ì •ë§ ì‹œê°„ ê°€ëŠ” ì¤„ ëª¨ë¥´ê³  ë´¤ë„¤ìš”. ë°°ìš°ë“¤ ì—°ê¸°ê°€ ì¼í’ˆì…ë‹ˆë‹¤!\"\n",
        "    my_review2 = \"ê¸°ëŒ€í•˜ê³  ë´¤ëŠ”ë° ë„ˆë¬´ ì§€ë£¨í–ˆì–´ìš”. ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ë»”í•©ë‹ˆë‹¤.\"\n",
        "\n",
        "    print(\"--- ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ---\")\n",
        "    label, text = predict_sentiment_pipeline(my_review1, method='tfidf')\n",
        "    print(f\"ë¦¬ë·°: '{my_review1}' (TF-IDF)\\n>> ì˜ˆì¸¡: {text} ({label})\\n\")\n",
        "\n",
        "    label, text = predict_sentiment_pipeline(my_review1, method='openai_logistic')\n",
        "    print(f\"ë¦¬ë·°: '{my_review1}' (ì§€ë„í•™ìŠµ ì„ë² ë”©)\\n>> ì˜ˆì¸¡: {text} ({label})\\n\")\n",
        "\n",
        "    label, text = predict_sentiment_pipeline(my_review1, method='openai_zero_shot')\n",
        "    print(f\"ë¦¬ë·°: '{my_review1}' (ì œë¡œìƒ· ì„ë² ë”©)\\n>> ì˜ˆì¸¡: {text} ({label})\\n\")\n",
        "\n",
        "    label, text = predict_sentiment_pipeline(my_review2, method='openai_zero_shot')\n",
        "    print(f\"ë¦¬ë·°: '{my_review2}' (ì œë¡œìƒ· ì„ë² ë”©)\\n>> ì˜ˆì¸¡: {text} ({label})\")\n",
        "else:\n",
        "    print(\"ğŸš¨ ëª¨ë¸ì´ ì•„ì§ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ì „ ì½”ë“œ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuMIswhhHAun"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
