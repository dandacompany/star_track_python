## 경사하강법과 로지스틱 회귀

머신러닝 모델을 '학습'시킨다는 것은 결국 **손실(오차)을 최소화**하는 최적의 파라미터를 찾는 과정입니다. 이 과정의 핵심 엔진이 바로 **경사하강법(Gradient Descent)**입니다.

가장 단순한 모델인 **단순 선형 회귀**의 구체적인 숫자 예시를 통해 경사하강법이 어떻게 작동하는지 알아보고, 이 개념을 **로지스틱 회귀**의 확률적 분류 문제로 확장해 보겠습니다.

---

### 1. 단순 선형 회귀: "가장 잘 맞는 직선" 찾아가기

**단순 선형 회귀**는 입력값 $x$와 출력값 $y$ 사이의 관계를 $y = wx + b$ 형태의 직선으로 모델링합니다. 여기서 $w$는 기울기, $b$는 절편입니다.

**경사하강법**은 이 직선을 데이터에 가장 잘 맞추기 위해, 오차(예측값과 실제값의 차이)를 가장 작게 만드는 $w, b$를 찾는 과정입니다.

#### 경사하강법의 핵심 아이디어

1. **손실함수(Loss Function) 정의**: 오차가 얼마나 큰지 측정합니다. (선형 회귀에서는 주로 **평균제곱오차(MSE)** 사용)
2. **기울기(Gradient) 계산**: 손실함수를 각 파라미터($w, b$)로 미분하여, 현재 위치에서 손실이 가장 가파르게 증가하는 방향(기울기)을 계산합니다.
3. **파라미터 업데이트**: 기울기의 **반대 방향**으로 파라미터를 조금씩 이동시킵니다. 이 과정을 반복하면 결국 손실이 가장 낮은 지점(최적의 $w, b$)에 도달하게 됩니다.

#### 손실함수: 평균제곱오차 (Mean Squared Error, MSE)

$$
L(w, b) = \frac{1}{n} \sum_{i=1}^n (y_i - (wx_i + b))^2
$$

- $y_i$: 실제 정답 값
- $wx_i + b$: 모델의 예측값
- $n$: 데이터 개수

#### 예시 데이터로 직접 계산해보기

- **데이터**: `(x, y)` = `(1, 2)`, `(2, 4)`
- **초기 파라미터**: $w = 0$, $b = 0$
- **학습률**: $\alpha = 0.1$ (파라미터를 업데이트할 보폭)

**1. 최초 예측 및 손실 계산**

- 첫 번째 예측값: $\hat{y}_1 = 0 \times 1 + 0 = 0$
- 두 번째 예측값: $\hat{y}_2 = 0 \times 2 + 0 = 0$
- **현재 손실 (MSE)**:
  $L(0, 0) = \frac{1}{2} \left[ (2-0)^2 + (4-0)^2 \right] = \frac{1}{2} (4 + 16) = \mathbf{10}$

**2. 기울기(Gradient) 계산**

손실함수를 $w$와 $b$에 대해 각각 편미분하여 기울기를 구합니다.

- $\frac{\partial L}{\partial w} = \frac{1}{n} \sum -2x_i (y_i - (wx_i + b))$
- $\frac{\partial L}{\partial b} = \frac{1}{n} \sum -2(y_i - (wx_i + b))$

초기값 $w=0, b=0$을 대입하면:

- $\frac{\partial L}{\partial w} = \frac{1}{2} [ -2 \times 1 \times (2-0) + -2 \times 2 \times (4-0) ] = \frac{1}{2} [ -4 -16 ] = \mathbf{-10}$
- $\frac{\partial L}{\partial b} = \frac{1}{2} [ -2 \times (2-0) + -2 \times (4-0) ] = \frac{1}{2} [ -4 -8 ] = \mathbf{-6}$

**3. 파라미터 업데이트**

"새 파라미터 = 현재 파라미터 - 학습률 × 기울기" 공식을 따릅니다.

- $w_{\text{new}} = 0 - 0.1 \times (-10) = \mathbf{1}$
- $b_{\text{new}} = 0 - 0.1 \times (-6) = \mathbf{0.6}$

**4. 업데이트 후 손실 확인**

업데이트된 $w=1, b=0.6$으로 다시 손실을 계산해봅시다.

- 첫 번째 예측값: $1 \times 1 + 0.6 = 1.6$
- 두 번째 예측값: $1 \times 2 + 0.6 = 2.6$
- **새로운 손실 (MSE)**:
  $L(1, 0.6) = \frac{1}{2} \left[ (2-1.6)^2 + (4-2.6)^2 \right] = \frac{1}{2} (0.16 + 1.96) = \mathbf{1.06}$

> 단 한 번의 업데이트만으로 손실이 **10**에서 **1.06**으로 크게 줄었습니다! 이 과정을 반복하면 $w$는 2에, $b$는 0에 가까워지며 손실은 0으로 수렴할 것입니다. 이것이 경사하강법의 작동 원리입니다.

---

### 2. 로지스틱 회귀: 분류 문제는 어떻게 접근할까?

선형 회귀가 연속적인 값을 예측하는 '회귀' 문제였다면, 로지스틱 회귀는 "A인가 B인가?"를 판단하는 '분류' 문제입니다. (e.g., 스팸 메일인가 아닌가, 대출 승인인가 거절인가)

이를 위해 모델은 예측값을 $y=wx+b$처럼 쭉 뻗어나가는 직선이 아니라, **0과 1 사이의 '확률'**로 출력해야 합니다. 여기서 시그모이드 함수 $\sigma(\eta) = \frac{1}{1+e^{-\eta}}$가 등장하며, 모델의 최종 식은 다음과 같습니다.

$$
P(Y=1\mid\mathbf x)=\sigma\!\bigl(\beta_0+\beta_1x_1+\dots+\beta_kx_k\bigr)
$$

이때 손실 함수로 MSE를 그대로 사용하면 몇 가지 문제가 발생하여, 통계학의 **최대우도추정법(Maximum Likelihood Estimation, MLE)** 이라는 아이디어에서 새로운 손실 함수를 유도합니다.

#### 최대우도: "이 데이터가 나올 확률을 최대로!"

경사하강법이 "오차를 최소화"하는 관점이라면, 최대우도는 **"현재 주어진 데이터가 관측될 확률을 가장 높이는 파라미터 $\boldsymbol\beta$를 찾자"** 는 관점입니다.

예를 들어, 실제 정답이 `(1, 0, 1)`일 때, 모델 A가 `(0.9, 0.1, 0.8)`의 확률을 예측하고 모델 B가 `(0.6, 0.7, 0.5)`를 예측했다면, 실제 정답이 나올 확률을 더 높게 예측한 모델 A를 "더 좋은 모델"이라고 보는 것이 합리적입니다.

이 "전체 데이터가 나올 확률"을 식으로 표현한 것이 **우도(Likelihood) 함수**입니다.

$$
L(\boldsymbol\beta)=\prod_{i=1}^{n}p_i^{\,y_i}\,(1-p_i)^{1-y_i}
$$

- $y_i=1$이면 $p_i$가, $y_i=0$이면 $(1-p_i)$가 선택되어 모든 데이터에 대한 확률의 곱이 계산됩니다.

#### 손실 함수로의 변환: 로그우도와 교차 엔트로피

곱셈은 미분하기 복잡하므로, 양변에 로그를 취해 **로그우도(Log-Likelihood)** 로 바꿉니다. 로그를 씌워도 값이 최대가 되는 지점은 변하지 않습니다.

$$
\ell(\boldsymbol\beta)=\sum_{i=1}^{n}\Bigl[y_i\log p_i+(1-y_i)\log(1-p_i)\Bigr]
$$

이제 다 왔습니다. 경사하강법은 '최소화'를 하는 알고리즘이므로, **로그우도를 최대화** 하는 대신 **음수(-) 로그우도를 최소화**하도록 목표를 바꿀 수 있습니다.

- **손실함수 $J(\boldsymbol\beta) = -\ell(\boldsymbol\beta)$**
  $$
  J(\boldsymbol\beta) = \sum_{i=1}^{n}\Bigl[-y_i\log p_i-(1-y_i)\log(1-p_i)\Bigr]
  $$

이 식이 바로 머신러닝에서 널리 쓰이는 **이진 교차 엔트로피(Binary Cross-Entropy) 손실 함수**입니다.

---

### 3. 결론: 결국은 같은 길

- **선형 회귀**에서는 **평균제곱오차(MSE)** 라는 손실 함수를 최소화하기 위해 경사하강법을 사용했습니다.
- **로지스틱 회귀**에서는 **최대우도**라는 원칙으로부터 **교차 엔트로피**라는 손실 함수를 유도했고, 이 손실을 최소화하기 위해 **똑같이 경사하강법을 사용합니다.**

즉, 풀려는 문제(회귀 vs. 분류)에 맞춰 가장 적절한 **손실 함수**를 정의하고 나면, 그 손실을 줄여나가는 최적화 과정은 경사하강법이라는 동일한 원리로 이루어집니다. 이 "데이터 → 손실 → 최적화" 파이프라인이 바로 머신러닝의 핵심입니다.
