#!/usr/bin/env python3
"""
ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°ì…‹ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- ë¶„ë¥˜ ë¬¸ì œ í•´ê²°
- ë‹¤ì–‘í•œ ëª¨ë¸ ë¹„êµ (Tree ê³„ì—´, LogReg, ì•™ìƒë¸”)
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (RandomSearchCV)
- ì‹œê°í™” (Plotly)
- ëª¨ë¸ ì €ì¥ (Pickle)
"""-

import pandas as pd
import numpy as np
import pickle
import os
from datetime import datetime

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.pipeline import Pipeline

# ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.figure_factory as ff

# ê²½ê³  ë¬´ì‹œ
import warnings
warnings.filterwarnings('ignore')

class IrisAnalysis:
    def __init__(self, data_path='datasets/Iris.csv'):
        """
        ì•„ì´ë¦¬ìŠ¤ ë¶„ì„ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        """
        self.data_path = data_path
        self.df = None
        self.X = None
        self.y = None
        self.X_train = None
        self.X_val = None
        self.X_test = None
        self.y_train = None
        self.y_val = None
        self.y_test = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.models = {}
        self.best_models = {}
        self.results = {}
        
        # ê²°ê³¼ ì €ì¥ í´ë” ìƒì„±
        os.makedirs('models', exist_ok=True)
        os.makedirs('visualizations', exist_ok=True)
        
    def load_and_explore_data(self):
        """
        ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰ì  ë°ì´í„° ë¶„ì„
        """
        print("=" * 50)
        print("1. ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰")
        print("=" * 50)
        
        # ë°ì´í„° ë¡œë“œ
        self.df = pd.read_csv(self.data_path)
        print(f"ë°ì´í„° í˜•íƒœ: {self.df.shape}")
        print(f"\në°ì´í„° ì •ë³´:")
        print(self.df.info())
        print(f"\nê¸°ë³¸ í†µê³„:")
        print(self.df.describe())
        print(f"\ní´ë˜ìŠ¤ ë¶„í¬:")
        print(self.df['Species'].value_counts())
        print(f"\nê²°ì¸¡ê°’:")
        print(self.df.isnull().sum())
        
        # ê¸°ë³¸ ì‹œê°í™”
        self.create_basic_visualizations()
        
    def create_basic_visualizations(self):
        """
        ê¸°ë³¸ ë°ì´í„° ì‹œê°í™” ìƒì„±
        """
        print("\nê¸°ë³¸ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # 1. íŠ¹ì„±ë³„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']
        )
        
        features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
        positions = [(1,1), (1,2), (2,1), (2,2)]
        
        for feature, pos in zip(features, positions):
            for species in self.df['Species'].unique():
                data = self.df[self.df['Species'] == species][feature]
                fig.add_trace(
                    go.Histogram(x=data, name=f'{species}', opacity=0.7, nbinsx=15),
                    row=pos[0], col=pos[1]
                )
        
        fig.update_layout(
            title="ì•„ì´ë¦¬ìŠ¤ íŠ¹ì„±ë³„ ë¶„í¬",
            height=600,
            showlegend=True
        )
        fig.write_image("visualizations/feature_distributions.png")
        print("âœ“ íŠ¹ì„±ë³„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ ì €ì¥: visualizations/feature_distributions.png")
        
        # 2. ì‚°ì ë„ ë§¤íŠ¸ë¦­ìŠ¤
        fig = px.scatter_matrix(
            self.df,
            dimensions=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'],
            color='Species',
            title="ì•„ì´ë¦¬ìŠ¤ íŠ¹ì„± ê°„ ìƒê´€ê´€ê³„"
        )
        fig.write_image("visualizations/scatter_matrix.png")
        print("âœ“ ì‚°ì ë„ ë§¤íŠ¸ë¦­ìŠ¤ ì €ì¥: visualizations/scatter_matrix.png")
        
        # 3. ë°•ìŠ¤í”Œë¡¯
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']
        )
        
        for i, feature in enumerate(features):
            row = (i // 2) + 1
            col = (i % 2) + 1
            
            for species in self.df['Species'].unique():
                data = self.df[self.df['Species'] == species][feature]
                fig.add_trace(
                    go.Box(y=data, name=f'{species}', showlegend=(i==0)),
                    row=row, col=col
                )
        
        fig.update_layout(
            title="ì•„ì´ë¦¬ìŠ¤ íŠ¹ì„±ë³„ ë°•ìŠ¤í”Œë¡¯",
            height=600
        )
        fig.write_image("visualizations/boxplots.png")
        print("âœ“ ë°•ìŠ¤í”Œë¡¯ ì €ì¥: visualizations/boxplots.png")
        
    def prepare_data(self):
        """
        ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í• 
        """
        print("\n" + "=" * 50)
        print("2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í• ")
        print("=" * 50)
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        self.X = self.df.drop(['Id', 'Species'], axis=1)
        self.y = self.df['Species']
        
        # ë ˆì´ë¸” ì¸ì½”ë”©
        self.y = self.label_encoder.fit_transform(self.y)
        
        # ë°ì´í„° ë¶„í• : 60% train, 20% validation, 20% test
        self.X_temp, self.X_test, self.y_temp, self.y_test = train_test_split(
            self.X, self.y, test_size=0.2, random_state=42, stratify=self.y
        )
        
        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(
            self.X_temp, self.y_temp, test_size=0.25, random_state=42, stratify=self.y_temp
        )
        
        # ìŠ¤ì¼€ì¼ë§
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_val_scaled = self.scaler.transform(self.X_val)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        print(f"í›ˆë ¨ ì„¸íŠ¸: {self.X_train.shape}")
        print(f"ê²€ì¦ ì„¸íŠ¸: {self.X_val.shape}")
        print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {self.X_test.shape}")
        
    def define_models(self):
        """
        ëª¨ë¸ ì •ì˜ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •
        """
        print("\n" + "=" * 50)
        print("3. ëª¨ë¸ ì •ì˜")
        print("=" * 50)
        
        # ê¸°ë³¸ ëª¨ë¸ë“¤ ì •ì˜
        self.models = {
            'DecisionTree': {
                'model': DecisionTreeClassifier(random_state=42),
                'params': {
                    'max_depth': [3, 5, 7, 10, None],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4],
                    'criterion': ['gini', 'entropy']
                },
                'use_scaling': False
            },
            'RandomForest': {
                'model': RandomForestClassifier(random_state=42),
                'params': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [3, 5, 7, None],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                },
                'use_scaling': False
            },
            'GradientBoosting': {
                'model': GradientBoostingClassifier(random_state=42),
                'params': {
                    'n_estimators': [50, 100, 200],
                    'learning_rate': [0.01, 0.1, 0.2],
                    'max_depth': [3, 5, 7],
                    'subsample': [0.8, 0.9, 1.0]
                },
                'use_scaling': False
            },
            'LogisticRegression': {
                'model': LogisticRegression(random_state=42, max_iter=1000),
                'params': {
                    'C': [0.01, 0.1, 1, 10, 100],
                    'penalty': ['l1', 'l2'],
                    'solver': ['liblinear', 'saga']
                },
                'use_scaling': True
            }
        }
        
        print("ì •ì˜ëœ ëª¨ë¸ë“¤:")
        for name in self.models.keys():
            print(f"- {name}")
            
    def train_and_tune_models(self):
        """
        ëª¨ë¸ í›ˆë ¨ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
        """
        print("\n" + "=" * 50)
        print("4. ëª¨ë¸ í›ˆë ¨ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
        print("=" * 50)
        
        for name, model_info in self.models.items():
            print(f"\n{name} ëª¨ë¸ íŠœë‹ ì¤‘...")
            
            # ë°ì´í„° ì„ íƒ (ìŠ¤ì¼€ì¼ë§ ì—¬ë¶€ì— ë”°ë¼)
            if model_info['use_scaling']:
                X_train = self.X_train_scaled
                X_val = self.X_val_scaled
            else:
                X_train = self.X_train
                X_val = self.X_val
            
            # RandomizedSearchCVë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
            random_search = RandomizedSearchCV(
                model_info['model'],
                model_info['params'],
                n_iter=50,  # 50ë²ˆì˜ ëœë¤ ì¡°í•© ì‹œë„
                cv=5,
                scoring='accuracy',
                random_state=42,
                n_jobs=-1
            )
            
            random_search.fit(X_train, self.y_train)
            
            # ìµœì  ëª¨ë¸ ì €ì¥
            self.best_models[name] = random_search.best_estimator_
            
            # ê²€ì¦ ì„¸íŠ¸ì—ì„œ ì„±ëŠ¥ í‰ê°€
            val_score = random_search.best_estimator_.score(X_val, self.y_val)
            
            # êµì°¨ ê²€ì¦ ì ìˆ˜
            cv_scores = cross_val_score(random_search.best_estimator_, X_train, self.y_train, cv=5)
            
            self.results[name] = {
                'best_params': random_search.best_params_,
                'best_cv_score': random_search.best_score_,
                'val_score': val_score,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
            
            print(f"âœ“ ìµœì  íŒŒë¼ë¯¸í„°: {random_search.best_params_}")
            print(f"âœ“ CV ì ìˆ˜: {random_search.best_score_:.4f}")
            print(f"âœ“ ê²€ì¦ ì ìˆ˜: {val_score:.4f}")
            
    def create_stacking_ensemble(self):
        """
        ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ ìƒì„±
        """
        print("\n" + "=" * 50)
        print("5. ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ ìƒì„±")
        print("=" * 50)
        
        # ë² ì´ìŠ¤ ëª¨ë¸ë“¤ (ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í•œ ëª¨ë¸ê³¼ ì•„ë‹Œ ëª¨ë¸ ë¶„ë¦¬)
        base_models = [
            ('dt', self.best_models['DecisionTree']),
            ('rf', self.best_models['RandomForest']),
            ('gb', self.best_models['GradientBoosting'])
        ]
        
        # ë©”íƒ€ ëª¨ë¸ (ë¡œì§€ìŠ¤í‹± íšŒê·€)
        meta_model = LogisticRegression(random_state=42)
        
        # ìŠ¤íƒœí‚¹ ë¶„ë¥˜ê¸° ìƒì„±
        stacking_clf = StackingClassifier(
            estimators=base_models,
            final_estimator=meta_model,
            cv=5,
            stack_method='predict_proba'
        )
        
        # í›ˆë ¨
        stacking_clf.fit(self.X_train, self.y_train)
        
        # ê²€ì¦ ì„¸íŠ¸ì—ì„œ ì„±ëŠ¥ í‰ê°€
        val_score = stacking_clf.score(self.X_val, self.y_val)
        cv_scores = cross_val_score(stacking_clf, self.X_train, self.y_train, cv=5)
        
        self.best_models['StackingEnsemble'] = stacking_clf
        self.results['StackingEnsemble'] = {
            'best_params': 'Ensemble of best models',
            'best_cv_score': cv_scores.mean(),
            'val_score': val_score,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
        
        print(f"âœ“ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” CV ì ìˆ˜: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        print(f"âœ“ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ê²€ì¦ ì ìˆ˜: {val_score:.4f}")
        
    def evaluate_models(self):
        """
        ëª¨ë“  ëª¨ë¸ì˜ ìµœì¢… ì„±ëŠ¥ í‰ê°€
        """
        print("\n" + "=" * 50)
        print("6. ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
        print("=" * 50)
        
        test_results = {}
        
        for name, model in self.best_models.items():
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„ íƒ
            if name == 'LogisticRegression':
                X_test = self.X_test_scaled
            else:
                X_test = self.X_test
                
            # ì˜ˆì¸¡
            y_pred = model.predict(X_test)
            test_score = accuracy_score(self.y_test, y_pred)
            
            test_results[name] = {
                'test_accuracy': test_score,
                'predictions': y_pred
            }
            
            print(f"\n{name}:")
            print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_score:.4f}")
            print("ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
            print(classification_report(self.y_test, y_pred, 
                                      target_names=self.label_encoder.classes_))
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        best_model_name = max(test_results.keys(), 
                             key=lambda x: test_results[x]['test_accuracy'])
        
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
        print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_results[best_model_name]['test_accuracy']:.4f}")
        
        self.test_results = test_results
        self.best_model_name = best_model_name
        
        return best_model_name
        
    def create_performance_visualizations(self):
        """
        ì„±ëŠ¥ ë¹„êµ ì‹œê°í™” ìƒì„±
        """
        print("\n" + "=" * 50)
        print("7. ì„±ëŠ¥ ì‹œê°í™” ìƒì„±")
        print("=" * 50)
        
        # 1. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë°” ì°¨íŠ¸
        model_names = list(self.results.keys())
        cv_scores = [self.results[name]['cv_mean'] for name in model_names]
        val_scores = [self.results[name]['val_score'] for name in model_names]
        test_scores = [self.test_results[name]['test_accuracy'] for name in model_names]
        
        fig = go.Figure(data=[
            go.Bar(name='CV Score', x=model_names, y=cv_scores),
            go.Bar(name='Validation Score', x=model_names, y=val_scores),
            go.Bar(name='Test Score', x=model_names, y=test_scores)
        ])
        
        fig.update_layout(
            title='ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ',
            xaxis_title='ëª¨ë¸',
            yaxis_title='ì •í™•ë„',
            barmode='group'
        )
        fig.write_image("visualizations/model_performance_comparison.png")
        print("âœ“ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ì €ì¥: visualizations/model_performance_comparison.png")
        
        # 2. í˜¼ë™ í–‰ë ¬ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)
        best_model = self.best_models[self.best_model_name]
        if self.best_model_name == 'LogisticRegression':
            X_test = self.X_test_scaled
        else:
            X_test = self.X_test
            
        y_pred = best_model.predict(X_test)
        cm = confusion_matrix(self.y_test, y_pred)
        
        # í´ë˜ìŠ¤ ì´ë¦„ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        class_names = list(self.label_encoder.classes_)
        
        fig = ff.create_annotated_heatmap(
            cm,
            x=class_names,
            y=class_names,
            annotation_text=cm,
            colorscale='Blues'
        )
        
        fig.update_layout(
            title=f'{self.best_model_name} ëª¨ë¸ì˜ í˜¼ë™ í–‰ë ¬',
            xaxis_title='ì˜ˆì¸¡ê°’',
            yaxis_title='ì‹¤ì œê°’'
        )
        fig.write_image("visualizations/confusion_matrix_best_model.png")
        print("âœ“ í˜¼ë™ í–‰ë ¬ ì €ì¥: visualizations/confusion_matrix_best_model.png")
        
        # 3. íŠ¹ì„± ì¤‘ìš”ë„ (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì¸ ê²½ìš°)
        if hasattr(best_model, 'feature_importances_'):
            feature_names = self.X.columns
            importances = best_model.feature_importances_
            
            fig = go.Figure(data=[
                go.Bar(x=feature_names, y=importances)
            ])
            
            fig.update_layout(
                title=f'{self.best_model_name} ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„',
                xaxis_title='íŠ¹ì„±',
                yaxis_title='ì¤‘ìš”ë„'
            )
            fig.write_image("visualizations/feature_importance_best_model.png")
            print("âœ“ íŠ¹ì„± ì¤‘ìš”ë„ ì°¨íŠ¸ ì €ì¥: visualizations/feature_importance_best_model.png")
        
    def save_models(self):
        """
        í›ˆë ¨ëœ ëª¨ë¸ë“¤ì„ pickle íŒŒì¼ë¡œ ì €ì¥
        """
        print("\n" + "=" * 50)
        print("8. ëª¨ë¸ ì €ì¥")
        print("=" * 50)
        
        # ê° ëª¨ë¸ ì €ì¥
        for name, model in self.best_models.items():
            filename = f"models/{name.lower()}_model.pkl"
            with open(filename, 'wb') as f:
                pickle.dump(model, f)
            print(f"âœ“ {name} ëª¨ë¸ ì €ì¥: {filename}")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ì™€ ë ˆì´ë¸” ì¸ì½”ë” ì €ì¥
        with open("models/scaler.pkl", 'wb') as f:
            pickle.dump(self.scaler, f)
        print("âœ“ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: models/scaler.pkl")
        
        with open("models/label_encoder.pkl", 'wb') as f:
            pickle.dump(self.label_encoder, f)
        print("âœ“ ë ˆì´ë¸” ì¸ì½”ë” ì €ì¥: models/label_encoder.pkl")
        
        # ê²°ê³¼ ìš”ì•½ ì €ì¥
        results_summary = {
            'model_results': self.results,
            'test_results': self.test_results,
            'best_model': self.best_model_name,
            'timestamp': datetime.now().isoformat()
        }
        
        with open("models/results_summary.pkl", 'wb') as f:
            pickle.dump(results_summary, f)
        print("âœ“ ê²°ê³¼ ìš”ì•½ ì €ì¥: models/results_summary.pkl")
        
    def run_complete_analysis(self):
        """
        ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        """
        print("ğŸŒ¸ ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°ì…‹ ë¶„ì„ ì‹œì‘ ğŸŒ¸")
        print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            self.load_and_explore_data()
            self.prepare_data()
            self.define_models()
            self.train_and_tune_models()
            self.create_stacking_ensemble()
            best_model = self.evaluate_models()
            self.create_performance_visualizations()
            self.save_models()
            
            print("\n" + "=" * 50)
            print("ğŸ‰ ë¶„ì„ ì™„ë£Œ!")
            print("=" * 50)
            print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model}")
            print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {self.test_results[best_model]['test_accuracy']:.4f}")
            print(f"ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("\nì €ì¥ëœ íŒŒì¼ë“¤:")
            print("ğŸ“ models/ - í›ˆë ¨ëœ ëª¨ë¸ë“¤")
            print("ğŸ“ visualizations/ - ì‹œê°í™” ê²°ê³¼ë“¤")
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise

if __name__ == "__main__":
    # ë¶„ì„ ì‹¤í–‰
    analyzer = IrisAnalysis()
    analyzer.run_complete_analysis() 