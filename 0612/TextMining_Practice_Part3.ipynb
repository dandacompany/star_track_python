{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7bJpLg3_lrC"
      },
      "source": [
        "## 🐍 Python 텍스트 분석: 고전적 벡터화 기법 마스터하기\n",
        "이번 시간에는 자연어 처리(NLP)의 가장 기본적이면서도 중요한 단계인 **텍스트 벡터화(Text Vectorization)** 의 고전적인 기법들을 다룹니다. \n",
        "\n",
        "컴퓨터가 텍스트를 이해하고 처리할 수 있도록 숫자로 변환하는 다양한 방법을 배우고, `kiwipiepy`를 활용한 한국어 처리 예제를 통해 실전 감각을 익힙니다.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Bag-of-Words (BoW)와 문서-단어 행렬 (DTM)\n",
        "\n",
        "#### 💡 개념 (Concept)\n",
        "\n",
        "머신러닝 모델은 숫자 데이터만 처리할 수 있으므로, 텍스트를 숫자 벡터로 변환하는 과정이 필수적입니다. \n",
        "\n",
        "**Bag-of-Words(BoW)** 는 가장 기초적인 텍스트 표현 방법으로, 이름처럼 문서를 '단어들의 가방'으로 간주합니다. \n",
        "\n",
        "이 모델은 문맥이나 단어의 순서는 무시하고, 각 단어가 문서에 몇 번 등장했는지(빈도)에만 집중합니다.\n",
        "\n",
        "BoW 모델을 사용해 텍스트 모음(Corpus)을 벡터화하면 **문서-단어 행렬(Document-Term Matrix, DTM)** 이 생성됩니다. DTM에서 각 행은 문서를, 각 열은 전체 어휘 사전에 포함된 단어를, 그리고 각 셀의 값은 해당 단어가 해당 문서에 나타난 빈도를 의미합니다.\n",
        "\n",
        "#### 💻 예시 코드 (Example Code)\n",
        "\n",
        "`scikit-learn`의 `CountVectorizer`는 BoW 모델을 손쉽게 구현하도록 도와줍니다. 한국어는 조사가 발달하여 \"영화가\", \"영화는\" 등을 다른 단어로 인식하는 문제가 있으므로, `kiwipiepy` 형태소 분석기를 토크나이저로 연결하여 사용해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fLhNi_ms_lrE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from kiwipiepy import Kiwi\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 1. kiwipiepy 형태소 분석기 준비\n",
        "kiwi = Kiwi()\n",
        "\n",
        "# 2. CountVectorizer에 연결할 토크나이저 함수 정의\n",
        "# 명사(NNG, NNP), 동사 어근(VV), 형용사 어근(VA)만 추출\n",
        "def kiwi_tokenizer(text: str) -> list[str]:\n",
        "    \"\"\"kiwipiepy를 사용하여 명사, 동사, 형용사를 추출하는 토크나이저\"\"\"\n",
        "    tokens = kiwi.tokenize(text)\n",
        "    return [token.form for token in tokens if token.tag in ['NNG', 'NNP', 'VV', 'VA']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "어휘 사전 (Vocabulary):\n",
            "['가능' '감독' '배우' '스토리' '연기' '연기력' '연출' '영화' '예측' '조화']\n",
            "\n",
            "문서-단어 행렬 (DTM):\n",
            "   가능  감독  배우  스토리  연기  연기력  연출  영화  예측  조화\n",
            "0   0   0   1    0   0    1   0   1   0   0\n",
            "1   1   0   0    1   0    1   0   0   1   0\n",
            "2   0   1   1    0   1    0   1   1   0   1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dante/workspace/dante-code/class/star_track_python/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 3. 실습용 텍스트 데이터\n",
        "corpus = [\n",
        "    '배우의 연기력이 정말 대단한 영화였어요.',\n",
        "    '스토리가 너무 예측 가능해서 연기력이 아까웠다.',\n",
        "    '감독의 연출과 배우의 연기가 조화로웠던 영화.',\n",
        "]\n",
        "\n",
        "# 4. CountVectorizer 생성 및 DTM 구축\n",
        "vectorizer = CountVectorizer(tokenizer=kiwi_tokenizer)\n",
        "dtm = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# 생성된 어휘 사전 확인\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(\"어휘 사전 (Vocabulary):\")\n",
        "print(feature_names)\n",
        "\n",
        "# DTM을 DataFrame으로 시각화\n",
        "dtm_df = pd.DataFrame(dtm.toarray(), columns=feature_names)\n",
        "print(\"\\n문서-단어 행렬 (DTM):\")\n",
        "print(dtm_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ✏️ 연습 문제 (Practice Problems)\n",
        "\n",
        "1.  위 예제의 `kiwi_tokenizer` 함수를 수정하여, 품사가 `NNG`(일반 명사)인 단어만 추출하도록 변경한 뒤 DTM을 다시 생성하고 결과를 비교해 보세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 코드 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  `CountVectorizer`를 생성할 때 `ngram_range=(1, 2)` 파라미터를 추가하여 DTM을 만들어 보세요. 생성된 어휘 사전에 어떤 변화가 생겼는지, 그리고 이 파라미터가 어떤 의미를 갖는지 설명해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 코드 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 🚀 여기서 잠깐! Python 실력 다지기: 좋은 디자인 패턴 클래스로 래핑하기\n",
        "\n",
        "위의 코드는 간단한 예시였지만, 실제 프로젝트에서는 더 체계적이고 재사용 가능한 코드 구조가 필요합니다. \n",
        "\n",
        "아래에서는 **싱글톤 패턴(Singleton Pattern)** 을 활용하여 Kiwi 형태소 분석기를 효율적으로 관리하고, 텍스트 마이닝 작업을 캡슐화한 클래스를 만들어보겠습니다.\n",
        "\n",
        "**주요 개선 사항:**\n",
        "- 🎯 **싱글톤 패턴**: Kiwi 객체를 한 번만 생성하여 메모리 효율성 증대\n",
        "- 🔧 **캡슐화**: 토큰화와 DTM 생성 기능을 하나의 클래스로 통합\n",
        "- 🎨 **유연성**: 품사 태그를 매개변수로 받아 다양한 분석 요구사항 대응\n",
        "- 📊 **편의성**: DTM을 바로 DataFrame 형태로 반환하여 분석 작업 간소화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from kiwipiepy import Kiwi\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "class KiwiTokenizer:\n",
        "    \"\"\"Kiwi 형태소 분석기를 싱글톤 패턴으로 관리하는 토크나이저 클래스\"\"\"\n",
        "    \n",
        "    _instance = None\n",
        "    _kiwi = None\n",
        "    \n",
        "    def __new__(cls): # __new__ 메서드는 클래스 인스턴스를 생성할 때 호출되는 메서드입니다.\n",
        "        if cls._instance is None:\n",
        "            cls._instance = super(KiwiTokenizer, cls).__new__(cls)\n",
        "            cls._kiwi = Kiwi() # Kiwi 객체 생성\n",
        "            cls._instance.vectorizer = None  # vectorizer 초기화\n",
        "        return cls._instance\n",
        "    \n",
        "    def tokenize(self, text: str, pos_tags: list[str] = None) -> list[str]:\n",
        "        \"\"\"\n",
        "        텍스트를 토큰화하여 지정된 품사의 단어만 추출\n",
        "        \n",
        "        Args:\n",
        "            text: 분석할 텍스트\n",
        "            pos_tags: 추출할 품사 태그 리스트 (기본값: ['NNG', 'NNP', 'VV', 'VA'])\n",
        "        \n",
        "        Returns:\n",
        "            추출된 단어들의 리스트\n",
        "        \"\"\"\n",
        "        if pos_tags is None:\n",
        "            pos_tags = ['NNG', 'NNP', 'VV', 'VA']\n",
        "        \n",
        "        tokens = self._kiwi.tokenize(text)\n",
        "        return [token.form for token in tokens if token.tag in pos_tags]\n",
        "    \n",
        "    def create_dtm(self, corpus: list[str], pos_tags: list[str] = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        텍스트 코퍼스를 이용하여 DTM(Document-Term Matrix)을 생성\n",
        "        \n",
        "        Args:\n",
        "            corpus: 텍스트 문서들의 리스트\n",
        "            pos_tags: 추출할 품사 태그 리스트 (기본값: ['NNG', 'NNP', 'VV', 'VA'])\n",
        "        \n",
        "        Returns:\n",
        "            DTM DataFrame\n",
        "        \"\"\"\n",
        "        def tokenizer_func(text: str) -> list[str]:\n",
        "            return self.tokenize(text, pos_tags)\n",
        "        \n",
        "        self.vectorizer = CountVectorizer(tokenizer=tokenizer_func)\n",
        "        dtm = self.vectorizer.fit_transform(corpus)\n",
        "        feature_names = self.vectorizer.get_feature_names_out()\n",
        "        dtm_df = pd.DataFrame(dtm.toarray(), columns=feature_names)\n",
        "        \n",
        "        return dtm_df\n",
        "    \n",
        "    def get_vocabulary(self) -> list[str]:\n",
        "        \"\"\"생성된 어휘 사전을 반환\"\"\"\n",
        "        if self.vectorizer is None:\n",
        "            print(\"먼저 create_dtm() 메서드를 호출하여 vectorizer를 생성해주세요.\")\n",
        "            return\n",
        "        feature_names = self.vectorizer.get_feature_names_out()\n",
        "        return list(feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 토크나이저 인스턴스 생성\n",
        "tokenizer = KiwiTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dante/workspace/dante-code/class/star_track_python/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>가능</th>\n",
              "      <th>감독</th>\n",
              "      <th>배우</th>\n",
              "      <th>스토리</th>\n",
              "      <th>연기</th>\n",
              "      <th>연기력</th>\n",
              "      <th>연출</th>\n",
              "      <th>영화</th>\n",
              "      <th>예측</th>\n",
              "      <th>조화</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   가능  감독  배우  스토리  연기  연기력  연출  영화  예측  조화\n",
              "0   0   0   1    0   0    1   0   1   0   0\n",
              "1   1   0   0    1   0    1   0   0   1   0\n",
              "2   0   1   1    0   1    0   1   1   0   1"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dtm_df = tokenizer.create_dtm(corpus)\n",
        "dtm_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['가능', '감독', '배우', '스토리', '연기', '연기력', '연출', '영화', '예측', '조화']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----\n",
        "\n",
        "### 2\\. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "#### 💡 개념 (Concept)\n",
        "\n",
        "DTM은 구현이 간단하지만, \"그리고\", \"있다\"와 같이 모든 문서에 자주 나타나는 단어들이 높은 값을 가져 중요도를 왜곡할 수 있다는 한계가 있습니다. **TF-IDF**는 이러한 단점을 보완하기 위해 등장한 가중치 부여 방식입니다.\n",
        "\n",
        "  - **TF (단어 빈도, Term Frequency)**: 특정 문서 내에서 단어가 얼마나 자주 등장하는지를 나타내는 값. DTM의 값과 같습니다.\n",
        "  - **IDF (역문서 빈도, Inverse Document Frequency)**: 특정 단어가 전체 문서에서 얼마나 희귀하게 등장하는지를 나타내는 값입니다. 모든 문서에 흔하게 등장하는 단어는 낮은 IDF 값을, 특정 소수의 문서에만 등장하는 희귀한 단어는 높은 IDF 값을 갖습니다.\n",
        "\n",
        "**TF-IDF 값은 TF와 IDF를 곱하여 계산**되며, 특정 문서에는 자주 등장하지만(높은 TF) 전체 문서에서는 희귀한(높은 IDF) 단어일수록 높은 값을 가집니다. 이는 그 단어가 해당 문서를 대표하는 **핵심어**일 가능성이 높다는 의미입니다.\n",
        "\n",
        "![수식](https://blog.kakaocdn.net/dn/QOLOh/btrFbMBsp6x/wln93zToLdJS6QR9g1Kwg0/img.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### 💻 예시 코드 (Example Code)\n",
        "`scikit-learn`의 `TfidfVectorizer`를 사용하면 TF-IDF 행렬을 쉽게 생성할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF 행렬:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dante/workspace/dante-code/class/star_track_python/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>가</th>\n",
              "      <th>가능</th>\n",
              "      <th>감독</th>\n",
              "      <th>감동</th>\n",
              "      <th>결말</th>\n",
              "      <th>괜찮</th>\n",
              "      <th>긴장감</th>\n",
              "      <th>깔끔</th>\n",
              "      <th>꿀</th>\n",
              "      <th>끝</th>\n",
              "      <th>...</th>\n",
              "      <th>좋</th>\n",
              "      <th>주연</th>\n",
              "      <th>중반</th>\n",
              "      <th>찍</th>\n",
              "      <th>처음</th>\n",
              "      <th>최고</th>\n",
              "      <th>추천</th>\n",
              "      <th>케미</th>\n",
              "      <th>표현</th>\n",
              "      <th>하</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.587</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.396</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.438</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.493</td>\n",
              "      <td>0.493</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.421</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.309</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.475</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.488</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.416</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.416</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.343</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.369</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.499</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.581</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.499</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.403</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.413</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.574</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.508</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.413</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24 rows × 68 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        가     가능     감독     감동     결말     괜찮    긴장감     깔끔      꿀      끝  ...  \\\n",
              "0   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "1   0.000  0.587  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "2   0.000  0.000  0.396  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "3   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "4   0.000  0.000  0.438  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "5   0.000  0.000  0.000  0.000  0.493  0.493  0.000  0.000  0.000  0.000  ...   \n",
              "6   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.421  0.000  ...   \n",
              "7   0.000  0.000  0.309  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "8   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "9   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "10  0.488  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "11  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.479  0.000  0.000  ...   \n",
              "12  0.000  0.000  0.000  0.000  0.000  0.000  0.416  0.000  0.000  0.416  ...   \n",
              "13  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "14  0.000  0.000  0.343  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "15  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "16  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "17  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "18  0.000  0.000  0.499  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "19  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "20  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "21  0.000  0.000  0.000  0.574  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "22  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "23  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  ...   \n",
              "\n",
              "        좋     주연     중반     찍     처음     최고     추천     케미     표현      하  \n",
              "0   0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "1   0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "2   0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "3   0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "4   0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "5   0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "6   0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.338  \n",
              "7   0.000  0.000  0.000  0.45  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "8   0.000  0.475  0.000  0.00  0.000  0.420  0.000  0.000  0.000  0.000  \n",
              "9   0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.582  \n",
              "10  0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "11  0.355  0.000  0.000  0.00  0.000  0.000  0.479  0.479  0.000  0.000  \n",
              "12  0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "13  0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "14  0.369  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.499  0.000  \n",
              "15  0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "16  0.581  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "17  0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "18  0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.582  \n",
              "19  0.000  0.000  0.455  0.00  0.403  0.000  0.000  0.000  0.000  0.000  \n",
              "20  0.413  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "21  0.000  0.000  0.000  0.00  0.508  0.000  0.000  0.000  0.000  0.000  \n",
              "22  0.000  0.000  0.000  0.00  0.000  0.413  0.000  0.000  0.000  0.000  \n",
              "23  0.000  0.000  0.000  0.00  0.000  0.000  0.000  0.000  0.000  0.000  \n",
              "\n",
              "[24 rows x 68 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from kiwipiepy import Kiwi\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 1. kiwipiepy 토크나이저 (위와 동일)\n",
        "kiwi = Kiwi()\n",
        "def kiwi_tokenizer(text: str) -> list[str]:\n",
        "    tokens = kiwi.tokenize(text)\n",
        "    return [token.form for token in tokens if token.tag in ['NNG', 'NNP', 'VV', 'VA']]\n",
        "\n",
        "# 2. 실습용 텍스트 데이터\n",
        "corpus = [\n",
        "    '배우의 연기력이 정말 대단한 영화였어요.',\n",
        "    '스토리가 너무 예측 가능해서 연기력이 아까웠다.',\n",
        "    '감독의 연출과 배우의 연기가 조화로웠던 영화.',\n",
        "    '와 이 영화 진짜 대박이야! 배우들 연기 미쳤고 스토리도 완전 몰입됨',\n",
        "    '음... 좀 아쉽네요. 감독이 뭘 말하고 싶었는지 모르겠어요',\n",
        "    '연기는 괜찮았는데 결말이 너무 뻔해서 실망했습니다',\n",
        "    '헐 이거 완전 꿀잼ㅋㅋ 예상 못한 반전에 소름돋았어',\n",
        "    '감독님... 제발 좀 더 신경써서 찍으시길... 연출이 엉망이에요',\n",
        "    '주연배우 연기 진짜 자연스럽더라! 몰입도 최고였음',\n",
        "    '스토리가 조금 복잡하긴 했지만 나름 볼만했어요',\n",
        "    '이런 영화를 왜 만들었는지 이해가 안 가네... 시간 아까움',\n",
        "    '배우들 케미 완전 좋았고 연출도 깔끔했음. 추천!',\n",
        "    '예측할 수 없는 전개로 끝까지 출긴장감 넘쳤습니다',\n",
        "    '연기력은 인정하지만 스토리가 너무 뻔해서... 그냥 그래요',\n",
        "    '감독의 의도는 좋았으나 표현 방식이 아쉬웠네요',\n",
        "    'ㅋㅋㅋ 이거 뭐야 완전 재밌잖아? 배우들 연기 ㄹㅇ 대단함',\n",
        "    '조용한 영화인데 배우들 연기가 워낙 좋아서 지루하지 않았어요',\n",
        "    '액션은 별로였지만 인간관계 드라마가 탄탄해서 만족',\n",
        "    '아 진짜... 왜 이렇게 만들었을까? 감독 뭐하는 거야',\n",
        "    '처음엔 지루했는데 중반부터 완전 몰입! 연출 센스 있네',\n",
        "    '배우들 연기는 좋았지만 전체적으로 밋밋한 느낌이에요',\n",
        "    '와... 이런 스토리는 처음 봐! 정말 신선하고 감동적이었어',\n",
        "    '연출과 연기 모두 완벽했습니다. 올해 최고의 작품 중 하나!',\n",
        "    '뭔가 아쉬운 부분들이 있지만 그래도 볼만한 영화였어요'\n",
        "]\n",
        "\n",
        "# 3. TfidfVectorizer 생성 및 TF-IDF 행렬 구축\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=kiwi_tokenizer)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# 어휘 사전 확인\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# TF-IDF 행렬을 DataFrame으로 시각화 (소수점 셋째 자리까지)\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "tfidf_df.round(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeORWeZZ_lrF"
      },
      "source": [
        "#### ✏️ 연습 문제 (Practice Problems)\n",
        "\n",
        "1.  학습이 완료된 `tfidf_vectorizer` 객체의 `idf_` 속성을 출력해 보세요. 어떤 단어의 IDF 값이 가장 높고, 어떤 단어의 IDF 값이 가장 낮은가요? 그 이유를 설명해 보세요. (힌트: `zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_)`를 사용하면 단어와 IDF 값을 함께 볼 수 있습니다.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 코드 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  `TfidfVectorizer` 생성 시 `max_df=0.8`과 `min_df=2` 파라미터를 추가하여 TF-IDF 행렬을 다시 만들고, 어휘 사전이 어떻게 변하는지 확인해 보세요. 각 파라미터가 어떤 역할을 하는지 설명해 보세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 코드 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. BM25 알고리즘 (Best Matching 25)\n",
        "\n",
        "**BM25(Best Matching 25)** 는 정보 검색 분야에서 문서와 질의(query) 간의 관련성을 평가하는 대표적인 랭킹 함수입니다. \n",
        "\n",
        "TF-IDF에서 더 발전된 알고리즘으로, 문서 길이와 단어길이를 보정하여 더 정규화된 점수를 계산합니다.\n",
        "\n",
        "검색 엔진(예: 구글, 네이버, 엘라스틱서치 등)에서 사용자가 입력한 키워드와 가장 관련성이 높은 문서를 찾아 순위를 매기는 데 널리 활용됩니다.\n",
        "\n",
        "\n",
        "### BM25의 핵심 원리\n",
        "\n",
        "- **TF(단어 빈도, Term Frequency):** 특정 단어가 문서 내에서 얼마나 자주 등장하는지 측정합니다.\n",
        "- **IDF(역문서 빈도, Inverse Document Frequency):** 특정 단어가 전체 문서 집합에서 얼마나 희귀한지 반영합니다. 자주 등장하지 않는 단어일수록 더 중요한 정보로 간주합니다.\n",
        "- **문서 길이 보정:** 문서마다 길이가 다르기 때문에, 긴 문서가 점수를 과도하게 받지 않도록 길이 보정이 들어갑니다.\n",
        "\n",
        "BM25는 TF-IDF와 비슷한 개념을 사용하지만, TF(단어 빈도)의 증가에 따라 점수가 무한정 커지지 않도록 조절하고, 문서 길이 차이도 공정하게 반영합니다.\n",
        "\n",
        "### BM25의 수식\n",
        "\n",
        "BM25 점수는 다음과 같이 계산됩니다.\n",
        "\n",
        "$$\n",
        "\\text{Score}(D, Q) = \\sum_{q_i \\in Q} \\text{IDF}(q_i) \\cdot \\frac{TF(q_i, D) \\cdot (k_1 + 1)}{TF(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})}\n",
        "$$\n",
        "\n",
        "- $TF(q_i, D)$: 문서 D에서 단어 $q_i$의 빈도\n",
        "- $|D|$: 문서 D의 길이(단어 수)\n",
        "- $\\text{avgdl}$: 전체 문서의 평균 길이\n",
        "- $k_1, b$: 조정 가능한 하이퍼파라미터(보통 $k_1$은 1.2~2.0, $b$는 0.75)\n",
        "\n",
        "### BM25의 특징과 장점\n",
        "\n",
        "- **정확성:** 문서와 질의 간의 관련성을 효과적으로 평가\n",
        "- **유연성:** 파라미터 조정으로 다양한 데이터셋에 대응 가능\n",
        "- **효율성:** 계산이 간단해 대규모 데이터셋에서도 빠르게 작동\n",
        "- **검색 엔진 표준:** 엘라스틱서치, 구글, 네이버 등 주요 검색 시스템에서 기본 랭킹 함수로 채택\n",
        "\n",
        "### BM25와 TF-IDF의 차이점\n",
        "\n",
        "| 구분         | TF-IDF                                   | BM25                                                         |\n",
        "|--------------|------------------------------------------|--------------------------------------------------------------|\n",
        "| 단어 빈도    | 단순히 많이 등장할수록 점수 증가           | TF가 일정 수준 이상 올라가지 않도록 조절                      |\n",
        "| 문서 길이    | 고려하지 않음                             | 문서 길이 보정 적용(긴 문서 불리, 짧은 문서 유리 현상 완화)   |\n",
        "| 파라미터     | 없음                                     | k1, b 등 파라미터로 세밀한 조정 가능                         |\n",
        "\n",
        "### BM25의 응용 분야\n",
        "\n",
        "- 검색 엔진(웹, 문서, 뉴스 등)\n",
        "- 추천 시스템\n",
        "- 자연어 처리 기반 문서 분류, 요약 등"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### 💻 예시 코드 (Example Code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /Users/dante/workspace/dante-code/class/star_track_python/.venv/lib/python3.12/site-packages (from rank_bm25) (1.26.4)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* BM25 모델 생성 및 학습\n",
            "* 토큰화된 문서들\n",
            "문서 0: ['배우', '연기력', '영화']\n",
            "문서 1: ['스토리', '예측', '가능', '연기력']\n",
            "문서 2: ['감독', '연출', '배우', '연기', '조화', '영화']\n",
            "문서 3: ['영화', '대박', '배우', '연기', '미치', '스토리', '몰입']\n",
            "문서 4: ['감독', '말', '모르']\n",
            "문서 5: ['연기', '괜찮', '결말', '뻔하', '실망']\n",
            "문서 6: ['꿀', '잼', '예상', '하', '반전', '소름']\n",
            "문서 7: ['감독', '신경', '쓰', '찍', '연출', '엉망']\n",
            "문서 8: ['주연', '배우', '연기', '자연', '몰입도', '최고']\n",
            "문서 9: ['스토리', '보', '하']\n",
            "문서 10: ['영화', '만들', '이해', '가', '시간']\n",
            "문서 11: ['배우', '케미', '좋', '연출', '깔끔', '추천']\n",
            "문서 12: ['예측', '없', '전개', '끝', '긴장감', '넘치']\n",
            "문서 13: ['연기력', '인정', '스토리', '뻔하']\n",
            "문서 14: ['감독', '의도', '좋', '표현', '방식']\n",
            "문서 15: ['재밌', '배우', '연기', '대단하']\n",
            "문서 16: ['영화', '배우', '연기', '좋']\n",
            "문서 17: ['액션', '인간관계', '드라마', '만족']\n",
            "문서 18: ['만들', '감독', '하']\n",
            "문서 19: ['처음', '중반', '몰입', '연출', '센스', '있']\n",
            "문서 20: ['배우', '연기', '좋', '전체', '느낌']\n",
            "문서 21: ['스토리', '처음', '보', '감동']\n",
            "문서 22: ['연출', '연기', '완벽', '올해', '최고', '작품']\n",
            "문서 23: ['부분', '있', '영화']\n",
            "* BM25 모델 학습 완료\n"
          ]
        }
      ],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# 1. BM25 모델 생성 및 학습\n",
        "print(\"* BM25 모델 생성 및 학습\")\n",
        "tokenized_corpus_for_bm25 = [kiwi_tokenizer(doc) for doc in corpus]\n",
        "print(\"* 토큰화된 문서들\")\n",
        "for i, tokens in enumerate(tokenized_corpus_for_bm25):\n",
        "    print(f\"문서 {i}: {tokens}\")\n",
        "\n",
        "bm25 = BM25Okapi(tokenized_corpus_for_bm25)\n",
        "print(\"* BM25 모델 학습 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "질의: '배우 연기력'\n",
            "토큰화된 질의: ['배우', '연기력']\n"
          ]
        }
      ],
      "source": [
        "# 2. 질의(Query)에 대한 BM25 점수 계산\n",
        "query = \"배우 연기력\"\n",
        "tokenized_query = kiwi_tokenizer(query)\n",
        "print(f\"\\n질의: '{query}'\")\n",
        "print(f\"토큰화된 질의: {tokenized_query}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "각 문서의 BM25 점수:\n",
            "문서 0: 2.9712\n",
            "문서 1: 1.9541\n",
            "문서 2: 0.5931\n",
            "문서 3: 0.5468\n",
            "문서 4: 0.0000\n",
            "문서 5: 0.0000\n",
            "문서 6: 0.0000\n",
            "문서 7: 0.0000\n",
            "문서 8: 0.5931\n",
            "문서 9: 0.0000\n",
            "문서 10: 0.0000\n",
            "문서 11: 0.5931\n",
            "문서 12: 0.0000\n",
            "문서 13: 1.9541\n",
            "문서 14: 0.0000\n",
            "문서 15: 0.7140\n",
            "문서 16: 0.7140\n",
            "문서 17: 0.0000\n",
            "문서 18: 0.0000\n",
            "문서 19: 0.0000\n",
            "문서 20: 0.6479\n",
            "문서 21: 0.0000\n",
            "문서 22: 0.0000\n",
            "문서 23: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# 각 문서에 대한 BM25 점수 계산\n",
        "doc_scores = bm25.get_scores(tokenized_query)\n",
        "print(f\"각 문서의 BM25 점수:\")\n",
        "for i, score in enumerate(doc_scores):\n",
        "    print(f\"문서 {i}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "가장 관련성 높은 문서 순위:\n",
            "1위 (점수: 2.9712): 배우의 연기력이 정말 대단한 영화였어요.\n",
            "2위 (점수: 1.9541): 스토리가 너무 예측 가능해서 연기력이 아까웠다.\n",
            "3위 (점수: 1.9541): 연기력은 인정하지만 스토리가 너무 뻔해서... 그냥 그래요\n",
            "4위 (점수: 0.7140): ㅋㅋㅋ 이거 뭐야 완전 재밌잖아? 배우들 연기 ㄹㅇ 대단함\n",
            "5위 (점수: 0.7140): 조용한 영화인데 배우들 연기가 워낙 좋아서 지루하지 않았어요\n",
            "6위 (점수: 0.6479): 배우들 연기는 좋았지만 전체적으로 밋밋한 느낌이에요\n",
            "7위 (점수: 0.5931): 감독의 연출과 배우의 연기가 조화로웠던 영화.\n",
            "8위 (점수: 0.5931): 주연배우 연기 진짜 자연스럽더라! 몰입도 최고였음\n",
            "9위 (점수: 0.5931): 배우들 케미 완전 좋았고 연출도 깔끔했음. 추천!\n",
            "10위 (점수: 0.5468): 와 이 영화 진짜 대박이야! 배우들 연기 미쳤고 스토리도 완전 몰입됨\n",
            "11위 (점수: 0.0000): 음... 좀 아쉽네요. 감독이 뭘 말하고 싶었는지 모르겠어요\n",
            "12위 (점수: 0.0000): 연기는 괜찮았는데 결말이 너무 뻔해서 실망했습니다\n",
            "13위 (점수: 0.0000): 헐 이거 완전 꿀잼ㅋㅋ 예상 못한 반전에 소름돋았어\n",
            "14위 (점수: 0.0000): 감독님... 제발 좀 더 신경써서 찍으시길... 연출이 엉망이에요\n",
            "15위 (점수: 0.0000): 스토리가 조금 복잡하긴 했지만 나름 볼만했어요\n",
            "16위 (점수: 0.0000): 이런 영화를 왜 만들었는지 이해가 안 가네... 시간 아까움\n",
            "17위 (점수: 0.0000): 예측할 수 없는 전개로 끝까지 긴장감 넘쳤습니다\n",
            "18위 (점수: 0.0000): 감독의 의도는 좋았으나 표현 방식이 아쉬웠네요\n",
            "19위 (점수: 0.0000): 액션은 별로였지만 인간관계 드라마가 탄탄해서 만족\n",
            "20위 (점수: 0.0000): 아 진짜... 왜 이렇게 만들었을까? 감독 뭐하는 거야\n",
            "21위 (점수: 0.0000): 처음엔 지루했는데 중반부터 완전 몰입! 연출 센스 있네\n",
            "22위 (점수: 0.0000): 와... 이런 스토리는 처음 봐! 정말 신선하고 감동적이었어\n",
            "23위 (점수: 0.0000): 연출과 연기 모두 완벽했습니다. 올해 최고의 작품 중 하나!\n",
            "24위 (점수: 0.0000): 뭔가 아쉬운 부분들이 있지만 그래도 볼만한 영화였어요\n"
          ]
        }
      ],
      "source": [
        "# 3. 가장 관련성 높은 문서 반환\n",
        "print(f\"가장 관련성 높은 문서 순위:\")\n",
        "# 점수와 인덱스를 함께 저장하여 정렬\n",
        "score_doc_pairs = [(score, i, corpus[i]) for i, score in enumerate(doc_scores)]\n",
        "sorted_results = sorted(score_doc_pairs, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "for rank, (score, doc_idx, doc_text) in enumerate(sorted_results, 1):\n",
        "    print(f\"{rank}위 (점수: {score:.4f}): {doc_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "상위 3개 문서 (get_top_n 사용):\n",
            "1. 배우의 연기력이 정말 대단한 영화였어요.\n",
            "2. 스토리가 너무 예측 가능해서 연기력이 아까웠다.\n",
            "3. 연기력은 인정하지만 스토리가 너무 뻔해서... 그냥 그래요\n"
          ]
        }
      ],
      "source": [
        "# 4. get_top_n 메서드 사용\n",
        "top_n_docs = bm25.get_top_n(tokenized_query, corpus, n=3)\n",
        "print(f\"상위 3개 문서 (get_top_n 사용):\")\n",
        "for i, doc in enumerate(top_n_docs, 1):\n",
        "    print(f\"{i}. {doc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "질의: '스토리 감독'\n",
            "토큰화된 질의: ['스토리', '감독']\n",
            "\n",
            "각 문서의 BM25 점수:\n",
            "문서 0: 0.0000\n",
            "문서 1: 1.3625\n",
            "문서 2: 1.1317\n",
            "문서 3: 1.0433\n",
            "문서 4: 1.5172\n",
            "문서 5: 0.0000\n",
            "문서 6: 0.0000\n",
            "문서 7: 1.1317\n",
            "문서 8: 0.0000\n",
            "문서 9: 1.5172\n",
            "문서 10: 0.0000\n",
            "문서 11: 0.0000\n",
            "문서 12: 0.0000\n",
            "문서 13: 1.3625\n",
            "문서 14: 1.2364\n",
            "문서 15: 0.0000\n",
            "문서 16: 0.0000\n",
            "문서 17: 0.0000\n",
            "문서 18: 1.5172\n",
            "문서 19: 0.0000\n",
            "문서 20: 0.0000\n",
            "문서 21: 1.3625\n",
            "문서 22: 0.0000\n",
            "문서 23: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# 5. 다른 질의로 테스트\n",
        "query2 = \"스토리 감독\"\n",
        "tokenized_query2 = kiwi_tokenizer(query2)\n",
        "print(f\"질의: '{query2}'\")\n",
        "print(f\"토큰화된 질의: {tokenized_query2}\")\n",
        "\n",
        "doc_scores2 = bm25.get_scores(tokenized_query2)\n",
        "print(f\"\\n각 문서의 BM25 점수:\")\n",
        "for i, score in enumerate(doc_scores2):\n",
        "    print(f\"문서 {i}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BM25 파라미터 조정 (k1=0.5, b=0.75)\n",
            "기본 파라미터 vs 조정된 파라미터 비교:\n",
            "문서\t기본 BM25\t조정 BM25\t차이\n",
            "0\t2.9712\t\t2.7300\t\t-0.2411\n",
            "1\t1.9541\t\t1.8899\t\t-0.0642\n",
            "2\t0.5931\t\t0.6224\t\t+0.0293\n",
            "3\t0.5468\t\t0.5931\t\t+0.0463\n",
            "4\t0.0000\t\t0.0000\t\t+0.0000\n",
            "5\t0.0000\t\t0.0000\t\t+0.0000\n",
            "6\t0.0000\t\t0.0000\t\t+0.0000\n",
            "7\t0.0000\t\t0.0000\t\t+0.0000\n",
            "8\t0.5931\t\t0.6224\t\t+0.0293\n",
            "9\t0.0000\t\t0.0000\t\t+0.0000\n",
            "10\t0.0000\t\t0.0000\t\t+0.0000\n",
            "11\t0.5931\t\t0.6224\t\t+0.0293\n",
            "12\t0.0000\t\t0.0000\t\t+0.0000\n",
            "13\t1.9541\t\t1.8899\t\t-0.0642\n",
            "14\t0.0000\t\t0.0000\t\t+0.0000\n",
            "15\t0.7140\t\t0.6906\t\t-0.0235\n",
            "16\t0.7140\t\t0.6906\t\t-0.0235\n",
            "17\t0.0000\t\t0.0000\t\t+0.0000\n",
            "18\t0.0000\t\t0.0000\t\t+0.0000\n",
            "19\t0.0000\t\t0.0000\t\t+0.0000\n",
            "20\t0.6479\t\t0.6547\t\t+0.0067\n",
            "21\t0.0000\t\t0.0000\t\t+0.0000\n",
            "22\t0.0000\t\t0.0000\t\t+0.0000\n",
            "23\t0.0000\t\t0.0000\t\t+0.0000\n",
            "\n",
            "=== BM25 분석 ===\n",
            "BM25 점수가 높을수록 질의와 관련성이 높은 문서입니다.\n",
            "k1: 단어 빈도의 포화도 조절 (높을수록 빈도 영향 증가)\n",
            "b: 문서 길이 정규화 정도 (0에 가까우면 길이 무시, 1에 가까우면 길이 크게 반영)\n"
          ]
        }
      ],
      "source": [
        "# 6. BM25 파라미터 조정 (k1, b 값 변경)\n",
        "print(\"BM25 파라미터 조정 (k1=0.5, b=0.75)\")\n",
        "\n",
        "bm25_custom = BM25Okapi(tokenized_corpus_for_bm25, k1=0.5, b=0.75)\n",
        "doc_scores_custom = bm25_custom.get_scores(tokenized_query)\n",
        "\n",
        "print(\"기본 파라미터 vs 조정된 파라미터 비교:\")\n",
        "print(\"문서\\t기본 BM25\\t조정 BM25\\t차이\")\n",
        "for i, (default_score, custom_score) in enumerate(zip(doc_scores, doc_scores_custom)):\n",
        "    diff = custom_score - default_score\n",
        "    print(f\"{i}\\t{default_score:.4f}\\t\\t{custom_score:.4f}\\t\\t{diff:+.4f}\")\n",
        "\n",
        "print(\"\\n=== BM25 분석 ===\")\n",
        "print(\"BM25 점수가 높을수록 질의와 관련성이 높은 문서입니다.\")\n",
        "print(\"k1: 단어 빈도의 포화도 조절 (높을수록 빈도 영향 증가)\")\n",
        "print(\"b: 문서 길이 정규화 정도 (0에 가까우면 길이 무시, 1에 가까우면 길이 크게 반영)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF vs BM25 비교 (질의: '배우 연기력')\n",
            "문서\tTF-IDF 유사도\tBM25 점수\t선호 모델\n",
            "0\t0.8372\t\t2.9712\t\tBM25\n",
            "1\t0.3838\t\t1.9541\t\tBM25\n",
            "2\t0.1916\t\t0.5931\t\tBM25\n",
            "3\t0.1601\t\t0.5468\t\tBM25\n",
            "4\t0.0000\t\t0.0000\t\t동일\n",
            "5\t0.0000\t\t0.0000\t\t동일\n",
            "6\t0.0000\t\t0.0000\t\t동일\n",
            "7\t0.0000\t\t0.0000\t\t동일\n",
            "8\t0.1581\t\t0.5931\t\tBM25\n",
            "9\t0.0000\t\t0.0000\t\t동일\n",
            "10\t0.0000\t\t0.0000\t\t동일\n",
            "11\t0.1597\t\t0.5931\t\tBM25\n",
            "12\t0.0000\t\t0.0000\t\t동일\n",
            "13\t0.3838\t\t1.9541\t\tBM25\n",
            "14\t0.0000\t\t0.0000\t\t동일\n",
            "15\t0.2043\t\t0.7140\t\tBM25\n",
            "16\t0.2616\t\t0.7140\t\tBM25\n",
            "17\t0.0000\t\t0.0000\t\t동일\n",
            "18\t0.0000\t\t0.0000\t\t동일\n",
            "19\t0.0000\t\t0.0000\t\t동일\n",
            "20\t0.1861\t\t0.6479\t\tBM25\n",
            "21\t0.0000\t\t0.0000\t\t동일\n",
            "22\t0.0000\t\t0.0000\t\t동일\n",
            "23\t0.0000\t\t0.0000\t\t동일\n",
            "\n",
            "=== 비교 분석 ===\n",
            "TF-IDF: 코사인 유사도 기반, 벡터 공간에서의 각도 측정\n",
            "BM25: 확률론적 랭킹 함수, 문서 길이와 단어 빈도 포화 고려\n",
            "BM25가 일반적으로 검색 성능이 더 우수하다고 알려져 있습니다.\n"
          ]
        }
      ],
      "source": [
        "# 7. TF-IDF와 BM25 비교\n",
        "print(\"TF-IDF vs BM25 비교 (질의: '배우 연기력')\")\n",
        "\n",
        "# TF-IDF로 질의 변환\n",
        "query_tfidf = tfidf_vectorizer.transform([query])\n",
        "# 각 문서와의 코사인 유사도 계산\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "tfidf_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
        "\n",
        "print(\"문서\\tTF-IDF 유사도\\tBM25 점수\\t선호 모델\")\n",
        "for i, (tfidf_sim, bm25_score) in enumerate(zip(tfidf_similarities, doc_scores)):\n",
        "    preferred = \"BM25\" if bm25_score > tfidf_sim else \"TF-IDF\" if tfidf_sim > bm25_score else \"동일\"\n",
        "    print(f\"{i}\\t{tfidf_sim:.4f}\\t\\t{bm25_score:.4f}\\t\\t{preferred}\")\n",
        "\n",
        "print(\"\\n=== 비교 분석 ===\")\n",
        "print(\"TF-IDF: 코사인 유사도 기반, 벡터 공간에서의 각도 측정\")\n",
        "print(\"BM25: 확률론적 랭킹 함수, 문서 길이와 단어 빈도 포화 고려\")\n",
        "print(\"BM25가 일반적으로 검색 성능이 더 우수하다고 알려져 있습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "-----\n",
        "\n",
        "### 4\\. Word2Vec: 단어의 의미를 벡터에 담다\n",
        "\n",
        "#### 💡 개념 (Concept)\n",
        "\n",
        "BoW와 TF-IDF는 단어의 등장 빈도만 고려할 뿐, **단어의 의미나 문맥 정보**를 담지 못합니다. 예를 들어, \"영화\"와 \"작품\"은 의미가 유사하지만 DTM이나 TF-IDF 행렬에서는 완전히 다른 단어로 취급됩니다.\n",
        "\n",
        "\\*\\*단어 임베딩(Word Embedding)\\*\\*은 이런 한계를 극복하기 위해 등장했습니다. 단어를 저차원(보통 100\\~300차원)의 \\*\\*밀집 벡터(Dense Vector)\\*\\*로 표현하며, 이 벡터 공간 안에 단어의 의미와 문맥 정보를 압축하여 담아냅니다. **Word2Vec**은 가장 대표적인 단어 임베딩 모델로, \"비슷한 문맥에서 등장하는 단어는 비슷한 의미를 가진다\"는 **분포 가설**을 기반으로 합니다.\n",
        "\n",
        "  - **CBOW (Continuous Bag-of-Words)**: 주변 단어들을 이용해 중심 단어를 예측합니다. (`[___]가 방에 들어간다` -\\> `아버지`)\n",
        "  - **Skip-gram**: 중심 단어를 이용해 주변 단어들을 예측합니다. (`아버지` -\\> `[___]가 방에 들어간다`)\n",
        "\n",
        "일반적으로 Skip-gram 방식이 더 많은 학습을 수행하여 희귀 단어나 데이터가 방대할 때 성능이 더 좋다고 알려져 있습니다.\n",
        "\n",
        "#### 💻 예시 코드 (Example Code)\n",
        "\n",
        "Word2Vec 모델을 학습시키려면 '토큰화된 문장들의 리스트' 형태의 데이터가 필요합니다. `gensim` 라이브러리를 사용하여 Word2Vec 모델을 학습하고 활용해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "TjYUITWH_lrF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "토큰화된 데이터 (일부): ['배우', '연기력', '영화']\n"
          ]
        }
      ],
      "source": [
        "from kiwipiepy import Kiwi\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# 1. 모델 학습을 위한 데이터 준비 (토큰화된 문장 리스트)\n",
        "# 실제 프로젝트에서는 대용량의 텍스트 데이터가 필요합니다.\n",
        "# corpus = [\n",
        "#     '배우의 연기력이 정말 대단한 영화였어요',\n",
        "#     '스토리가 너무 예측 가능해서 연기력이 아까웠다',\n",
        "#     '감독의 연출과 배우의 연기가 조화로웠던 영화',\n",
        "#     '이 영화의 배우들은 연기를 정말 잘한다',\n",
        "#     '스토리 구성이 탄탄해서 좋았던 작품이다',\n",
        "#     '연출이 아쉬웠지만 배우들의 연기는 최고였다',\n",
        "# ]\n",
        "\n",
        "kiwi = Kiwi()\n",
        "# 명사, 동사, 형용사만 추출\n",
        "tokenized_corpus = [\n",
        "    [token.form for token in kiwi.tokenize(doc) if token.tag in ['NNG', 'NNP', 'VV', 'VA']]\n",
        "    for doc in corpus\n",
        "]\n",
        "print(\"토큰화된 데이터 (일부):\", tokenized_corpus[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Word2Vec 모델 학습\n",
        "# vector_size: 임베딩 벡터의 차원\n",
        "# window: 학습 시 고려할 주변 단어의 개수\n",
        "# min_count: 학습에 사용할 단어의 최소 빈도\n",
        "# sg=1: Skip-gram 방식 사용 (0은 CBOW)\n",
        "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=3, min_count=1, sg=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'배우'와 가장 유사한 단어: [('처음', 0.21915924549102783), ('예측', 0.21625031530857086), ('주연', 0.2043164223432541)]\n"
          ]
        }
      ],
      "source": [
        "# 3. 학습된 임베딩 활용\n",
        "# '배우'와 가장 유사한 단어 찾기\n",
        "similar_words = model.wv.most_similar('배우', topn=3)\n",
        "print(\"'배우'와 가장 유사한 단어:\", similar_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'연기'와 '스토리'의 유사도: 0.0678\n"
          ]
        }
      ],
      "source": [
        "# 두 단어의 유사도 계산\n",
        "similarity = model.wv.similarity('연기', '스토리')\n",
        "print(f\"'연기'와 '스토리'의 유사도: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'배우'의 임베딩 벡터 (처음 10개 값): [-0.00053156  0.00023815  0.0050992   0.00900977 -0.0093051  -0.00711709\n",
            "  0.00645848  0.00898237 -0.00503068 -0.00377373]\n"
          ]
        }
      ],
      "source": [
        "# 단어의 임베딩 벡터 확인\n",
        "vector_actor = model.wv['배우']\n",
        "print(\"'배우'의 임베딩 벡터 (처음 10개 값):\", vector_actor[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1o043ZK_lrF"
      },
      "source": [
        "#### ✏️ 연습 문제 (Practice Problems)\n",
        "\n",
        "1.  Word2Vec 모델을 학습시킬 때, `vector_size`를 200으로, `window`를 5로 변경하여 새로운 모델을 만들고, '영화'와 가장 유사한 단어를 찾아보세요. 결과가 어떻게 달라지는지 확인해 보세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 코드 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  학습된 모델을 사용하여 '연기'와 '연출'의 유사도, '연기'와 '스토리'의 유사도를 각각 계산하고 비교 분석해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 코드 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "-----\n",
        "\n",
        "### 4\\. FastText: OOV 문제를 해결하다\n",
        "\n",
        "#### 💡 개념 (Concept)\n",
        "\n",
        "Word2Vec은 강력하지만, 학습 데이터에 등장하지 않은 단어, 즉 **OOV(Out-of-Vocabulary)** 단어에 대해서는 임베딩 벡터를 생성할 수 없다는 치명적인 단점이 있습니다.\n",
        "\n",
        "**FastText**는 Word2Vec의 Skip-gram 모델을 확장하여 이 문제를 해결했습니다. FastText의 핵심은 단어를 통째로 하나의 단위로 보지 않고, \\*\\*문자 단위의 n-gram(character n-gram)\\*\\*으로 세분화하여 보는 것입니다. 예를 들어, '영화'라는 단어를 3-gram으로 분해하면 `<영, 영화, 화>` 와 같은 부분 문자열들의 집합으로 표현됩니다. (여기서 `<, >`는 단어의 시작과 끝을 알리는 특수 문자입니다.)\n",
        "\n",
        "이러한 접근 방식 덕분에, 학습 시 보지 못한 '감독님'이라는 OOV 단어가 등장해도, 그 단어를 구성하는 '감독', '독님', '님\\>' 등의 부분 문자열 벡터들의 합으로 '감독님'의 전체 벡터를 추정할 수 있습니다. 이는 특히 신조어나 오탈자가 많은 한국어 처리에 매우 유용합니다.\n",
        "\n",
        "#### 💻 예시 코드 (Example Code)\n",
        "\n",
        "`gensim`의 `FastText` 모델은 `Word2Vec`과 사용법이 거의 동일합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "pvUrtcur_lrF"
      },
      "outputs": [],
      "source": [
        "from kiwipiepy import Kiwi\n",
        "from gensim.models import FastText\n",
        "\n",
        "# 1. 모델 학습을 위한 데이터 준비 (위의 코퍼스 사용)\n",
        "kiwi = Kiwi()\n",
        "tokenized_corpus = [\n",
        "    [token.form for token in kiwi.tokenize(doc) if token.tag in ['NNG', 'NNP', 'VV', 'VA']]\n",
        "    for doc in corpus\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. FastText 모델 학습 (Word2Vec과 파라미터 동일)\n",
        "ft_model = FastText(sentences=tokenized_corpus, vector_size=100, window=3, min_count=1, sg=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FastText: OOV 단어 '연출가'의 벡터를 성공적으로 생성했습니다.\n"
          ]
        }
      ],
      "source": [
        "# 3. OOV 단어 테스트\n",
        "oov_word = '연출가' # '연출'은 사전에 있지만 '연출가'는 코퍼스와 사전에 존재하지 않음\n",
        "\n",
        "# FastText는 OOV 단어의 벡터를 생성할 수 있음\n",
        "oov_vector = ft_model.wv[oov_word]\n",
        "print(f\"FastText: OOV 단어 '{oov_word}'의 벡터를 성공적으로 생성했습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "'연출가'와 유사한 단어들: [('전체', 0.21631333231925964), ('자연', 0.19833992421627045), ('연출', 0.1884857416152954), ('예측', 0.18476420640945435), ('연기력', 0.16182444989681244), ('대단하', 0.161748006939888), ('감동', 0.13976065814495087), ('완벽', 0.13893361389636993), ('만족', 0.1362549513578415), ('센스', 0.13480506837368011)]\n"
          ]
        }
      ],
      "source": [
        "# 생성된 OOV 벡터로 유사 단어 찾기\n",
        "similar_to_oov = ft_model.wv.most_similar(oov_word)\n",
        "print(f\"\\n'{oov_word}'와 유사한 단어들: {similar_to_oov}\")\n",
        "\n",
        "# Word2Vec 모델(이전 섹션에서 생성)과 비교\n",
        "# model.wv[oov_word] # -> 이 코드는 KeyError를 발생시킴"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Xm636s__CP"
      },
      "source": [
        "#### ✏️ 연습 문제 (Practice Problems)\n",
        "\n",
        "1.  `FastText` 모델을 사용하여, 학습 말뭉치에 없을 법한 단어(예: '시나리오', '미장센' 등)의 임베딩 벡터가 생성되는지 확인하고, 해당 단어와 가장 유사한 단어들을 찾아보세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  위에서 학습한 `Word2Vec` 모델과 `FastText` 모델에서 각각 '스토리'와 가장 유사한 단어를 찾아보고, 그 결과가 어떻게 다른지 비교해 보세요. `FastText`가 더 나은 결과를 보이는 경향이 있다면 그 이유는 무엇일지 설명해 보세요."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
