{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ íŒŒì´ì¬ í…ìŠ¤íŠ¸ ë¶„ì„: ê³ ì „ì  ë²¡í„°í™” ê¸°ë²• ì‹¤ìŠµ ê³¼ì œ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ 'ê³ ì „ì  ë²¡í„°í™” ê¸°ë²• ë§ˆìŠ¤í„°í•˜ê¸°' íŠœí† ë¦¬ì–¼ì˜ ì—°ìŠµ ë¬¸ì œì™€ ìµœì¢… ì‹¤ìŠµ í”„ë¡œì íŠ¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kiwipiepy scikit-learn pandas gensim plotly_express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kiwipiepy import Kiwi\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag-of-Words (BoW)ì™€ DTM: ì—°ìŠµ ë¬¸ì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì‹¤ìŠµìš© ë°ì´í„°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'ë°°ìš°ì˜ ì—°ê¸°ë ¥ì´ ì •ë§ ëŒ€ë‹¨í•œ ì˜í™”ì˜€ì–´ìš”.',\n",
    "    'ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ì˜ˆì¸¡ ê°€ëŠ¥í•´ì„œ ì—°ê¸°ë ¥ì´ ì•„ê¹Œì› ë‹¤.',\n",
    "    'ê°ë…ì˜ ì—°ì¶œê³¼ ë°°ìš°ì˜ ì—°ê¸°ê°€ ì¡°í™”ë¡œì› ë˜ ì˜í™”.',\n",
    "    'ì™€ ì´ ì˜í™” ì§„ì§œ ëŒ€ë°•ì´ì•¼! ë°°ìš°ë“¤ ì—°ê¸° ë¯¸ì³¤ê³  ìŠ¤í† ë¦¬ë„ ì™„ì „ ëª°ì…ë¨',\n",
    "    'ìŒ... ì¢€ ì•„ì‰½ë„¤ìš”. ê°ë…ì´ ë­˜ ë§í•˜ê³  ì‹¶ì—ˆëŠ”ì§€ ëª¨ë¥´ê² ì–´ìš”',\n",
    "    'ì—°ê¸°ëŠ” ê´œì°®ì•˜ëŠ”ë° ê²°ë§ì´ ë„ˆë¬´ ë»”í•´ì„œ ì‹¤ë§í–ˆìŠµë‹ˆë‹¤',\n",
    "    'í— ì´ê±° ì™„ì „ ê¿€ì¼ã…‹ã…‹ ì˜ˆìƒ ëª»í•œ ë°˜ì „ì— ì†Œë¦„ë‹ì•˜ì–´',\n",
    "    'ê°ë…ë‹˜... ì œë°œ ì¢€ ë” ì‹ ê²½ì¨ì„œ ì°ìœ¼ì‹œê¸¸... ì—°ì¶œì´ ì—‰ë§ì´ì—ìš”',\n",
    "    'ì£¼ì—°ë°°ìš° ì—°ê¸° ì§„ì§œ ìì—°ìŠ¤ëŸ½ë”ë¼! ëª°ì…ë„ ìµœê³ ì˜€ìŒ',\n",
    "    'ìŠ¤í† ë¦¬ê°€ ì¡°ê¸ˆ ë³µì¡í•˜ê¸´ í–ˆì§€ë§Œ ë‚˜ë¦„ ë³¼ë§Œí–ˆì–´ìš”',\n",
    "    'ì´ëŸ° ì˜í™”ë¥¼ ì™œ ë§Œë“¤ì—ˆëŠ”ì§€ ì´í•´ê°€ ì•ˆ ê°€ë„¤... ì‹œê°„ ì•„ê¹Œì›€',\n",
    "    'ë°°ìš°ë“¤ ì¼€ë¯¸ ì™„ì „ ì¢‹ì•˜ê³  ì—°ì¶œë„ ê¹”ë”í–ˆìŒ. ì¶”ì²œ!',\n",
    "    'ì˜ˆì¸¡í•  ìˆ˜ ì—†ëŠ” ì „ê°œë¡œ ëê¹Œì§€ ì¶œê¸´ì¥ê° ë„˜ì³¤ìŠµë‹ˆë‹¤',\n",
    "    'ì—°ê¸°ë ¥ì€ ì¸ì •í•˜ì§€ë§Œ ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ë»”í•´ì„œ... ê·¸ëƒ¥ ê·¸ë˜ìš”',\n",
    "    'ê°ë…ì˜ ì˜ë„ëŠ” ì¢‹ì•˜ìœ¼ë‚˜ í‘œí˜„ ë°©ì‹ì´ ì•„ì‰¬ì› ë„¤ìš”',\n",
    "    'ã…‹ã…‹ã…‹ ì´ê±° ë­ì•¼ ì™„ì „ ì¬ë°Œì–ì•„? ë°°ìš°ë“¤ ì—°ê¸° ã„¹ã…‡ ëŒ€ë‹¨í•¨',\n",
    "    'ì¡°ìš©í•œ ì˜í™”ì¸ë° ë°°ìš°ë“¤ ì—°ê¸°ê°€ ì›Œë‚™ ì¢‹ì•„ì„œ ì§€ë£¨í•˜ì§€ ì•Šì•˜ì–´ìš”',\n",
    "    'ì•¡ì…˜ì€ ë³„ë¡œì˜€ì§€ë§Œ ì¸ê°„ê´€ê³„ ë“œë¼ë§ˆê°€ íƒ„íƒ„í•´ì„œ ë§Œì¡±',\n",
    "    'ì•„ ì§„ì§œ... ì™œ ì´ë ‡ê²Œ ë§Œë“¤ì—ˆì„ê¹Œ? ê°ë… ë­í•˜ëŠ” ê±°ì•¼',\n",
    "    'ì²˜ìŒì—” ì§€ë£¨í–ˆëŠ”ë° ì¤‘ë°˜ë¶€í„° ì™„ì „ ëª°ì…! ì—°ì¶œ ì„¼ìŠ¤ ìˆë„¤',\n",
    "    'ë°°ìš°ë“¤ ì—°ê¸°ëŠ” ì¢‹ì•˜ì§€ë§Œ ì „ì²´ì ìœ¼ë¡œ ë°‹ë°‹í•œ ëŠë‚Œì´ì—ìš”',\n",
    "    'ì™€... ì´ëŸ° ìŠ¤í† ë¦¬ëŠ” ì²˜ìŒ ë´! ì •ë§ ì‹ ì„ í•˜ê³  ê°ë™ì ì´ì—ˆì–´',\n",
    "    'ì—°ì¶œê³¼ ì—°ê¸° ëª¨ë‘ ì™„ë²½í–ˆìŠµë‹ˆë‹¤. ì˜¬í•´ ìµœê³ ì˜ ì‘í’ˆ ì¤‘ í•˜ë‚˜!',\n",
    "    'ë­”ê°€ ì•„ì‰¬ìš´ ë¶€ë¶„ë“¤ì´ ìˆì§€ë§Œ ê·¸ë˜ë„ ë³¼ë§Œí•œ ì˜í™”ì˜€ì–´ìš”'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ë¬¸ì œ 1: í† í¬ë‚˜ì´ì €ë¥¼ ìˆ˜ì •í•˜ì—¬ ì¼ë°˜ ëª…ì‚¬(`NNG`)ë§Œ ì¶”ì¶œí•˜ê³  DTMì„ ìƒì„±í•˜ì„¸ìš”.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—°ìŠµ ë¬¸ì œ 1ë²ˆ í’€ì´ ê³µê°„\n",
    "\n",
    "# 1. NNGë§Œ ì¶”ì¶œí•˜ëŠ” í† í¬ë‚˜ì´ì € ì •ì˜\n",
    "def nng_tokenizer(text: str) -> list[str]:\n",
    "    # YOUR CODE HERE\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [token.form for token in tokens if token.tag == 'NNG']\n",
    "\n",
    "# 2. CountVectorizer ìƒì„± ë° DTM êµ¬ì¶•\n",
    "nng_vectorizer = CountVectorizer(tokenizer=nng_tokenizer)\n",
    "dtm_nng = nng_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 3. ê²°ê³¼ í™•ì¸\n",
    "feature_names_nng = nng_vectorizer.get_feature_names_out()\n",
    "dtm_nng_df = pd.DataFrame(dtm_nng.toarray(), columns=feature_names_nng)\n",
    "\n",
    "print(\"ì–´íœ˜ ì‚¬ì „ (NNG):\")\n",
    "print(feature_names_nng)\n",
    "print(\"\\nDTM (NNG):\")\n",
    "print(dtm_nng_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ë¬¸ì œ 2: `ngram_range=(1, 2)`ë¥¼ ì‚¬ìš©í•˜ì—¬ DTMì„ ë§Œë“¤ê³  ì–´íœ˜ ì‚¬ì „ì„ í™•ì¸í•˜ì„¸ìš”.**\n",
    "\n",
    "* **`ngram_range=(1, 2)`ì˜ ì˜ë¯¸:** ë‹¨ì–´ë¥¼ 1ê°œì”©(unigram) ê·¸ë¦¬ê³  ì—°ì†ëœ 2ê°œì”©(bigram) ë¬¶ì–´ì„œ í† í°ìœ¼ë¡œ ì‚¬ìš©í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 'ë°°ìš° ì—°ê¸°'ê°€ í† í°í™”ë˜ë©´, 'ë°°ìš°', 'ì—°ê¸°' (unigrams) ë¿ë§Œ ì•„ë‹ˆë¼ 'ë°°ìš° ì—°ê¸°' (bigram)ë„ í•˜ë‚˜ì˜ í”¼ì²˜ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‹¨ì–´ì˜ ìˆœì„œ ì •ë³´ë¥¼ ì¼ë¶€ ë³´ì¡´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—°ìŠµ ë¬¸ì œ 2ë²ˆ í’€ì´ ê³µê°„\n",
    "\n",
    "# íŠœí† ë¦¬ì–¼ì˜ ê¸°ë³¸ í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ì‹œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "def kiwi_tokenizer(text: str) -> list[str]:\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [token.form for token in tokens if token.tag in ['NNG', 'NNP', 'VV', 'VA']]\n",
    "\n",
    "# 1. ngram_rangeë¥¼ ì ìš©í•˜ì—¬ CountVectorizer ìƒì„±\n",
    "ngram_vectorizer = CountVectorizer(tokenizer=kiwi_tokenizer, ngram_range=(1, 2))\n",
    "dtm_ngram = ngram_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 2. ê²°ê³¼ í™•ì¸\n",
    "feature_names_ngram = ngram_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"ì–´íœ˜ ì‚¬ì „ (ngram_range=(1, 2)):\")\n",
    "print(feature_names_ngram)\n",
    "\n",
    "dtm_ngram_df = pd.DataFrame(dtm_ngram.toarray(), columns=feature_names_ngram)\n",
    "print(\"\\nDTM (ngram_range=(1, 2)):\")\n",
    "print(dtm_ngram_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-IDF: ì—°ìŠµ ë¬¸ì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ë¬¸ì œ 1: í•™ìŠµëœ `TfidfVectorizer`ì˜ `idf_` ì†ì„±ì„ í™•ì¸í•˜ê³ , IDF ê°’ì´ ê°€ì¥ ë†’ì€ ë‹¨ì–´ì™€ ë‚®ì€ ë‹¨ì–´ë¥¼ ì°¾ì•„ ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—°ìŠµ ë¬¸ì œ 1ë²ˆ í’€ì´ ê³µê°„\n",
    "\n",
    "# íŠœí† ë¦¬ì–¼ ì˜ˆì œ ì½”ë“œ ì¬ì‚¬ìš©\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=kiwi_tokenizer)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 1. ë‹¨ì–´ì™€ IDF ê°’ì„ ë¬¶ì–´ì„œ DataFrame ìƒì„±\n",
    "idf_df = pd.DataFrame({\n",
    "    'word': tfidf_vectorizer.get_feature_names_out(),\n",
    "    'idf': tfidf_vectorizer.idf_\n",
    "})\n",
    "\n",
    "idf_df_sorted = idf_df.sort_values('idf', ascending=False)\n",
    "\n",
    "print(idf_df_sorted)\n",
    "\n",
    "print(\"\\n--- ë¶„ì„ ---\")\n",
    "print(\"ê°€ì¥ IDFê°€ ë†’ì€ ë‹¨ì–´ë“¤(ê°€ì¥ í¬ê·€í•œ ë‹¨ì–´ë“¤): 'ê°ë…', 'ëŒ€ë‹¨', 'ìŠ¤í† ë¦¬' ë“±. ì´ ë‹¨ì–´ë“¤ì€ ë‹¨ í•˜ë‚˜ì˜ ë¬¸ì„œì—ë§Œ ë“±ì¥í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\")\n",
    "print(\"ê°€ì¥ IDFê°€ ë‚®ì€ ë‹¨ì–´ë“¤(ê°€ì¥ í”í•œ ë‹¨ì–´ë“¤): 'ë°°ìš°', 'ì—°ê¸°', 'ì˜í™”'. ì´ ë‹¨ì–´ë“¤ì€ ì—¬ëŸ¬ ë¬¸ì„œì— ê±¸ì³ ë“±ì¥í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ë¬¸ì œ 2: `max_df=0.8`, `min_df=2` íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•˜ì—¬ TF-IDF í–‰ë ¬ì„ ë‹¤ì‹œ ë§Œë“¤ê³  ì–´íœ˜ ì‚¬ì „ì˜ ë³€í™”ë¥¼ í™•ì¸í•˜ì„¸ìš”.**\n",
    "\n",
    "* `max_df=0.8`: ë‹¨ì–´ê°€ ì „ì²´ ë¬¸ì„œì˜ 80%ë¥¼ ì´ˆê³¼í•˜ì—¬ ë‚˜íƒ€ë‚˜ë©´ ì–´íœ˜ ì‚¬ì „ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤. ë„ˆë¬´ í”í•œ ë‹¨ì–´ë¥¼ ì œê±°í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. (0~1 ì‚¬ì´ì˜ float ê°’ì€ ë¹„ìœ¨, ì •ìˆ˜ ê°’ì€ ë¬¸ì„œ ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.)\n",
    "* `min_df=2`: ë‹¨ì–´ê°€ ìµœì†Œ 2ê°œ ì´ìƒì˜ ë¬¸ì„œì— ë‚˜íƒ€ë‚˜ì•¼ë§Œ ì–´íœ˜ ì‚¬ì „ì— í¬í•¨ì‹œí‚µë‹ˆë‹¤. ë„ˆë¬´ í¬ê·€í•œ ë‹¨ì–´ë‚˜ ì˜¤íƒˆìë¥¼ ì œê±°í•˜ëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—°ìŠµ ë¬¸ì œ 2ë²ˆ í’€ì´ ê³µê°„\n",
    "\n",
    "# 1. max_df, min_df íŒŒë¼ë¯¸í„°ë¥¼ ì ìš©í•˜ì—¬ TfidfVectorizer ìƒì„±\n",
    "tfidf_vectorizer_filtered = TfidfVectorizer(\n",
    "    tokenizer=kiwi_tokenizer,\n",
    "    max_df=0.8,\n",
    "    min_df=2\n",
    ")\n",
    "tfidf_matrix_filtered = tfidf_vectorizer_filtered.fit_transform(corpus)\n",
    "\n",
    "# 2. ê²°ê³¼ í™•ì¸\n",
    "feature_names_filtered = tfidf_vectorizer_filtered.get_feature_names_out()\n",
    "\n",
    "print(\"í•„í„°ë§ëœ ì–´íœ˜ ì‚¬ì „:\")\n",
    "print(feature_names_filtered)\n",
    "\n",
    "print(\"\\n--- ë¶„ì„ ---\")\n",
    "print(\"min_df=2 ì¡°ê±´ì— ë”°ë¼, 2ê°œ ë¯¸ë§Œì˜ ë¬¸ì„œì— ë“±ì¥í•œ 'ê°ë…', 'ëŒ€ë‹¨', 'ìŠ¤í† ë¦¬', 'ì•„ê¹ë‹¤', 'ì˜ˆì¸¡', 'ì—°ì¶œ', 'ì¡°í™”', 'ê°€ëŠ¥' ë“±ì˜ ë‹¨ì–´ë“¤ì´ ëª¨ë‘ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"'ë°°ìš°', 'ì—°ê¸°ë ¥', 'ì˜í™”'ëŠ” 2ê°œ ì´ìƒì˜ ë¬¸ì„œì— ë“±ì¥í–ˆìœ¼ë¯€ë¡œ ì–´íœ˜ ì‚¬ì „ì— ë‚¨ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "print(\"(max_df=0.8ì€ 3ê°œ ë¬¸ì„œ ì¤‘ 80%ì¸ 2.4ê°œë¥¼ ì´ˆê³¼í•˜ëŠ”, ì¦‰ 3ê°œ ë¬¸ì„œ ëª¨ë‘ì— ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ë¥¼ ì œì™¸í•˜ëŠ”ë°, í˜„ì¬ ê·¸ëŸ° ë‹¨ì–´ê°€ ì—†ìœ¼ë¯€ë¡œ ì´ ì¡°ê±´ì€ ì•„ë¬´ëŸ° ì˜í–¥ì„ ì£¼ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 & 4. Word2Vec & FastText: ì—°ìŠµ ë¬¸ì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì‹¤ìŠµìš© ë°ì´í„°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ\n",
    "tokenized_corpus = [\n",
    "    [token.form for token in kiwi.tokenize(doc) if token.tag in ['NNG', 'NNP', 'VV', 'VA']]\n",
    "    for doc in corpus\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec ë¬¸ì œ 1: `vector_size=200`, `window=5`ë¡œ ë³€ê²½í•˜ì—¬ ëª¨ë¸ì„ ë§Œë“¤ê³  'ì˜í™”'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ì°¾ìœ¼ì„¸ìš”.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—°ìŠµ ë¬¸ì œ 1ë²ˆ í’€ì´ ê³µê°„ (Word2Vec)\n",
    "\n",
    "model_w2v_2 = Word2Vec(sentences=tokenized_corpus, vector_size=200, window=5, min_count=1, sg=1)\n",
    "\n",
    "try:\n",
    "    similar_to_movie = model_w2v_2.wv.most_similar('ì˜í™”', topn=3)\n",
    "    print(\"'ì˜í™”'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ (vector_size=200, window=5):\", similar_to_movie)\n",
    "except KeyError as e:\n",
    "    print(f\"ì˜¤ë¥˜: '{e.args[0]}' ë‹¨ì–´ê°€ ì–´íœ˜ ì‚¬ì „ì— ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec ë¬¸ì œ 2: 'ì—°ê¸°' vs 'ì—°ì¶œ', 'ì—°ê¸°' vs 'ìŠ¤í† ë¦¬' ìœ ì‚¬ë„ë¥¼ ë¹„êµ ë¶„ì„í•˜ì„¸ìš”.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—°ìŠµ ë¬¸ì œ 2ë²ˆ í’€ì´ ê³µê°„ (Word2Vec)\n",
    "model_w2v_1 = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=3, min_count=1, sg=1)\n",
    "\n",
    "try:\n",
    "    sim_acting_directing = model_w2v_1.wv.similarity('ì—°ê¸°', 'ì—°ì¶œ')\n",
    "    sim_acting_story = model_w2v_1.wv.similarity('ì—°ê¸°', 'ìŠ¤í† ë¦¬')\n",
    "\n",
    "    print(f\"'ì—°ê¸°' vs 'ì—°ì¶œ' ìœ ì‚¬ë„: {sim_acting_directing:.4f}\")\n",
    "    print(f\"'ì—°ê¸°' vs 'ìŠ¤í† ë¦¬' ìœ ì‚¬ë„: {sim_acting_story:.4f}\")\n",
    "    print(\"\\n--- ë¶„ì„ ---\")\n",
    "    print(\"í•™ìŠµ ë°ì´í„°ê°€ ì‘ì•„ ê²°ê³¼ê°€ ë§¤ë²ˆ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì§€ë§Œ, 'ì—°ê¸°'ì™€ 'ì—°ì¶œ'ì´ 'ì—°ê¸°'ì™€ 'ìŠ¤í† ë¦¬'ë³´ë‹¤ ë” ìœ ì‚¬í•˜ê²Œ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"'ì—°ê¸°'ì™€ 'ì—°ì¶œ'ì€ 'ë°°ìš°', 'ê°ë…' ë“±ê³¼ í•¨ê»˜ ì˜í™”ì˜ ì œì‘/í‘œí˜„ ìš”ì†Œë¥¼ êµ¬ì„±í•˜ë¯€ë¡œ ë” ê°•í•œ ë¬¸ë§¥ì  ì—°ê´€ì„±ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "except KeyError as e:\n",
    "    print(f\"ì˜¤ë¥˜: '{e.args[0]}' ë‹¨ì–´ê°€ ì–´íœ˜ ì‚¬ì „ì— ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FastText ë¬¸ì œ 1: OOV ë‹¨ì–´ 'ì‹œë‚˜ë¦¬ì˜¤'ì˜ ìœ ì‚¬ ë‹¨ì–´ë¥¼ ì°¾ì•„ë³´ì„¸ìš”.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—°ìŠµ ë¬¸ì œ 1ë²ˆ í’€ì´ ê³µê°„ (FastText)\n",
    "\n",
    "ft_model = FastText(sentences=tokenized_corpus, vector_size=100, window=3, min_count=1, sg=1)\n",
    "\n",
    "oov_word = 'ì‹œë‚˜ë¦¬ì˜¤'\n",
    "\n",
    "try:\n",
    "    similar_to_oov = ft_model.wv.most_similar(oov_word)\n",
    "    print(f\"OOV ë‹¨ì–´ '{oov_word}'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´: {similar_to_oov}\")\n",
    "    print(\"\\n--- ë¶„ì„ ---\")\n",
    "    print(\"'ì‹œë‚˜ë¦¬ì˜¤'ëŠ” í•™ìŠµ ë§ë­‰ì¹˜ì— ì—†ì§€ë§Œ, ì´ë¥¼ êµ¬ì„±í•˜ëŠ” 'ì‹œë‚˜ë¦¬', 'ë‚˜ë¦¬ì˜¤' ë“±ì˜ ë¬¸ì n-gram ì •ë³´ë¥¼ í†µí•´ 'ìŠ¤í† ë¦¬', 'êµ¬ì„±'ê³¼ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ê³¼ ê°€ê¹ê²Œ ë²¡í„°ê°€ í˜•ì„±ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"FastText ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¼ ì‹¤ìŠµ í”„ë¡œì íŠ¸: ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„ ë° í•µì‹¬ì–´ ì‹œê°í™”\n",
    "\n",
    "**ë°ì´í„° ë‹¤ìš´ë¡œë“œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naver Sentiment Movie Corpus ë‹¤ìš´ë¡œë“œ\n",
    "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt -O ratings_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ (íƒ­ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ ìˆìŒ)\n",
    "df = pd.read_csv('ratings_train.txt', sep='\\t')\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì œê±°\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# ë¶„ì„ì„ ìœ„í•´ ë°ì´í„° 1000ê°œ ìƒ˜í”Œë§ (ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì „ì²´ ë°ì´í„° ì‚¬ìš©)\n",
    "df_sample = df.sample(n=1000, random_state=42)\n",
    "\n",
    "# kiwipiepy í† í¬ë‚˜ì´ì € (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ë¶€ì‚¬ ì¶”ì¶œ)\n",
    "def nsmc_tokenizer(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [token.form for token in tokens if token.tag in ['NNG', 'NNP', 'VV', 'VA', 'MAG']]\n",
    "\n",
    "# X (ë¦¬ë·°), y (ë¼ë²¨) ë¶„ë¦¬\n",
    "X = df_sample['document']\n",
    "y = df_sample['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. TF-IDF ê¸°ë°˜ ëª¨ë¸ë§ ë° í•µì‹¬ì–´ ì¶”ì¶œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# TF-IDF ë²¡í„°í™”\n",
    "tfidf_vec = TfidfVectorizer(tokenizer=nsmc_tokenizer, ngram_range=(1, 2), min_df=3, max_df=0.9)\n",
    "X_tfidf = tfidf_vec.fit_transform(X)\n",
    "\n",
    "# ë°ì´í„° ë¶„ë¦¬\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(f\"TF-IDF ê¸°ë°˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ ì •í™•ë„: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# ê¸ì •/ë¶€ì • í•µì‹¬ì–´ ì¶”ì¶œ\n",
    "feature_names = tfidf_vec.get_feature_names_out()\n",
    "coef_df = pd.DataFrame({'word': feature_names, 'coef': lr.coef_.flatten()})\n",
    "\n",
    "print(\"\\n--- ê¸ì • ë¦¬ë·° í•µì‹¬ì–´ TOP 10 ---\")\n",
    "print(coef_df.sort_values('coef', ascending=False).head(10))\n",
    "\n",
    "print(\"\\n--- ë¶€ì • ë¦¬ë·° í•µì‹¬ì–´ TOP 10 ---\")\n",
    "print(coef_df.sort_values('coef', ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. BM25 ê¸°ë°˜ ëª¨ë¸ë§ ë° ê´€ë ¨ë„ ì¶”ì¶œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# BM25ë¥¼ ìœ„í•œ ë¬¸ì„œ í† í°í™”\n",
    "tokenized_docs = [nsmc_tokenizer(doc) for doc in X]\n",
    "\n",
    "# BM25 ê°ì²´ ìƒì„±\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§ˆì˜ ì •ì˜\n",
    "queries = [\"ì¬ë¯¸ìˆëŠ” ì˜í™”\", \"ì§€ë£¨í•œ ìŠ¤í† ë¦¬\", \"í›Œë¥­í•œ ì—°ê¸°\"]\n",
    "\n",
    "print(\"=== BM25 ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ===\\n\")\n",
    "\n",
    "for query in queries:\n",
    "    # ì§ˆì˜ í† í°í™”\n",
    "    tokenized_query = nsmc_tokenizer(query)\n",
    "    print(f\"ì§ˆì˜: '{query}'\")\n",
    "    print(f\"í† í°í™”ëœ ì§ˆì˜: {tokenized_query}\")\n",
    "    \n",
    "    # BM25 ì ìˆ˜ ê³„ì‚°\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # ìƒìœ„ 5ê°œ ë¬¸ì„œ ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
    "    top_docs = np.argsort(doc_scores)[::-1][:5]\n",
    "    \n",
    "    print(f\"\\nìƒìœ„ 5ê°œ ê´€ë ¨ ë¬¸ì„œ:\")\n",
    "    for i, doc_idx in enumerate(top_docs):\n",
    "        print(f\"{i+1}. ë¬¸ì„œ {doc_idx} (ì ìˆ˜: {doc_scores[doc_idx]:.4f})\")\n",
    "        print(f\"   ë‚´ìš©: {X.iloc[doc_idx][:100]}...\")\n",
    "        print(f\"   ë¼ë²¨: {'ê¸ì •' if y.iloc[doc_idx] == 1 else 'ë¶€ì •'}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. (ì‹¬í™”) Word2Vec ì„ë² ë”© í•™ìŠµ ë° ì‹œê°í™”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ìƒ˜í”Œ ë°ì´í„° í† í°í™”\n",
    "tokenized_nsmc = [nsmc_tokenizer(doc) for doc in X]\n",
    "\n",
    "# Word2Vec ëª¨ë¸ í•™ìŠµ\n",
    "w2v_nsmc_model = Word2Vec(sentences=tokenized_nsmc, vector_size=100, window=5, min_count=5, workers=4, sg=1)\n",
    "\n",
    "# ì£¼ìš” ë‹¨ì–´ ë° ìœ ì‚¬ì–´ ë¦¬ìŠ¤íŠ¸\n",
    "target_words = ['ì˜í™”', 'ì¬ë¯¸', 'ìµœê³ ', 'ë°°ìš°', 'ê°ë™', 'ì“°ë ˆê¸°', 'ì§€ë£¨í•˜ë‹¤']\n",
    "vocab_to_show = []\n",
    "for word in target_words:\n",
    "    if word in w2v_nsmc_model.wv:\n",
    "        vocab_to_show.append(word)\n",
    "        vocab_to_show.extend([w for w, s in w2v_nsmc_model.wv.most_similar(word, topn=5)])\n",
    "\n",
    "vocab_to_show = list(set(vocab_to_show)) # ì¤‘ë³µ ì œê±°\n",
    "\n",
    "# ì‹œê°í™”ë¥¼ ìœ„í•œ ë‹¨ì–´ ë²¡í„° ì¶”ì¶œ\n",
    "word_vectors = np.array([w2v_nsmc_model.wv[word] for word in vocab_to_show])\n",
    "\n",
    "# PCAë¡œ 2ì°¨ì› ì¶•ì†Œ\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(word_vectors)\n",
    "\n",
    "pca_df = pd.DataFrame(pca_result, columns=['x', 'y'])\n",
    "pca_df['word'] = vocab_to_show\n",
    "\n",
    "# Plotly Expressë¡œ ì‹œê°í™”\n",
    "fig = px.scatter(pca_df, x='x', y='y', text='word', title='Word2Vec ì„ë² ë”© ì‹œê°í™” (PCA)')\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
