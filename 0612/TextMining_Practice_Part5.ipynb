{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39NQWzORTx_p"
      },
      "source": [
        "## 🐍 Python 텍스트 분석: Embedding API 정복하기 (Part 5)\n",
        "\n",
        "이 학습자료에서는 `OpenAI`, `OpenRouter`, `Hugging Face` 등 주요 AI 플랫폼들의 임베딩 API를 사용하여 텍스트를 벡터로 변환하는 방법을 다룹니다. \n",
        "\n",
        "각 서비스의 특징과 사용법을 익히고, 이를 손쉽게 전환하며 사용하는 실용적인 코드 패턴까지 학습합니다.\n",
        "\n",
        "-----\n",
        "\n",
        "## 0\\. 준비 : 공통 환경 세팅\n",
        "\n",
        "```bash\n",
        "# 1) Python 가상환경 준비 (여러분들이 사용하시는 로컬 환경으로 설정 요망)\n",
        "python -m venv .venv && source .venv/bin/activate\n",
        "\n",
        "# 2) 공통 패키지\n",
        "pip install --upgrade openai tiktoken numpy requests sentence-transformers\n",
        "```\n",
        "\n",
        "환경 변수로 API 키를 보관하는 것이 가장 안전합니다.\n",
        "\n",
        "### 환경 변수 설정 방법\n",
        "\n",
        "**1: .env 파일 사용 (권장)**\n",
        "\n",
        "프로젝트 루트 디렉토리에 `.env` 파일을 생성하고 API 키들을 저장합니다:\n",
        "\n",
        "```bash\n",
        "# .env 파일 생성\n",
        "OPENAI_API_KEY=sk-...\n",
        "OPENROUTER_API_KEY=or-...\n",
        "HUGGINGFACE_API_KEY=hf-...\n",
        "```\n",
        "\n",
        "**2: python-dotenv 패키지 사용**\n",
        "\n",
        "```python\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv() # .env 파일의 경로가 다를경우 직접 상대경로 또는 절대 경로를 입력해주어야합니다. load_dotenv(path=\"경로\")\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "-----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 1\\. OpenAI 임베딩 API 사용법\n",
        "\n",
        "| 최신 모델 | 최대 컨텍스트 | 가격 (입력 + 출력 합산) |\n",
        "| :--- | :--- | :--- |\n",
        "| `text-embedding-3-small` | 8,191 tokens | **$0.02 / 1M tokens** |\n",
        "| `text-embedding-3-large` | 8,191 tokens | $0.13 / 1M tokens |\n",
        "\n",
        "> `text-embedding-3-small`는 이전 세대(`ada-002`) 대비 5배 더 저렴하고 품질도 높습니다.\n",
        "\n",
        "### 1-1. 계정·키 만들기\n",
        "\n",
        "1.  [https://platform.openai.com](https://platform.openai.com) 에 가입합니다.\n",
        "2.  “API Keys” 메뉴에서 “Create new secret key”를 클릭합니다.\n",
        "3.  키를 복사해 `OPENAI_API_KEY` 환경 변수에 저장합니다.\n",
        "\n",
        "### 1-2. 간단 예제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep3GHlqdTx_r"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# API 키는 환경 변수(`OPENAI_API_KEY`)에서 자동으로 로드됩니다.\n",
        "client = OpenAI()\n",
        "text_list = [\"인공지능의 미래는 밝다.\", \"오늘 점심은 무엇을 먹을까?\"]\n",
        "\n",
        "response = client.embeddings.create(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    input=text_list\n",
        ")\n",
        "\n",
        "vectors = [obj.embedding for obj in response.data]\n",
        "print(f\"벡터 차원: {len(vectors[0])}\")\n",
        "print(f\"첫 번째 벡터 (앞 5개 차원): {vectors[0][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBMBl05tTx_r"
      },
      "source": [
        "### 1-3. 비용 감 잡기\n",
        "\n",
        "  - **문서 1,000개** (각 500 토큰) ≈ 500,000 토큰 ≈ **$0.01** (약 13원)\n",
        "  - **Chunking**: 문서를 512 토큰 이하로 잘라 처리하면 품질과 비용을 최적화할 수 있습니다.\n",
        "  - **캐싱**: 동일한 문장은 한 번만 임베딩하여 결과를 저장하고 재사용하는 것이 비용 절감에 필수적입니다.\n",
        "\n",
        "-----\n",
        "\n",
        "## 2\\. OpenRouter (무료/저요금) 임베딩 API 사용법\n",
        "\n",
        "OpenRouter는 400개 이상의 다양한 모델을 OpenAI API와 호환되는 형식으로 제공하는 프록시 서비스입니다. 여러 모델이 무료 또는 매우 저렴하게 제공되어 개발, 연구, 학습용으로 인기가 높습니다.\n",
        "\n",
        "### 2-1. 계정·키 만들기\n",
        "\n",
        "1.  [https://openrouter.ai](https://openrouter.ai) 에 가입합니다.\n",
        "2.  **Dashboard → API Keys** 메뉴에서 키를 발급받습니다.\n",
        "3.  발급받은 키를 `OPENROUTER_API_KEY` 환경 변수에 저장합니다.\n",
        "\n",
        "### 2-2. 무료 임베딩 모델 예시\n",
        "\n",
        "| 모델 ID (OpenRouter) | 가격 | 특징 |\n",
        "| :--- | :--- | :--- |\n",
        "| `google/gemma-2-9b-it:free` | $0 / token | Google의 최신 모델, 다국어 지원 |\n",
        "| `mistralai/mistral-7b-instruct:free` | $0 / token | 작고 빠른 모델, 준수한 성능 |\n",
        "| `jinaai/jina-embeddings-v2-base-en`| $0.01 / 1M tokens| 임베딩 전용 모델, 영어 특화 |\n",
        "\n",
        "> **참고**: 무료 모델은 분당 토큰 처리량(TPM)과 동시 요청 수에 제한이 있습니다.\n",
        "\n",
        "### 2-3. 최소 예제 (OpenAI SDK 그대로)\n",
        "\n",
        "OpenAI SDK 코드에서 `base_url`과 `model` ID만 변경하면 바로 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2boiWWQTx_s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        ")\n",
        "\n",
        "text_list = [\"인공지능의 미래는 밝다.\", \"오늘 점심은 무엇을 먹을까?\"]\n",
        "\n",
        "response = client.embeddings.create(\n",
        "    model=\"google/gemma-2-9b-it:free\", # OpenRouter에서 제공하는 모델 ID\n",
        "    input=text_list\n",
        ")\n",
        "\n",
        "vectors = [obj.embedding for obj in response.data]\n",
        "print(f\"벡터 차원: {len(vectors[0])}\")\n",
        "print(f\"첫 번째 벡터 (앞 5개 차원): {vectors[0][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhwwrmkNTx_s"
      },
      "source": [
        "### 2-4. 품질 및 제한 팁\n",
        "\n",
        "  * **속도**: 무료 모델은 응답이 느릴 수 있으므로, 대량 작업 시에는 배치 처리와 `time.sleep()`을 이용한 간격 조절이 필요합니다.\n",
        "  * **품질**: 모델마다 성능 편차가 크므로, 작은 데이터셋으로 사전 테스트 후 사용할 모델을 결정하는 것이 좋습니다.\n",
        "  * **가용성**: 무료 모델은 서버 용량이 가득 차면 에러를 반환할 수 있습니다. `try-except`와 재시도 로직을 구현하면 안정성을 높일 수 있습니다.\n",
        "\n",
        "-----\n",
        "\n",
        "## 3\\. Hugging Face 임베딩 API 사용법\n",
        "\n",
        "Hugging Face는 전 세계 AI 모델의 허브로, **Inference API**를 통해 수많은 오픈소스 모델을 서버리스 형태로 사용할 수 있게 지원합니다. 특히 한국어 특화 임베딩 모델을 무료 티어에서 테스트하기에 매우 유용합니다.\n",
        "\n",
        "### 3-1. 계정·키 만들기\n",
        "\n",
        "1.  [https://huggingface.co](https://huggingface.co) 에 가입합니다.\n",
        "2.  **Profile → Settings → Access Tokens** 메뉴로 이동합니다.\n",
        "3.  `New token` 버튼을 눌러 'read' 권한의 키를 발급하고, 이 값을 `HUGGINGFACE_API_KEY` 환경 변수에 저장합니다.\n",
        "\n",
        "### 3-2. 추천 한국어 임베딩 모델\n",
        "\n",
        "| 모델 ID (Hugging Face) | 특징 |\n",
        "| :--- | :--- |\n",
        "| `jhgan/ko-sroberta-multitask` | 다양한 한국어 태스크로 학습되어 범용성이 높음 (SBERT 기반) |\n",
        "| `snunlp/KR-SBERT-V40K-klueNLI-augSTS` | 한국어 문장 유사도(STS) 측정에 특화된 모델 |\n",
        "| `google-bert/bert-base-multilingual-cased` | 다국어 모델로, 여러 언어가 섞인 환경에서 유용 |\n",
        "\n",
        "### 3-3. 최소 예제 (`requests` 라이브러리)\n",
        "\n",
        "Hugging Face Inference API는 `requests` 라이브러리를 통해 직접 호출할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HfyCtI6Tx_s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "# 사용할 모델의 API URL\n",
        "API_URL = \"https://api-inference.huggingface.co/models/jhgan/ko-sroberta-multitask\"\n",
        "headers = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n",
        "\n",
        "def get_hf_embedding(text_list: list[str]) -> np.ndarray | None:\n",
        "    try:\n",
        "        response = requests.post(API_URL, headers=headers, json={\"inputs\": text_list})\n",
        "        response.raise_for_status()\n",
        "        # Hugging Face API는 벡터 리스트를 직접 반환합니다.\n",
        "        return np.array(response.json())\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"API 요청 중 에러가 발생했습니다: {e}\")\n",
        "        return None\n",
        "\n",
        "# API 호출\n",
        "text_list = [\"인공지능의 미래는 밝다.\", \"오늘 점심은 무엇을 먹을까?\"]\n",
        "vectors = get_hf_embedding(text_list)\n",
        "\n",
        "if vectors is not None:\n",
        "    print(f\"벡터 차원: {len(vectors[0])}\")\n",
        "    print(f\"첫 번째 벡터 (앞 5개 차원): {vectors[0][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z5EGJVUTx_s"
      },
      "source": [
        "### 3-4. (심화) `sentence-transformers` 라이브러리 활용\n",
        "\n",
        "`sentence-transformers` 라이브러리를 사용하면 Hugging Face의 모델을 더 간편하게 다룰 수 있습니다. API 호출 대신 모델을 로컬에 다운로드하여 실행하는 방식이지만, 코드가 매우 직관적입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BL_V_bG1Tx_s"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 모델을 로컬에 다운로드하고 초기화합니다. (최초 실행 시 시간 소요)\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "text_list = [\"인공지능의 미래는 밝다.\", \"오늘 점심은 무엇을 먹을까?\"]\n",
        "vectors = model.encode(text_list)\n",
        "\n",
        "print(f\"벡터 차원: {vectors.shape[1]}\")\n",
        "print(f\"첫 번째 벡터 (앞 5개 차원): {vectors[0][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Yv2dUPTx_t"
      },
      "source": [
        "-----\n",
        "\n",
        "## 4\\. 여러 환경을 자유롭게 전환하는 패턴\n",
        "\n",
        "각기 다른 API를 하나의 인터페이스로 통일하면 코드 변경을 최소화하며 서비스를 전환할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj9XzmAXTx_t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "class EmbeddingService:\n",
        "    def __init__(self, provider: str = \"openai\", model: str = None):\n",
        "        self.provider = provider\n",
        "\n",
        "        if provider == \"openai\":\n",
        "            self.client = OpenAI()\n",
        "            self.model = model or \"text-embedding-3-small\"\n",
        "        elif provider == \"openrouter\":\n",
        "            self.client = OpenAI(\n",
        "                base_url=\"https://openrouter.ai/api/v1\",\n",
        "                api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        "            )\n",
        "            self.model = model or \"google/gemma-2-9b-it:free\"\n",
        "        elif provider == \"huggingface\":\n",
        "            self.model = model or \"jhgan/ko-sroberta-multitask\"\n",
        "            self.api_url = f\"https://api-inference.huggingface.co/models/{self.model}\"\n",
        "            self.headers = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n",
        "        else:\n",
        "            raise ValueError(\"지원하지 않는 provider입니다: openai, openrouter, huggingface\")\n",
        "\n",
        "    def embed(self, text_list: list[str]) -> list[list[float]] | None:\n",
        "        print(f\"--- {self.provider}({self.model}) 모델로 임베딩 수행 ---\")\n",
        "        if self.provider in [\"openai\", \"openrouter\"]:\n",
        "            response = self.client.embeddings.create(model=self.model, input=text_list)\n",
        "            return [obj.embedding for obj in response.data]\n",
        "\n",
        "        elif self.provider == \"huggingface\":\n",
        "            try:\n",
        "                response = requests.post(self.api_url, headers=self.headers, json={\"inputs\": text_list})\n",
        "                response.raise_for_status()\n",
        "                return response.json()\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"API 요청 오류: {e}\")\n",
        "                return None\n",
        "\n",
        "# --- 사용 예시 ---\n",
        "# service = EmbeddingService(provider=\"openai\")\n",
        "# service = EmbeddingService(provider=\"openrouter\", model=\"mistralai/mistral-7b-instruct:free\")\n",
        "service = EmbeddingService(provider=\"huggingface\")\n",
        "\n",
        "vectors = service.embed([\"이것은 테스트 문장입니다.\"])\n",
        "if vectors:\n",
        "    print(\"임베딩 성공!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO4ZLe4kTx_t"
      },
      "source": [
        "이처럼 \\*\\*클래스 기반의 래퍼(Wrapper)\\*\\*를 만들면, `provider` 이름만 바꿔서 일관된 방식으로 모든 서비스를 사용할 수 있습니다.\n",
        "\n",
        "-----\n",
        "\n",
        "## 5\\. 후속 활용 : 벡터 DB에 저장하기\n",
        "\n",
        "생성된 임베딩 벡터는 \\*\\*벡터 데이터베이스(Vector DB)\\*\\*에 저장하여 의미 기반 검색, 추천 시스템, RAG(검색 증강 생성) 파이프라인 등에 활용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supabase를 Vector DB로 활용하기\n",
        "\n",
        "Supabase는 PostgreSQL 기반의 오픈소스 백엔드 서비스로, **pgvector** 확장을 통해 벡터 데이터베이스 기능을 제공합니다. 임베딩 벡터를 저장하고 의미 기반 검색을 수행할 수 있는 강력한 플랫폼입니다.\n",
        "\n",
        "#### 주요 특징:\n",
        "- **pgvector 확장**: 벡터 유사도 검색 지원\n",
        "- **SQL 기반**: 익숙한 SQL 쿼리로 벡터 검색 가능\n",
        "- **실시간 API**: REST API와 실시간 구독 기능\n",
        "- **무료 티어**: 개발 및 학습용으로 충분한 무료 사용량 제공"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 준비 절차\n",
        "\n",
        "##### 1. pgvector 확장 설치 (한 번만 실행, 여러분은 이미 설치했어요 지난 시간에)\n",
        "\n",
        "```sql\n",
        "-- Postgres 15 이상 / Supabase 기본 버전\n",
        "create extension if not exists vector cascade;\n",
        "```\n",
        "\n",
        "`pgvector` 확장이 활성화되면 **vector**, **halfvec**(16-bit 부동소수형), **svec**(sparse) 자료형과 K-NN 인덱스(HNSW, IVFFLAT 등)를 쓸 수 있습니다. ([supabase.com][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##### 2. 테이블 생성\n",
        "   \n",
        "지난번에 생성했던 테이블이 있다면 콘솔에서 삭제하시고 진행해주세요\n",
        "\n",
        "```sql\n",
        "create table if not exists public.embeddings (\n",
        "  id          bigserial primary key,\n",
        "  content     text,                       -- 원본 문장 or 문서 요약\n",
        "  embedding   vector(1536),               -- OpenAI text-embedding-3-small / ada-002 크기\n",
        "  created_at  timestamptz default now()\n",
        ");\n",
        "```\n",
        "\n",
        "> 🔎 **vector(차원)** 은 모델 출력 차원과 반드시 일치해야 합니다. (text-embedding-3-small 및 text-embedding-ada-002 ⇒ 1536차원)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##### 3. K-NN 인덱스 추가(선택)\n",
        "\n",
        "빈도가 많은 검색·유사도 비교가 필요하다면 다음 중 하나를 추가하세요.\n",
        "\n",
        "```sql\n",
        "-- a. Cosine 유사도 IVFFLAT (빠르지만 삽입 전에 REINDEX 필요)\n",
        "create index if not exists embeddings_embedding_cos_ivfflat\n",
        "  on public.embeddings using ivfflat (embedding vector_cosine_ops)\n",
        "  with (lists = 100);       -- 리스트 수는 데이터 규모에 맞춰 조정\n",
        "\n",
        "-- b. HNSW(삽입·검색 동시성 ↑, 최신 PostgreSQL 필요)\n",
        "-- create index ... using hnsw (embedding vector_l2_ops) with (m = 16, ef_construction = 64);\n",
        "```\n",
        "\n",
        "인덱스를 만들기 전에 `ANALYZE embeddings;`를, IVFFLAT 인덱스 후에는 `REINDEX`를 권장합니다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 4. RLS(행 수준 보안) 설정 (어려운 부분이 여러분은 그냥 쿼리만 실행하세요)\n",
        "\n",
        "Supabase 프로젝트는 기본적으로 **RLS가 켜져** 있습니다. 익명 키(anon key)로 쓰기·읽기를 허용하려면 최소 두 개의 정책이 필요합니다.\n",
        "\n",
        "```sql\n",
        "alter table public.embeddings enable row level security;\n",
        "\n",
        "-- INSERT 허용\n",
        "create policy \"Allow insert for anon\" on public.embeddings\n",
        "  for insert\n",
        "  to anon\n",
        "  using (true)          -- 조건을 걸어야 한다면 여기서 필터\n",
        "\n",
        "-- SELECT 허용\n",
        "create policy \"Allow read for anon\" on public.embeddings\n",
        "  for select\n",
        "  to anon\n",
        "  using (true);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##### 5. 파이썬 클라이언트 사용 팁\n",
        "\n",
        "```python\n",
        "# 벡터는 list[float] 형태면 자동 캐스팅됩니다.\n",
        "vectors = [[0.01, 0.23, ... , 0.45]]  # 1536-dim list\n",
        "text_list = [\"이것은 테스트 문장입니다.\"]\n",
        "\n",
        "save_embeddings_to_supabase(vectors, text_list)\n",
        "```\n",
        "\n",
        "* **환경 변수**로 `SUPABASE_URL`과 `SUPABASE_ANON_KEY`를 관리해 키 노출을 방지하세요.\n",
        "* 수천 \\~ 수만 건 이상을 넣을 때는 `insert().execute()` 대신 `upsert()` 또는 `copy_from()`(psycopg) 방식을 고려하면 훨씬 빠릅니다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcD23MRqTx_t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from supabase import create_client, Client\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Supabase 클라이언트 설정\n",
        "url = os.getenv(\"SUPABASE_URL\") # .env 파일에 저장된 환경 변수 사용\n",
        "key = os.getenv(\"SUPABASE_ANON_KEY\") # .env 파일에 저장된 환경 변수 사용\n",
        "supabase: Client = create_client(url, key)\n",
        "\n",
        "# 임베딩 벡터를 Supabase에 저장하는 예제\n",
        "def save_embeddings_to_supabase(vectors, text_list):\n",
        "    \"\"\"\n",
        "    임베딩 벡터와 텍스트를 Supabase 테이블에 저장\n",
        "    \n",
        "    테이블 구조 예시:\n",
        "    - id: SERIAL PRIMARY KEY\n",
        "    - content: TEXT\n",
        "    - embedding: VECTOR(1536) -- 또는 해당 모델의 차원 수\n",
        "    - created_at: TIMESTAMP\n",
        "    \"\"\"\n",
        "    \n",
        "    data_to_insert = []\n",
        "    for i, (text, vector) in enumerate(zip(text_list, vectors)):\n",
        "        data_to_insert.append({\n",
        "            \"content\": text,\n",
        "            \"embedding\": vector  # Supabase는 리스트 형태의 벡터를 자동으로 처리\n",
        "        })\n",
        "    \n",
        "    try:\n",
        "        result = supabase.table(\"embeddings\").insert(data_to_insert).execute()\n",
        "        print(f\"성공적으로 {len(data_to_insert)}개의 임베딩을 저장했습니다.\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Supabase 저장 중 오류 발생: {e}\")\n",
        "        return None\n",
        "\n",
        "# 사용 예시\n",
        "if vectors:\n",
        "    text_list = [\"이것은 테스트 문장입니다.\"]\n",
        "    save_embeddings_to_supabase(vectors, text_list)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
