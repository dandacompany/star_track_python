{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI_K7cRUvMkQ"
      },
      "source": [
        "## 📈 정상성과 인과성\n",
        "\n",
        "시계열 데이터 분석의 핵심 개념인 **정상성(Stationarity)** 과 **그랜저 인과성(Granger Causality)** 을 다룹니다. \n",
        "\n",
        "---\n",
        "\n",
        "## 1. 시계열 정상성(Stationarity)이란?\n",
        "\n",
        "### 💡 개념 (Concept)\n",
        "\n",
        "**정상성(Stationarity)** 이란 시계열 데이터의 통계적 특성(평균, 분산 등)이 시간의 흐름에 따라 변하지 않는다는 것을 의미합니다. \n",
        "\n",
        "마치 잔잔한 호수처럼, 데이터의 전반적인 모습이 어느 시점에서 보아도 일정하게 유지되는 상태를 말합니다.\n",
        "\n",
        "* **왜 중요할까요?**\n",
        "  \n",
        "    미래를 예측하는 대부분의 시계열 모델은 \"데이터의 기본 규칙(패턴)이 과거나 지금이나 미래에도 동일할 것\"이라고 가정합니다. 데이터가 정상성을 띨 때, 우리는 과거의 패턴을 학습하여 미래를 더 안정적으로 예측할 수 있습니다. 반면, 데이터에 뚜렷한 **추세(Trend)** 나 시간에 따라 변하는 **변동성(Volatility)** 이 있다면, 이는 비정상(Non-stationary) 데이터이며 분석 전에 반드시 정상성을 만족하도록 변환해주어야 합니다.\n",
        "\n",
        "* **정상성과 비정상성의 시각적 비교**\n",
        "    * **정상 시계열**: 평균을 중심으로 일정한 폭 안에서 움직입니다.\n",
        "    * **비정상 시계열**: 시간에 따라 평균이 계속 상승(추세)하거나, 변동 폭이 점점 커지거나 작아집니다.\n",
        "\n",
        "### 💻 예시 코드 (Example Code)\n",
        "\n",
        "아래 코드는 정상성을 띠는 데이터(백색소음)와 뚜렷한 추세를 가진 비정상 데이터를 생성하고 시각화하여 둘의 차이를 명확히 보여줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DSzI1LoLvMkT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# 시각화를 위한 기본 설정\n",
        "pd.options.plotting.backend = \"plotly\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. 정상 시계열 데이터 생성 (Standard Normal Distribution)\n",
        "np.random.seed(42)\n",
        "stationary_data = np.random.randn(200)\n",
        "stationary_series = pd.Series(stationary_data, name=\"Stationary Data (White Noise)\")\n",
        "\n",
        "# 시각화\n",
        "fig_stationary = px.line(stationary_series, title=\"✅ 정상 시계열의 예시\")\n",
        "fig_stationary.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. 비정상 시계열 데이터 생성 (Random Walk with Drift)\n",
        "trend = np.arange(200) * 0.2\n",
        "non_stationary_data = np.random.randn(200).cumsum() + trend\n",
        "non_stationary_series = pd.Series(non_stationary_data, name=\"Non-Stationary Data (Random Walk)\")\n",
        "# 시각화\n",
        "fig_non_stationary = px.line(non_stationary_series, title=\"❌ 비정상 시계열의 예시 (추세 존재)\")\n",
        "fig_non_stationary.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB6xvZBCvMkU"
      },
      "source": [
        "### ✏️ 연습 문제 (Practice Problems)\n",
        "\n",
        "1.  미국 월별 항공 승객 수(`air`) 데이터를 불러와 시각화하고, 이 데이터가 정상적으로 보이는지 혹은 비정상적으로 보이는지 이유와 함께 설명해보세요. (힌트: `px.data.air()`를 사용하면 쉽게 데이터를 로드할 수 있습니다.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 연습 문제 1번 풀이 공간\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 이 데이터는 비정상적(Non-stationary)으로 보입니다. 이유는,\n",
        "> 1. 뚜렷한 상승 추세(Trend)가 관찰됩니다.\n",
        "> 2. 계절적 패턴(Seasonality)이 반복적으로 나타납니다.\n",
        "> 3. 시간이 지남에 따라 평균값이 지속적으로 증가합니다.\n",
        "> 4. 변동성도 시간에 따라 변화하는 것으로 보입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "2.  `np.random.randn(300)`으로 무작위 데이터를 생성한 뒤, `.cumprod()` 메소드를 적용하여 새로운 시계열을 만드세요. 이 데이터는 정상성을 띨까요? 시각화하여 확인해보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVwYpxesvMkU"
      },
      "outputs": [],
      "source": [
        "# 연습 문제 2번 풀이 공간\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(해석 작성)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieKi_Nr4vMkU"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. 정상성 통계적 검정 (ADF & KPSS)\n",
        "\n",
        "### 💡 개념 (Concept)\n",
        "\n",
        "눈으로 데이터의 정상성을 판단하는 것은 주관적일 수 있습니다. 따라서 우리는 통계적인 가설 검정 방법을 사용하여 객관적으로 정상성을 진단합니다.\n",
        "\n",
        "가장 널리 사용되는 두 가지 검정은 **ADF 검정**과 **KPSS 검정**입니다.\n",
        "\n",
        "* **ADF 검정 (Augmented Dickey-Fuller Test)**\n",
        "    * **귀무가설 (H0)**: \"데이터가 비정상적이다 (단위근이 존재한다).\"\n",
        "    * **해석**: `p-value`가 유의수준(보통 0.05)보다 **작으면**, 귀무가설을 기각합니다. 즉, **\"데이터가 정상적이다\"** 라고 판단합니다.\n",
        "\n",
        "* **KPSS 검정 (Kwiatkowski-Phillips-Schmidt-Shin Test)**\n",
        "    * **귀무가설 (H0)**: \"데이터가 (추세에 대해) 정상적이다.\"\n",
        "    * **해석**: `p-value`가 유의수준(보통 0.05)보다 **크면**, 귀무가설을 기각하지 못합니다. 즉, **\"데이터가 정상적이다\"** 라고 판단합니다.\n",
        "\n",
        "> **⚠️ 중요**: 두 검정은 귀무가설이 서로 반대이므로 해석에 주의해야 합니다. 보통 두 검정의 결과를 교차 확인하여 종합적으로 판단합니다.\n",
        ">\n",
        "> * ADF (p < 0.05) & KPSS (p > 0.05) → **정상성 (Stationary)**\n",
        "> * ADF (p > 0.05) & KPSS (p < 0.05) → **비정상성 (Non-stationary)**\n",
        "\n",
        "### 💻 예시 코드 (Example Code)\n",
        "\n",
        "`statsmodels` 라이브러리를 사용하여 ADF와 KPSS 검정을 수행합니다. 실제 경제 데이터인 미국 월별 소매 판매 지수를 가져와 테스트해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgmlhDPvvMkU"
      },
      "outputs": [],
      "source": [
        "!pip install statsmodels pandas-datareader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas_datareader.data as web\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "import pandas as pd\n",
        "\n",
        "# FRED에서 미국 월별 소매 판매 데이터 가져오기 (2000-01-01 ~ 2023-12-31)\n",
        "# RETAIL AND FOOD SERVICES SALES, TOTAL (MRTSSM44000USS)\n",
        "df_retail = web.DataReader('MRTSSM44000USS', 'fred', start='2000-01-01', end='2023-12-31')\n",
        "retail_sales = df_retail['MRTSSM44000USS']\n",
        "\n",
        "# 데이터 시각화\n",
        "fig = px.line(retail_sales, title=\"미국 월별 소매 판매 지수\", labels={\"value\": \"Sales Index\", \"index\": \"Date\"})\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- ADF 검정 함수 ---\n",
        "def adf_test(series):\n",
        "    result = adfuller(series.dropna())\n",
        "    print('* ADF Test Result')\n",
        "    print(f'- ADF Statistic: {result[0]:.4f}')\n",
        "    print(f'- p-value: {result[1]:.4f}')\n",
        "    if result[1] < 0.05:\n",
        "        print(\"결론: p-value < 0.05 이므로, 데이터는 정상적입니다. (귀무가설 기각)\")\n",
        "    else:\n",
        "        print(\"결론: p-value >= 0.05 이므로, 데이터는 비정상적입니다. (귀무가설 기각 실패)\")\n",
        "\n",
        "# --- KPSS 검정 함수 ---\n",
        "def kpss_test(series):\n",
        "    # 'c'는 constant(level)에 대한 정상성 검정\n",
        "    result = kpss(series.dropna(), regression='c')\n",
        "    print('* KPSS Test Result')\n",
        "    print(f'- KPSS Statistic: {result[0]:.4f}')\n",
        "    print(f'- p-value: {result[1]:.4f}')\n",
        "    if result[1] < 0.05:\n",
        "        print(\"결론: p-value < 0.05 이므로, 데이터는 비정상적입니다. (귀무가설 기각)\")\n",
        "    else:\n",
        "        print(\"결론: p-value >= 0.05 이므로, 데이터는 정상적입니다. (귀무가설 기각 실패)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADF 검정 수행\n",
        "adf_test(retail_sales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KPSS 검정 수행\n",
        "kpss_test(retail_sales)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tNyy3-6vMkV"
      },
      "source": [
        "### ✏️ 연습 문제 (Practice Problems)\n",
        "\n",
        "1.  FRED에서 **미국 산업 생산 지수(코드: `INDPRO`)** 데이터를 2010년부터 현재까지 가져오세요.\n",
        "   * API 키 설정 필요 (https://fred.stlouisfed.org) : 회원가입후, 프로필 > API Key > Request API Key 클릭하여 발급받기\n",
        "   * 발급받은 API 키를 환경변수 파일(.env)에 저장하세요. FRED_API_KEY=발급받은 API 키"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install fredapi python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from fredapi import Fred\n",
        "load_dotenv()\n",
        "fred = Fred(api_key=os.environ.get('FRED_API_KEY'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "indpro = fred.get_series('INDPRO', observation_start='2010-01-01')\n",
        "\n",
        "# 가져온 데이터 확인 코드 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  가져온 데이터를 시각화하고, ADF와 KPSS 검정을 모두 수행하여 정상성 여부를 판단하고 결과를 설명하세요.\n",
        "   \n",
        "    두 결과가 상반되게 나타났다면, 두 결과를 종합하여 판단합니다. 보통은 상반되게 나오면 비정상성으로 판단합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4mFJRmGvMkV"
      },
      "outputs": [],
      "source": [
        "# 연습 문제 2번 풀이 공간\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(정상성 판단 결과 해석 작성)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uksa_O2GvMkW"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. 비정상 데이터 정상화하기 (feat. 차분 & 로그 변환)\n",
        "\n",
        "### 💡 개념 (Concept)\n",
        "\n",
        "비정상 데이터를 정상 데이터로 변환하는 가장 일반적인 방법은 **차분(Differencing)** 과 **로그 변환(Log Transformation)** 입니다.\n",
        "\n",
        "* **차분 (Differencing)**\n",
        "    * **목적**: 데이터의 **추세(Trend)** 를 제거합니다.\n",
        "    * **방법**: 현재 시점의 데이터에서 바로 이전 시점의 데이터를 빼줍니다. ($Y'_t = Y_t - Y_{t-1}$)\n",
        "    * Pandas에서는 `.diff()` 메서드를 사용하여 간단하게 계산할 수 있습니다. 1차 차분으로 정상성을 만족하지 못하면, 2차 차분(`series.diff().diff()`)을 시도할 수 있습니다.\n",
        "\n",
        "* **로그 변환 (Log Transformation)**\n",
        "    * **목적**: 시간에 따라 변동성이 커지는 현상(분산의 불안정성)을 완화합니다.\n",
        "    * **방법**: 데이터의 모든 값에 자연로그(`np.log()`)를 취합니다.\n",
        "    * 보통 변동성과 추세가 함께 나타나는 데이터에 **로그 변환을 먼저 적용한 후, 차분을 수행**하는 경우가 많습니다.\n",
        "\n",
        "### 💻 예시 코드 (Example Code)\n",
        "\n",
        "앞서 사용한 비정상 데이터 '미국 월별 소매 판매 지수'를 정상 데이터로 변환해 보겠습니다. 이 데이터는 뚜렷한 추세와 함께 시간이 지날수록 변동성이 커지는 모습을 보이므로, 로그 변환 후 차분을 적용하는 것이 효과적입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKM_ZYozvMkW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1. 로그 변환 적용\n",
        "log_retail_sales = np.log(retail_sales)\n",
        "fig = px.line(log_retail_sales, title=\"1. 로그 변환 후 소매 판매 지수\",\n",
        "              labels={\"value\": \"Log(Sales Index)\", \"index\": \"Date\"})\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. 1차 차분 적용\n",
        "diff_log_retail_sales = log_retail_sales.diff().dropna()\n",
        "fig = px.line(diff_log_retail_sales, title=\"2. 로그 변환 및 1차 차분 후 소매 판매 지수\",\n",
        "              labels={\"value\": \"Differenced Log Sales\", \"index\": \"Date\"})\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. 변환된 데이터에 정상성 검정 재수행\n",
        "print(\"--- 변환 후 데이터 정상성 검정 ---\")\n",
        "adf_test(diff_log_retail_sales)\n",
        "kpss_test(diff_log_retail_sales)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YfYVoCxvMkW"
      },
      "source": [
        "### ✏️ 연습 문제 (Practice Problems)\n",
        "\n",
        "1.  **야후 파이낸스(`yfinance`)** 라이브러리를 사용하여 2015년부터 현재까지의 **애플(AAPL) 주가 데이터**를 가져오세요. 종가 데이터를 Series 변수에 저장합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 연습 문제 1번 풀이 공간\n",
        "# '(Close, AAPL)' 컬럼 선택\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  해당 주가 데이터가 비정상적임을 시각화와 통계 검정으로 확인하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 연습 문제 2번 풀이 공간"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. 통계 검정 (ADF, KPSS)을 통한 비정상성 확인\n",
        "\n",
        "# ADF (Augmented Dickey-Fuller) 검정\n",
        "\n",
        "# 코드 작성\n",
        "\n",
        "# KPSS (Kwiatkowski-Phillips-Schmidt-Shin) 검정\n",
        "\n",
        "# 코드 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.  주가 데이터에 로그 변환과 차분을 순서대로 적용하고, 각 단계별로 데이터를 시각화하세요. 최종적으로 변환된 데이터가 정상성을 만족하는지 ADF와 KPSS 검정으로 확인하고 결과를 설명하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9zbNmvuvMkW"
      },
      "outputs": [],
      "source": [
        "# 연습 문제 3번 풀이 공간\n",
        "\n",
        "# 1. 로그 변환\n",
        "# 코드 작성\n",
        "\n",
        "# 2. 1차 차분\n",
        "# 코드 작성\n",
        "# 3. 각 단계별 데이터 시각화 (Plotly 사용)\n",
        "# 코드 작성\n",
        "\n",
        "# 4. 최종 변환된 데이터 (aapl_diff)의 정상성 검정 (ADF, KPSS)\n",
        "# 코드 작성 및 해석 작성\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49xYUwU4vMkW"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. 그랜저 인과성 (Granger Causality) 검정\n",
        "\n",
        "### 💡 개념 (Concept)\n",
        "\n",
        "**그랜저 인과성 검정**은 한 시계열 X의 과거 값이 다른 시계열 Y의 미래 값을 예측하는 데 유용한 정보를 제공하는지를 판단하는 통계적 가설 검정입니다.\n",
        "\n",
        "* **핵심 질문**: \"Y의 과거 정보만으로 Y를 예측하는 것보다, X의 과거 정보까지 함께 사용했을 때 Y의 예측력이 더 향상되는가?\"\n",
        "* 만약 예측력이 의미 있게 향상된다면, **\"X가 Y를 그랜저-유발(Granger-causes)한다\"**고 말합니다.\n",
        "* **주의사항**:\n",
        "    * 그랜저 인과성은 'A가 B의 원인이다'라는 철학적/실제적 인과관계를 의미하지 않습니다. 오직 **'예측에 도움이 되는가'**라는 관점의 통계적 관계입니다.\n",
        "    * **매우 중요**: 그랜저 인과성 검정을 수행하기 전에, 분석에 사용될 **모든 시계열 데이터가 정상성을 만족**해야 합니다.\n",
        "\n",
        "* **가설 검정**:\n",
        "    * **귀무가설 (H0)**: \"X가 Y를 그랜저-유발하지 않는다.\" (X의 과거 정보는 Y 예측에 도움이 되지 않는다)\n",
        "    * **해석**: `p-value`가 유의수준(보통 0.05)보다 **작으면**, 귀무가설을 기각합니다. 즉, **\"X가 Y를 그랜저-유발한다\"**고 결론 내립니다.\n",
        "\n",
        "### 💻 예시 코드 (Example Code)\n",
        "\n",
        "\"소비자 물가지수(CPI)가 생산자 물가지수(PPI)에 영향을 줄까? 혹은 그 반대일까?\" 라는 질문을 그랜저 인과성 검정으로 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRgvdadovMkX"
      },
      "outputs": [],
      "source": [
        "# !pip install statsmodels pandas-datareader\n",
        "import pandas_datareader.data as web\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# FRED에서 월별 데이터 가져오기 (2000-01-01 ~ 2023-12-31)\n",
        "# CPI (CPIAUCSL), PPI (PPIACO)\n",
        "start = '2000-01-01'\n",
        "end = '2023-12-31'\n",
        "cpi = web.DataReader('CPIAUCSL', 'fred', start, end)\n",
        "ppi = web.DataReader('PPIACO', 'fred', start, end)\n",
        "\n",
        "# 두 데이터를 하나의 데이터프레임으로 합치기\n",
        "df_prices = pd.concat([cpi, ppi], axis=1)\n",
        "df_prices.columns = ['CPI', 'PPI']\n",
        "df_prices.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. 데이터 정상화 (로그 변환 + 1차 차분)\n",
        "df_prices_transformed = np.log(df_prices).diff().dropna()\n",
        "\n",
        "# 정상화된 데이터 시각화\n",
        "fig = df_prices_transformed.plot(title=\"정상화된 CPI와 PPI (로그 수익률)\", kind='line',\n",
        "                                 labels={\"value\": \"Log Return\", \"index\": \"Date\"})\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. 그랜저 인과성 검정\n",
        "# H0: PPI가 CPI를 그랜저-유발하지 않는다.\n",
        "print(\"--- 검정 1: PPI -> CPI ---\")\n",
        "granger_test_result_1 = grangercausalitytests(df_prices_transformed[['CPI', 'PPI']], maxlag=4, verbose=False)\n",
        "# p-value만 출력 (lag 1~4)\n",
        "p_values_1 = [round(granger_test_result_1[lag+1][0]['ssr_ftest'][1], 4) for lag in range(4)]\n",
        "print(f\"Lag 1~4 p-values: {p_values_1}\")\n",
        "if any(p < 0.05 for p in p_values_1):\n",
        "    print(\"결론: 하나 이상의 시차에서 p-value < 0.05 이므로, PPI가 CPI를 그랜저-유발한다고 볼 수 있습니다.\")\n",
        "else:\n",
        "    print(\"결론: 모든 시차에서 p-value >= 0.05 이므로, PPI가 CPI를 그랜저-유발한다고 보기 어렵습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# H0: CPI가 PPI를 그랜저-유발하지 않는다.\n",
        "print(\"--- 검정 2: CPI -> PPI ---\")\n",
        "granger_test_result_2 = grangercausalitytests(df_prices_transformed[['PPI', 'CPI']], maxlag=4, verbose=False)\n",
        "p_values_2 = [round(granger_test_result_2[lag+1][0]['ssr_ftest'][1], 4) for lag in range(4)]\n",
        "print(f\"Lag 1~4 p-values: {p_values_2}\")\n",
        "if any(p < 0.05 for p in p_values_2):\n",
        "    print(\"결론: 하나 이상의 시차에서 p-value < 0.05 이므로, CPI가 PPI를 그랜저-유발한다고 볼 수 있습니다.\")\n",
        "else:\n",
        "    print(\"결론: 모든 시차에서 p-value >= 0.05 이므로, CPI가 PPI를 그랜저-유발한다고 보기 어렵습니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHhRsa8VvMkX"
      },
      "source": [
        "### ✏️ 연습 문제 (Practice Problems)\n",
        "\n",
        "1.  \"M2 통화량(M2SL)\"과 \"실질 개인 소비 지출(PCECC96)\"은 어떤 관계가 있을까요? 두 변수를 FRED에서 1990년부터 현재까지 월별 데이터로 가져오세요. (단, PCECC96은 월별 데이터가 아니므로, `M2SL`만 가져옵니다. 대신 `UNRATE`(실업률)을 사용해봅시다.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 연습 문제 1번 풀이 공간\n",
        "# FRED에서 M2 통화량(M2SL)과 실업률(UNRATE) 데이터 가져오기\n",
        "# 코드 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  두 시계열(`M2SL`, `UNRATE`)을 각각 시각화하고, 로그 변환 + 차분을 통해 정상성을 만족하는 데이터로 변환하세요. (ADF/KPSS 검정으로 확인 필수)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 연습 문제 2번 풀이 공간\n",
        "# 시계열 시각화\n",
        "# 코드 작성\n",
        "\n",
        "# 로그 변환 (M2SL만, UNRATE는 이미 비율 데이터이므로 제외)\n",
        "# 코드 작성\n",
        "\n",
        "# 로그 변환된 M2SL 시각화\n",
        "# 코드 작성\n",
        "\n",
        "# ADF 검정 함수 정의\n",
        "# 코드 작성\n",
        "# KPSS 검정 함수 정의\n",
        "# 코드 작성\n",
        "\n",
        "# 원본 데이터 정상성 검정\n",
        "# 코드 작성\n",
        "# 1차 차분 수행\n",
        "# 코드 작성\n",
        "\n",
        "# 차분된 데이터 시각화\n",
        "# 코드 작성\n",
        "# 차분된 데이터 정상성 검정\n",
        "# 코드 작성\n",
        "# 최종 정상성 데이터 준비\n",
        "# 코드 작성\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(해석 작성)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.  아래의 두 가지 가설에 대해 그랜저 인과성 검정을 수행하고(최대 시차 4), 각각의 결과를 해석하여 결론을 도출하세요.\n",
        "    * 가설 1: M2 통화량이 실업률을 그랜저-유발하는가? (M2SL → UNRATE)\n",
        "    * 가설 2: 실업률이 M2 통화량을 그랜저-유발하는가? (UNRATE → M2SL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eLlxoe_vMkX"
      },
      "outputs": [],
      "source": [
        "# 연습 문제 3번 풀이 공간\n",
        "# 그랜저 인과성 검정을 위한 라이브러리 import\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "import pandas as pd\n",
        "\n",
        "# 정상성을 만족하는 변환된 데이터 사용 (이전 단계에서 생성된 데이터)\n",
        "# 로그 변환 후 1차 차분된 데이터\n",
        "# 코드 작성\n",
        "\n",
        "# 두 시계열의 인덱스를 맞춤\n",
        "# 코드 작성\n",
        "\n",
        "# 그랜저 인과성 검정용 데이터프레임 생성\n",
        "# 코드 작성\n",
        "\n",
        "print(\"=== 그랜저 인과성 검정 결과 ===\\n\")\n",
        "\n",
        "# 가설 1: M2 통화량이 실업률을 그랜저-유발하는가? (M2SL → UNRATE)\n",
        "# 코드 작성\n",
        "\n",
        "# 그랜저 인과성 검정 (최대 시차 4)\n",
        "# 종속변수가 첫 번째 열, 독립변수가 두 번째 열\n",
        "# 코드 작성\n",
        "\n",
        "\n",
        "\n",
        "# 가설 2: 실업률이 M2 통화량을 그랜저-유발하는가? (UNRATE → M2SL)\n",
        "# 코드 작성\n",
        "# 그랜저 인과성 검정 (최대 시차 4)\n",
        "# 종속변수가 첫 번째 열, 독립변수가 두 번째 열\n",
        "# 코드 작성\n",
        "\n",
        "# 결과 해석을 위한 p-value 추출 및 정리\n",
        "# 코드 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 그랜저 인과성 검정 결과 해석\n",
        "- **귀무가설**: X가 Y를 그랜저-유발하지 않는다\n",
        "- **대립가설**: X가 Y를 그랜저-유발한다\n",
        "- **판단기준**: 유의수준 5% 기준으로 p-value < 0.05이면 귀무가설 기각\n",
        "\n",
        "#### 경제학적 해석\n",
        "- M2 통화량의 변화가 실업률 변화를 예측하는 데 도움이 되는지 분석\n",
        "- 실업률의 변화가 M2 통화량 변화를 예측하는 데 도움이 되는지 분석\n",
        "- 통화정책과 노동시장 간의 상호작용을 이해하는 데 중요한 정보 제공\n",
        "\n",
        "#### 실무적 시사점\n",
        "- 중앙은행의 통화정책 결정 시 노동시장 상황 고려 필요성\n",
        "- 경제정책 수립 시 통화량과 고용 간의 시차 효과 반영\n",
        "- 거시경제 예측 모델에서 변수 간 인과관계 활용 가능\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
